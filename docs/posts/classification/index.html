<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Samheeta">
<meta name="dcterms.date" content="2023-12-06">
<meta name="description" content="Classification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.">

<title>MLBlog - Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">MLBlog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Samheeta</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gsamheeta" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Classification</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Classification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Samheeta </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 6, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><strong>Contents:</strong></p>
<ul>
<li><p>Introduction to Classification.</p></li>
<li><p>Different types of classification.</p></li>
<li><p>Example of Linear Regression with <a href="https://www.kaggle.com/datasets/itssuru/loan-data/">Loan dataset</a>.</p></li>
<li><p>Data Visualization</p></li>
<li><p>Data processing</p></li>
<li><p>Model implementation</p></li>
<li><p>Evaluation metrics implementation</p></li>
</ul>
<section id="introduction-to-classification" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-classification">Introduction to Classification</h2>
<p>In recent times, various industries are grappling with extremely large and diverse datasets. Processing this data manually is not only time-consuming but may also lack long-term value. To enhance return on investment, a range of strategies are being employed, from basic automation to advanced machine learning techniques. This blog will focus on a key concept in this field: classification in machine learning.</p>
<p>We’ll begin by explaining the essence of classification within the realm of Machine Learning, followed by a discussion on the two types of learners in machine learning, as well as distinguishing between classification and regression. Next, the blog will explore various real-life applications of classification. Subsequently, we’ll delve into the different kinds of classification methods and examine some specific examples of classification algorithms. To conclude, the blog will offer practical experience in implementing several of these algorithms.</p>
</section>
<section id="defining-classification-in-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="defining-classification-in-machine-learning">Defining Classification in Machine Learning</h2>
<p>Classification is a type of supervised learning in machine learning where the goal is to accurately predict the label of given input data. In this process, the model is thoroughly trained with training data and then evaluated using test data before it is applied to predict labels for new, unseen data.</p>
<p>For example, a machine learning algorithm can be trained to determine whether an email is spam or not (ham). However, before delving into the nuances of classification, it’s essential to understand the difference between two classifications of learners: lazy and eager learners, and to clear up common confusions between classification and regression. <img src="classification(Img1).png" class="img-fluid" alt="Machine Learning Algorithms for Classification (src: https://www.datacamp.com/blog/classification-machine-learning)"> Lazy Learners vs.&nbsp;Eager Learners: In the realm of machine learning classification, there are two distinct types of learners: lazy and eager learners.</p>
<p>Eager learners are those algorithms that construct a model based on the training dataset before making predictions on new data. These algorithms invest more time in the training phase to better generalize from the data by learning the weights, but they are quicker in making predictions. Examples of eager learners include:</p>
<p>Logistic Regression. Support Vector Machine. Decision Trees. Artificial Neural Networks. Conversely, lazy learners, or instance-based learners, do not immediately generate a model from the training data. This is where the ‘lazy’ aspect comes in. They store the training data and, for each prediction, they search the entire training set for the nearest neighbor, resulting in slower prediction times. Examples of lazy learners are:</p>
<p>K-Nearest Neighbor. Case-based reasoning. However, there are techniques like BallTrees and KDTrees that can enhance prediction speed in these algorithms.</p>
</section>
<section id="examples-of-machine-learning-classification-in-real-life" class="level2">
<h2 class="anchored" data-anchor-id="examples-of-machine-learning-classification-in-real-life">Examples of Machine Learning Classification in Real Life</h2>
<p>Supervised Machine Learning Classification has different applications in multiple domains of our day-to-day life. Below are some examples.</p>
<p><strong>Application in Healthcare :</strong></p>
<p>Supervised machine learning classification plays a significant role in various aspects of everyday life. Here are some key examples, particularly in the healthcare domain.</p>
<ul>
<li><p>During the COVID-19 pandemic, supervised machine learning models were crucial in predicting whether individuals were infected with the virus</p></li>
<li><p>Machine learning models are also utilized by researchers to forecast the likelihood of new diseases emerging in the future.</p></li>
</ul>
<p><strong>Education :</strong></p>
<ul>
<li><p>Utilizing machine learning for categorizing various documents like text, video, and audio.</p></li>
<li><p>Automatically determining the language of students’ application documents.</p></li>
<li><p>Analyzing students’ feedback about professors to gauge sentiments.</p></li>
</ul>
<p><strong>Transportation :</strong></p>
<ul>
<li><p>Predicting areas likely to experience increased traffic volume.</p></li>
<li><p>Anticipating potential transportation problems in specific areas due to weather conditions.</p></li>
</ul>
<p><strong>Sustainable Agriculture :</strong></p>
<ul>
<li><p>Using classification models to identify the most suitable land types for different seeds.</p></li>
<li><p>Forecasting weather conditions to aid farmers in taking appropriate preventive actions.</p></li>
</ul>
</section>
<section id="different-types-of-classification-tasks-in-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="different-types-of-classification-tasks-in-machine-learning">Different Types of Classification Tasks in Machine Learning</h2>
<p>There are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications.</p>
<ul>
<li><p><strong>Binary Classification</strong>: In a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat. Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification.</p></li>
<li><p><strong>Multi-Class Classification</strong>: The multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. In the following case, the model correctly classified the image to be a plane. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include but are not limited to:</p></li>
<li><p><a href="https://monkeylearn.com/blog/classification-algorithms/#naive-bayes">Naive Bayes</a>: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.</p></li>
<li><p><a href="https://monkeylearn.com/blog/classification-algorithms/#knn">K-Nearest Neighbors</a>: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.</p></li>
<li><p><a href="https://monkeylearn.com/blog/classification-algorithms/#svm">Support Vector Machines</a>: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data.</p></li>
<li><p><a href="https://monkeylearn.com/blog/classification-algorithms/#logistic-regression">Logistic Regression</a>: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.</p></li>
<li><p><strong>Multi-Label Classification</strong>: In multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. Such a scenario can be observed in different domains, such as auto-tagging in Natural Language Processing, where a given text can contain multiple topics. Similarly to computer vision, an image can contain multiple objects, as illustrated below: the model predicted that the image contains: a plane, a boat, a truck, and a dog. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification. We can cite:</p></li>
<li><p>Multi-label Decision Trees</p></li>
<li><p>Multi-label Gradient Boosting</p></li>
<li><p>Multi-label Random Forests</p></li>
<li><p><strong>Imbalanced Classification</strong>: For the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data. Let’s consider the following 3-class classification scenario where the training data contains: 60% of trucks, 25% of planes, and 15% of boats.The imbalanced classification problem could occur in the following scenario:</p></li>
<li><p>Fraudulent transaction detections in financial industries</p></li>
<li><p>Rare disease diagnosis</p></li>
<li><p>Customer churn analysis</p></li>
</ul>
<p>Using conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise.</p>
<p>So, does that mean that such problems are left behind?</p>
<p>Of course not! We can use multiple approaches to tackle the imbalance problem in a dataset. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms.</p>
<ul>
<li><p><strong>Sampling Techniques</strong> These techniques aim to balance the distribution of the original by:</p></li>
<li><p>Cluster-based Oversampling:</p></li>
<li><p>Random undersampling: random elimination of examples from the majority class.</p></li>
<li><p>SMOTE Oversampling: random replication of examples from the minority class.</p></li>
<li><p><strong>Cost-Sensitive Algorithms</strong> These algorithms take into consideration the cost of misclassification. They aim to minimize the total cost generated by the models.</p></li>
<li><p>Cost-sensitive Decision Trees.</p></li>
<li><p>Cost-sensitive Logistic Regression.</p></li>
<li><p>Cost-sensitive Support Vector Machines.</p></li>
</ul>
</section>
<section id="example-distribution-of-loans-in-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="example-distribution-of-loans-in-the-dataset">Example: Distribution of Loans in the Dataset</h2>
<ul>
<li>Look at the first five observations in the dataset.</li>
</ul>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>loan_data <span class="op">=</span> pd.read_csv(<span class="st">"loan_data.csv"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>loan_data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">credit.policy</th>
<th data-quarto-table-cell-role="th">purpose</th>
<th data-quarto-table-cell-role="th">int.rate</th>
<th data-quarto-table-cell-role="th">installment</th>
<th data-quarto-table-cell-role="th">log.annual.inc</th>
<th data-quarto-table-cell-role="th">dti</th>
<th data-quarto-table-cell-role="th">fico</th>
<th data-quarto-table-cell-role="th">days.with.cr.line</th>
<th data-quarto-table-cell-role="th">revol.bal</th>
<th data-quarto-table-cell-role="th">revol.util</th>
<th data-quarto-table-cell-role="th">inq.last.6mths</th>
<th data-quarto-table-cell-role="th">delinq.2yrs</th>
<th data-quarto-table-cell-role="th">pub.rec</th>
<th data-quarto-table-cell-role="th">not.fully.paid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>debt_consolidation</td>
<td>0.1189</td>
<td>829.10</td>
<td>11.350407</td>
<td>19.48</td>
<td>737</td>
<td>5639.958333</td>
<td>28854</td>
<td>52.1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>credit_card</td>
<td>0.1071</td>
<td>228.22</td>
<td>11.082143</td>
<td>14.29</td>
<td>707</td>
<td>2760.000000</td>
<td>33623</td>
<td>76.7</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>debt_consolidation</td>
<td>0.1357</td>
<td>366.86</td>
<td>10.373491</td>
<td>11.63</td>
<td>682</td>
<td>4710.000000</td>
<td>3511</td>
<td>25.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>debt_consolidation</td>
<td>0.1008</td>
<td>162.34</td>
<td>11.350407</td>
<td>8.10</td>
<td>712</td>
<td>2699.958333</td>
<td>33667</td>
<td>73.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>credit_card</td>
<td>0.1426</td>
<td>102.92</td>
<td>11.299732</td>
<td>14.97</td>
<td>667</td>
<td>4066.000000</td>
<td>4740</td>
<td>39.5</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<ul>
<li>Borrowers profile in the dataset.</li>
</ul>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function for data distribution</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_loan_distrib(data):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  count <span class="op">=</span> <span class="st">""</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(data, pd.DataFrame):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>      count <span class="op">=</span> data[<span class="st">"not.fully.paid"</span>].value_counts()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>      count <span class="op">=</span> data.value_counts()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  count.plot(kind <span class="op">=</span> <span class="st">'pie'</span>, explode <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.1</span>], </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>              figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">6</span>), autopct <span class="op">=</span> <span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>, shadow <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">"Loan: Fully Paid Vs. Not Fully Paid"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  plt.legend([<span class="st">"Fully Paid"</span>, <span class="st">"Not Fully Paid"</span>])</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(loan_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="481" height="463"></p>
</div>
</div>
<p>From the graphic above, we notice that 84% of the borrowers paid their loans back, and only 16% didn’t pay them back, which makes the dataset really imbalanced.</p>
<ul>
<li><strong>Variable Types</strong> Before further, we need to check the variables’ type so that we can encode those that need to be encoded.</li>
</ul>
<p>We notice that all the columns are continuous variables, except the purpose attribute, which needs to be encoded.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check column types</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loan_data.dtypes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>credit.policy          int64
purpose               object
int.rate             float64
installment          float64
log.annual.inc       float64
dti                  float64
fico                   int64
days.with.cr.line    float64
revol.bal              int64
revol.util           float64
inq.last.6mths         int64
delinq.2yrs            int64
pub.rec                int64
not.fully.paid         int64
dtype: object</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>encoded_loan_data <span class="op">=</span> pd.get_dummies(loan_data, prefix<span class="op">=</span><span class="st">"purpose"</span>,   </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                                   drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(encoded_loan_data.dtypes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>credit.policy                   int64
int.rate                      float64
installment                   float64
log.annual.inc                float64
dti                           float64
fico                            int64
days.with.cr.line             float64
revol.bal                       int64
revol.util                    float64
inq.last.6mths                  int64
delinq.2yrs                     int64
pub.rec                         int64
not.fully.paid                  int64
purpose_credit_card              bool
purpose_debt_consolidation       bool
purpose_educational              bool
purpose_home_improvement         bool
purpose_major_purchase           bool
purpose_small_business           bool
dtype: object</code></pre>
</div>
</div>
</section>
<section id="separate-data-into-train-and-test" class="level2">
<h2 class="anchored" data-anchor-id="separate-data-into-train-and-test">Separate data into train and test</h2>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> encoded_loan_data.drop(<span class="st">'not.fully.paid'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> encoded_loan_data[<span class="st">'not.fully.paid'</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                                           stratify <span class="op">=</span> y, random_state<span class="op">=</span><span class="dv">2022</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="application-of-the-sampling-strategies" class="level2">
<h2 class="anchored" data-anchor-id="application-of-the-sampling-strategies">Application of the Sampling Strategies</h2>
<p>We will explore two sampling strategies here: random undersampling, and SMOTE oversampling.</p>
</section>
<section id="random-undersampling" class="level2">
<h2 class="anchored" data-anchor-id="random-undersampling">Random Undersampling</h2>
<p>We will undersample the majority class, which corresponds to the “fully paid” (class 0).</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X_train_cp <span class="op">=</span> X_train.copy()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">=</span> y_train</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y_0 <span class="op">=</span> X_train_cp[X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>y_1 <span class="op">=</span> X_train_cp[X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>y_0_undersample <span class="op">=</span> y_0.sample(y_1.shape[<span class="dv">0</span>])</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>loan_data_undersample <span class="op">=</span> pd.concat([y_0_undersample, y_1], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(loan_data_undersample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="481" height="463"></p>
</div>
</div>
</section>
<section id="smote-oversampling" class="level2">
<h2 class="anchored" data-anchor-id="smote-oversampling">SMOTE Oversampling</h2>
<p>Perform oversampling on the minority class</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(sampling_strategy<span class="op">=</span><span class="st">'minority'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>X_train_SMOTE, y_train_SMOTE <span class="op">=</span> smote.fit_resample(X_train,y_train)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(y_train_SMOTE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\Lenovo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning:

Could not find the number of physical cores for the following reason:
[WinError 2] The system cannot find the file specified
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.

  File "C:\Users\Lenovo\AppData\Local\Programs\Python\Python312\Lib\site-packages\joblib\externals\loky\backend\context.py", line 257, in _count_physical_cores
    cpu_info = subprocess.run(
               ^^^^^^^^^^^^^^^
  File "C:\Users\Lenovo\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Lenovo\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "C:\Users\Lenovo\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 1538, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-2.png" width="481" height="463"></p>
</div>
</div>
<p>After applying the sampling strategies, we observe that the dataset is equally distributed across the different types of borrowers.</p>
</section>
<section id="application-of-some-machine-learning-classification-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="application-of-some-machine-learning-classification-algorithms">Application of Some Machine Learning Classification Algorithms</h2>
<p>This section will apply these two classification algorithms to the SMOTE smote sampled dataset. The same training approach can be applied to undersampled data as well.</p>
</section>
<section id="logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h2>
<p>This is an explainable algorithm. It classifies a data point by modeling its probability of belonging to a given class using the sigmoid function.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> loan_data_undersample.drop(<span class="st">'not.fully.paid'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> loan_data_undersample[<span class="st">'not.fully.paid'</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, stratify <span class="op">=</span> y, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>logistic_classifier <span class="op">=</span> LogisticRegression()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>logistic_classifier.fit(X_train, y_train)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_classifier.predict(X_test)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test,y_pred))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test,y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[104  57]
 [ 74  87]]
              precision    recall  f1-score   support

           0       0.58      0.65      0.61       161
           1       0.60      0.54      0.57       161

    accuracy                           0.59       322
   macro avg       0.59      0.59      0.59       322
weighted avg       0.59      0.59      0.59       322
</code></pre>
</div>
</div>
<p>These results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data.</p>
</section>
<section id="support-vector-machines" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines">Support Vector Machines</h2>
<p>This algorithm can be used for both classification and regression. It learns to draw the hyperplane (decision boundary) by using the margin to maximization principle. This decision boundary is drawn through the two closest support vectors.</p>
<p>SVM provides a transformation strategy called kernel tricks used to project non-learner separable data onto a higher dimension space to make them linearly separable.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>svc_classifier <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>svc_classifier.fit(X_train, y_train)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Make Prediction &amp; print the result</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svc_classifier.predict(X_test)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test,y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.60      0.49      0.54       161
           1       0.57      0.67      0.62       161

    accuracy                           0.58       322
   macro avg       0.58      0.58      0.58       322
weighted avg       0.58      0.58      0.58       322
</code></pre>
</div>
</div>
<p>These results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In summary, this blog has thoroughly explored the fundamental aspects of classification in machine learning, offering insights into its diverse applications across various fields. Additionally, it delved into the practical implementation of key machine learning models like Logistic Regression and Support Vector Machine, highlighting their application in scenarios involving both undersampling and SMOTE oversampling techniques to achieve a balanced dataset for model training.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Samheeta"</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-12-6"</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Classification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples."</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>**Contents:**</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Introduction to Classification.</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Different types of classification.</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Example of Linear Regression with <span class="co">[</span><span class="ot">Loan dataset</span><span class="co">](https://www.kaggle.com/datasets/itssuru/loan-data/)</span>.</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Data Visualization</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Data processing</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Model implementation</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Evaluation metrics implementation</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Classification</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>In recent times, various industries are grappling with extremely large and diverse datasets. Processing this data manually is not only time-consuming but may also lack long-term value. To enhance return on investment, a range of strategies are being employed, from basic automation to advanced machine learning techniques. This blog will focus on a key concept in this field: classification in machine learning.</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>We'll begin by explaining the essence of classification within the realm of Machine Learning, followed by a discussion on the two types of learners in machine learning, as well as distinguishing between classification and regression. Next, the blog will explore various real-life applications of classification. Subsequently, we'll delve into the different kinds of classification methods and examine some specific examples of classification algorithms. To conclude, the blog will offer practical experience in implementing several of these algorithms.</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Defining Classification in Machine Learning</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>Classification is a type of supervised learning in machine learning where the goal is to accurately predict the label of given input data. In this process, the model is thoroughly trained with training data and then evaluated using test data before it is applied to predict labels for new, unseen data.</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>For example, a machine learning algorithm can be trained to determine whether an email is spam or not (ham). However, before delving into the nuances of classification, it's essential to understand the difference between two classifications of learners: lazy and eager learners, and to clear up common confusions between classification and regression.</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Machine Learning Algorithms for Classification (src: &lt;https://www.datacamp.com/blog/classification-machine-learning&gt;)</span><span class="co">]</span>(classification(Img1).png){}</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>Lazy Learners vs. Eager Learners:</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>In the realm of machine learning classification, there are two distinct types of learners: lazy and eager learners.</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>Eager learners are those algorithms that construct a model based on the training dataset before making predictions on new data. These algorithms invest more time in the training phase to better generalize from the data by learning the weights, but they are quicker in making predictions. Examples of eager learners include:</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>Logistic Regression.</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>Support Vector Machine.</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>Decision Trees.</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>Artificial Neural Networks.</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>Conversely, lazy learners, or instance-based learners, do not immediately generate a model from the training data. This is where the 'lazy' aspect comes in. They store the training data and, for each prediction, they search the entire training set for the nearest neighbor, resulting in slower prediction times. Examples of lazy learners are:</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>K-Nearest Neighbor.</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>Case-based reasoning.</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>However, there are techniques like BallTrees and KDTrees that can enhance prediction speed in these algorithms.</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Examples of Machine Learning Classification in Real Life </span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>Supervised Machine Learning Classification has different applications in multiple domains of our day-to-day life. Below are some examples. </span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>**Application in Healthcare :**</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>Supervised machine learning classification plays a significant role in various aspects of everyday life. Here are some key examples, particularly in the healthcare domain. </span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>During the COVID-19 pandemic, supervised machine learning models were crucial in predicting whether individuals were infected with the virus </span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Machine learning models are also utilized by researchers to forecast the likelihood of new diseases emerging in the future.</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>**Education :**</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>Utilizing machine learning for categorizing various documents like text, video, and audio.</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatically determining the language of students' application documents.</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Analyzing students' feedback about professors to gauge sentiments.</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>**Transportation :**</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predicting areas likely to experience increased traffic volume.</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Anticipating potential transportation problems in specific areas due to weather conditions.</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a>**Sustainable Agriculture :**</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using classification models to identify the most suitable land types for different seeds.</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Forecasting weather conditions to aid farmers in taking appropriate preventive actions.</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## Different Types of Classification Tasks in Machine Learning </span></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a>There are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications. </span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Binary Classification**: In a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat. Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification. </span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Multi-Class Classification**: The multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. In the following case, the model correctly classified the image to be a plane. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include but are not limited to:</span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">Naive Bayes</span><span class="co">](https://monkeylearn.com/blog/classification-algorithms/#naive-bayes)</span>: Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It's particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.</span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">K-Nearest Neighbors</span><span class="co">](https://monkeylearn.com/blog/classification-algorithms/#knn)</span>: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It's versatile and can be applied to various types of data, but the choice of k is crucial for its performance.</span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">Support Vector Machines</span><span class="co">](https://monkeylearn.com/blog/classification-algorithms/#svm)</span>: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data.</span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">Logistic Regression</span><span class="co">](https://monkeylearn.com/blog/classification-algorithms/#logistic-regression)</span>: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It's simple, interpretable, and effective for binary and multiclass classification tasks.</span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Multi-Label Classification**: In multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. Such a scenario can be observed in different domains, such as auto-tagging in Natural Language Processing, where a given text can contain multiple topics. Similarly to computer vision, an image can contain multiple objects, as illustrated below: the model predicted that the image contains: a plane, a boat, a truck, and a dog. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification. We can cite: </span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Multi-label Decision Trees</span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Multi-label Gradient Boosting</span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Multi-label Random Forests</span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Imbalanced Classification**: For the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data. Let’s consider the following 3-class classification scenario where the training data contains: 60% of trucks, 25% of planes, and 15% of boats.The imbalanced classification problem could occur in the following scenario:</span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fraudulent transaction detections in financial industries</span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rare disease diagnosis </span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Customer churn analysis</span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a>Using conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise. </span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a>So, does that mean that such problems are left behind?</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>Of course not! We can use multiple approaches to tackle the imbalance problem in a dataset. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms. </span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sampling Techniques** </span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a>These techniques aim to balance the distribution of the original by: </span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cluster-based Oversampling:</span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Random undersampling: random elimination of examples from the majority class. </span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SMOTE Oversampling: random replication of examples from the minority class. </span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cost-Sensitive Algorithms** </span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a>These algorithms take into consideration the cost of misclassification. They aim to minimize the total cost generated by the models.</span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cost-sensitive Decision Trees.</span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cost-sensitive Logistic Regression. </span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cost-sensitive Support Vector Machines.</span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Distribution of Loans in the Dataset </span></span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Look at the first five observations in the dataset. </span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a>loan_data <span class="op">=</span> pd.read_csv(<span class="st">"loan_data.csv"</span>)</span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a>loan_data.head()</span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Borrowers profile in the dataset. </span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function for data distribution</span></span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_loan_distrib(data):</span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a>  count <span class="op">=</span> <span class="st">""</span></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">isinstance</span>(data, pd.DataFrame):</span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a>      count <span class="op">=</span> data[<span class="st">"not.fully.paid"</span>].value_counts()</span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a>      count <span class="op">=</span> data.value_counts()</span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a>  count.plot(kind <span class="op">=</span> <span class="st">'pie'</span>, explode <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.1</span>], </span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-164"><a href="#cb15-164" aria-hidden="true" tabindex="-1"></a>              figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">6</span>), autopct <span class="op">=</span> <span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>, shadow <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">"Loan: Fully Paid Vs. Not Fully Paid"</span>)</span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a>  plt.legend([<span class="st">"Fully Paid"</span>, <span class="st">"Not Fully Paid"</span>])</span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(loan_data)</span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a>From the graphic above, we notice that 84% of the borrowers paid their loans back, and only 16% didn’t pay them back, which makes the dataset really imbalanced.</span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Variable Types**</span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a>Before further, we need to check the variables’ type so that we can encode those that need to be encoded. </span>
<span id="cb15-179"><a href="#cb15-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a>We notice that all the columns are continuous variables, except the purpose attribute, which needs to be encoded. </span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a><span class="co"># Check column types</span></span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loan_data.dtypes)</span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a>encoded_loan_data <span class="op">=</span> pd.get_dummies(loan_data, prefix<span class="op">=</span><span class="st">"purpose"</span>,   </span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a>                                   drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(encoded_loan_data.dtypes)</span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a><span class="fu">## Separate data into train and test</span></span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> encoded_loan_data.drop(<span class="st">'not.fully.paid'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> encoded_loan_data[<span class="st">'not.fully.paid'</span>]</span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a>                                           stratify <span class="op">=</span> y, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application of the Sampling Strategies </span></span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a>We will explore two sampling strategies here: random undersampling, and SMOTE oversampling.</span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Undersampling </span></span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a>We will undersample the majority class, which corresponds to the “fully paid” (class 0). </span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a>X_train_cp <span class="op">=</span> X_train.copy()</span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a>X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">=</span> y_train</span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a>y_0 <span class="op">=</span> X_train_cp[X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a>y_1 <span class="op">=</span> X_train_cp[X_train_cp[<span class="st">'not.fully.paid'</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb15-227"><a href="#cb15-227" aria-hidden="true" tabindex="-1"></a>y_0_undersample <span class="op">=</span> y_0.sample(y_1.shape[<span class="dv">0</span>])</span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a>loan_data_undersample <span class="op">=</span> pd.concat([y_0_undersample, y_1], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(loan_data_undersample)</span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## SMOTE Oversampling</span></span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a>Perform oversampling on the minority class</span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb15-242"><a href="#cb15-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(sampling_strategy<span class="op">=</span><span class="st">'minority'</span>)</span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a>X_train_SMOTE, y_train_SMOTE <span class="op">=</span> smote.fit_resample(X_train,y_train)</span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the proportion of borrowers</span></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a>show_loan_distrib(y_train_SMOTE)</span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a>After applying the sampling strategies, we observe that the dataset is equally distributed across the different types of borrowers.</span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application of Some Machine Learning Classification Algorithms</span></span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a>This section will apply these two classification algorithms to the SMOTE smote sampled dataset. The same training approach can be applied to undersampled data as well. </span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## Logistic Regression </span></span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a>This is an explainable algorithm. It classifies a data point by modeling its probability of belonging to a given class using the sigmoid function. </span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> loan_data_undersample.drop(<span class="st">'not.fully.paid'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> loan_data_undersample[<span class="st">'not.fully.paid'</span>]</span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, stratify <span class="op">=</span> y, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>logistic_classifier <span class="op">=</span> LogisticRegression()</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a>logistic_classifier.fit(X_train, y_train)</span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_classifier.predict(X_test)</span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test,y_pred))</span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test,y_pred))</span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a>These results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data. </span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a><span class="fu">## Support Vector Machines </span></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a>This algorithm can be used for both classification and regression. It learns to draw the hyperplane (decision boundary) by using the margin to maximization principle. This decision boundary is drawn through the two closest support vectors. </span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a>SVM provides a transformation strategy called kernel tricks used to project non-learner separable data onto a higher dimension space to make them linearly separable. </span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a>svc_classifier <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>)</span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a>svc_classifier.fit(X_train, y_train)</span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-288"><a href="#cb15-288" aria-hidden="true" tabindex="-1"></a><span class="co"># Make Prediction &amp; print the result</span></span>
<span id="cb15-289"><a href="#cb15-289" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svc_classifier.predict(X_test)</span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test,y_pred))</span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a>These results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data. </span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a>In summary, this blog has thoroughly explored the fundamental aspects of classification in machine learning, offering insights into its diverse applications across various fields. Additionally, it delved into the practical implementation of key machine learning models like Logistic Regression and Support Vector Machine, highlighting their application in scenarios involving both undersampling and SMOTE oversampling techniques to achieve a balanced dataset for model training.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>