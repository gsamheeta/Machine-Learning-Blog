[
  {
    "objectID": "posts/linregression/index.html",
    "href": "posts/linregression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Contents:\n\nWhat Is Linear Regression?\nExample of Linear Regression with Real estate price prediction dataset.\nData Exploration\nModel Training\nModel Evaluation\nResidual Analysis"
  },
  {
    "objectID": "posts/linregression/index.html#import-library-and-dataset",
    "href": "posts/linregression/index.html#import-library-and-dataset",
    "title": "Linear Regression",
    "section": "Import Library and Dataset",
    "text": "Import Library and Dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline\n\n\ndf=pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/linregression/Real estate.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\n0\n1\n2012.917\n32.0\n84.87882\n10\n24.98298\n121.54024\n37.9\n\n\n1\n2\n2012.917\n19.5\n306.59470\n9\n24.98034\n121.53951\n42.2\n\n\n2\n3\n2013.583\n13.3\n561.98450\n5\n24.98746\n121.54391\n47.3\n\n\n3\n4\n2013.500\n13.3\n561.98450\n5\n24.98746\n121.54391\n54.8\n\n\n4\n5\n2012.833\n5.0\n390.56840\n5\n24.97937\n121.54245\n43.1\n\n\n\n\n\n\n\n\ndf.shape\n\n(414, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 414 entries, 0 to 413\nData columns (total 8 columns):\n #   Column                                  Non-Null Count  Dtype  \n---  ------                                  --------------  -----  \n 0   No                                      414 non-null    int64  \n 1   X1 transaction date                     414 non-null    float64\n 2   X2 house age                            414 non-null    float64\n 3   X3 distance to the nearest MRT station  414 non-null    float64\n 4   X4 number of convenience stores         414 non-null    int64  \n 5   X5 latitude                             414 non-null    float64\n 6   X6 longitude                            414 non-null    float64\n 7   Y house price of unit area              414 non-null    float64\ndtypes: float64(6), int64(2)\nmemory usage: 26.0 KB\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\nNo\n1.000000\n-0.048658\n-0.032808\n-0.013573\n-0.012699\n-0.010110\n-0.011059\n-0.028587\n\n\nX1 transaction date\n-0.048658\n1.000000\n0.017549\n0.060880\n0.009635\n0.035058\n-0.041082\n0.087491\n\n\nX2 house age\n-0.032808\n0.017549\n1.000000\n0.025622\n0.049593\n0.054420\n-0.048520\n-0.210567\n\n\nX3 distance to the nearest MRT station\n-0.013573\n0.060880\n0.025622\n1.000000\n-0.602519\n-0.591067\n-0.806317\n-0.673613\n\n\nX4 number of convenience stores\n-0.012699\n0.009635\n0.049593\n-0.602519\n1.000000\n0.444143\n0.449099\n0.571005\n\n\nX5 latitude\n-0.010110\n0.035058\n0.054420\n-0.591067\n0.444143\n1.000000\n0.412924\n0.546307\n\n\nX6 longitude\n-0.011059\n-0.041082\n-0.048520\n-0.806317\n0.449099\n0.412924\n1.000000\n0.523287\n\n\nY house price of unit area\n-0.028587\n0.087491\n-0.210567\n-0.673613\n0.571005\n0.546307\n0.523287\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True,cmap='Reds')\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/linregression/index.html#data-preprocessing",
    "href": "posts/linregression/index.html#data-preprocessing",
    "title": "Linear Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nData preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\nLabel Encoding: This method involves converting categorical labels into numerical values to enable algorithms to work with them.\nOne-Hot Encoding: One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\nDummy Variable Trap: This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called get_dummies. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like ‘sex,’ ‘children,’ ‘smoker,’ and ‘region.’ By setting the drop_first=True parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function."
  },
  {
    "objectID": "posts/linregression/index.html#train-test-split",
    "href": "posts/linregression/index.html#train-test-split",
    "title": "Linear Regression",
    "section": "Train Test split",
    "text": "Train Test split\n\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n\nThe parameter obtained from both the model are same.So we successfully build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluation.\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982026\nR square obtain for normal equation method is : 0.7795687545055303\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322981898\nR square obtain for scikit learn library is : 0.7795687545055318\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\nLinear Relationship: Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\nNormality of Residuals: The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\nMean of Residuals: The mean of the residual errors should ideally be close to 0.\nMultivariate Normality: Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\nMulticollinearity: Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\nHomoscedasticity: The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n# Check for Linearity\nf = plt.figure(figsize=(9,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(9,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911135\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\nThe actual vs. predicted plot doesn’t form a linear pattern, indicating a failure of the linear assumption.\nThe mean of the residuals is close to zero, and the residual error plot is skewed to the right.\nThe Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\nThe plot exhibits heteroscedasticity, with errors increasing after a certain point.\nThe variance inflation factor is less than 5, indicating the absence of multicollinearity."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/classification/index.html#introduction-to-classification",
    "href": "posts/classification/index.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to Classification",
    "text": "Introduction to Classification\nIn recent times, various industries are grappling with extremely large and diverse datasets. Processing this data manually is not only time-consuming but may also lack long-term value. To enhance return on investment, a range of strategies are being employed, from basic automation to advanced machine learning techniques. This blog will focus on a key concept in this field: classification in machine learning.\nWe’ll begin by explaining the essence of classification within the realm of Machine Learning, followed by a discussion on the two types of learners in machine learning, as well as distinguishing between classification and regression. Next, the blog will explore various real-life applications of classification. Subsequently, we’ll delve into the different kinds of classification methods and examine some specific examples of classification algorithms. To conclude, the blog will offer practical experience in implementing several of these algorithms."
  },
  {
    "objectID": "posts/classification/index.html#defining-classification-in-machine-learning",
    "href": "posts/classification/index.html#defining-classification-in-machine-learning",
    "title": "Classification",
    "section": "Defining Classification in Machine Learning",
    "text": "Defining Classification in Machine Learning\nClassification is a type of supervised learning in machine learning where the goal is to accurately predict the label of given input data. In this process, the model is thoroughly trained with training data and then evaluated using test data before it is applied to predict labels for new, unseen data.\nFor example, a machine learning algorithm can be trained to determine whether an email is spam or not (ham). However, before delving into the nuances of classification, it’s essential to understand the difference between two classifications of learners: lazy and eager learners, and to clear up common confusions between classification and regression.  Lazy Learners vs. Eager Learners: In the realm of machine learning classification, there are two distinct types of learners: lazy and eager learners.\nEager learners are those algorithms that construct a model based on the training dataset before making predictions on new data. These algorithms invest more time in the training phase to better generalize from the data by learning the weights, but they are quicker in making predictions. Examples of eager learners include:\nLogistic Regression. Support Vector Machine. Decision Trees. Artificial Neural Networks. Conversely, lazy learners, or instance-based learners, do not immediately generate a model from the training data. This is where the ‘lazy’ aspect comes in. They store the training data and, for each prediction, they search the entire training set for the nearest neighbor, resulting in slower prediction times. Examples of lazy learners are:\nK-Nearest Neighbor. Case-based reasoning. However, there are techniques like BallTrees and KDTrees that can enhance prediction speed in these algorithms."
  },
  {
    "objectID": "posts/classification/index.html#the-power-of-predefined-categories",
    "href": "posts/classification/index.html#the-power-of-predefined-categories",
    "title": "Classification",
    "section": "The Power of Predefined Categories",
    "text": "The Power of Predefined Categories\nThe magic of classification lies in these predefined categories. Think of them as labels, such as “spam” and “non-spam” for emails, “fraudulent” and “legitimate” for financial transactions, or “cat” and “dog” for image recognition. The ability to organize data into these categories enables decision-making, automation, and insights that would otherwise be impractical or impossible to achieve manually."
  },
  {
    "objectID": "posts/classification/index.html#how-classification-works",
    "href": "posts/classification/index.html#how-classification-works",
    "title": "Classification",
    "section": "How Classification Works",
    "text": "How Classification Works\nTo perform classification, machine learning algorithms need to learn from data first. This “training” phase involves feeding the algorithm a labeled dataset, where each data point is associated with the category it belongs to. The algorithm then learns the patterns, relationships, and features that characterize each category.\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences."
  },
  {
    "objectID": "posts/classification/index.html#real-life-applications",
    "href": "posts/classification/index.html#real-life-applications",
    "title": "Classification",
    "section": "Real-Life Applications",
    "text": "Real-Life Applications\nClassification has found its way into countless real-world applications. From medical diagnoses to recommendation systems, here are a few examples:\n\nMedical Diagnoses: Doctors use machine learning models to predict whether a patient has a particular disease based on symptoms, medical history, and test results.\nRecommendation Systems: Companies like Netflix and Amazon employ classification to recommend movies or products to users based on their preferences and behavior.\nSentiment Analysis: Social media platforms analyze posts to classify them as positive, negative, or neutral, providing valuable insights into public opinion.\nImage Recognition: In the field of computer vision, classification helps identify objects, animals, or handwritten text in images.\n\n\nPopular Classification Algorithms:\n\nLogistic Regression: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.\nNaive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\nK-Nearest Neighbors: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\nDecision Tree: Decision tree classifiers make decisions by splitting data based on features, creating a tree-like structure of decisions. They are interpretable and can handle both categorical and numerical data, making them useful in many applications.\nSupport Vector Machines: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data."
  },
  {
    "objectID": "posts/classification/index.html#example-heart-disease-prediction",
    "href": "posts/classification/index.html#example-heart-disease-prediction",
    "title": "Classification",
    "section": "Example: Heart Disease Prediction",
    "text": "Example: Heart Disease Prediction\n\nimport pandas as pd\nloan_data = pd.read_csv(\"loan_data.csv\")\nloan_data.head()\n\n\n\n\n\n\n\n\ncredit.policy\npurpose\nint.rate\ninstallment\nlog.annual.inc\ndti\nfico\ndays.with.cr.line\nrevol.bal\nrevol.util\ninq.last.6mths\ndelinq.2yrs\npub.rec\nnot.fully.paid\n\n\n\n\n0\n1\ndebt_consolidation\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0\n0\n0\n0\n\n\n1\n1\ncredit_card\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0\n0\n0\n0\n\n\n2\n1\ndebt_consolidation\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1\n0\n0\n0\n\n\n3\n1\ndebt_consolidation\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1\n0\n0\n0\n\n\n4\n1\ncredit_card\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0\n1\n0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n# Helper function for data distribution\n# Visualize the proportion of borrowers\ndef show_loan_distrib(data):\n  count = \"\"\n  if isinstance(data, pd.DataFrame):\n      count = data[\"not.fully.paid\"].value_counts()\n  else:\n      count = data.value_counts()\n\n\n  count.plot(kind = 'pie', explode = [0, 0.1], \n\n              figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n  plt.ylabel(\"Loan: Fully Paid Vs. Not Fully Paid\")\n  plt.legend([\"Fully Paid\", \"Not Fully Paid\"])\n  plt.show()\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data)\n\n\n\n\n\n# Check column types\nprint(loan_data.dtypes)\n\ncredit.policy          int64\npurpose               object\nint.rate             float64\ninstallment          float64\nlog.annual.inc       float64\ndti                  float64\nfico                   int64\ndays.with.cr.line    float64\nrevol.bal              int64\nrevol.util           float64\ninq.last.6mths         int64\ndelinq.2yrs            int64\npub.rec                int64\nnot.fully.paid         int64\ndtype: object\n\n\n\nencoded_loan_data = pd.get_dummies(loan_data, prefix=\"purpose\",   \n\n                                   drop_first=True)\nprint(encoded_loan_data.dtypes)\n\ncredit.policy                   int64\nint.rate                      float64\ninstallment                   float64\nlog.annual.inc                float64\ndti                           float64\nfico                            int64\ndays.with.cr.line             float64\nrevol.bal                       int64\nrevol.util                    float64\ninq.last.6mths                  int64\ndelinq.2yrs                     int64\npub.rec                         int64\nnot.fully.paid                  int64\npurpose_credit_card              bool\npurpose_debt_consolidation       bool\npurpose_educational              bool\npurpose_home_improvement         bool\npurpose_major_purchase           bool\npurpose_small_business           bool\ndtype: object\n\n\nAttributes:\n\nAge: Age of the patient [years]\nSex: Sex of the patient [M: Male, F: Female]\nChestPainType: Chest Pain Type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\nRestingBP: Resting blood pressure [mm Hg]\nCholesterol: Serum cholesterol [mm/dl]\nFastingBS: Fasting blood sugar [1: if FastingBS &gt; 120 mg/dl, 0: otherwise]\nRestingECG: Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes’ criteria]\nMaxHR: Maximum heart rate achieved [Numeric value between 60 and 202]\nExerciseAngina: Exercise-induced angina [Y: Yes, N: No]\nOldpeak: Oldpeak = ST [Numeric value measured in depression]\nST_Slope: The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\nHeartDisease: Output class [1: heart disease, 0: Normal]\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = encoded_loan_data.drop('not.fully.paid', axis = 1)\ny = encoded_loan_data['not.fully.paid']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, \n\n                                           stratify = y, random_state=2022)\n\n\nX_train_cp = X_train.copy()\nX_train_cp['not.fully.paid'] = y_train\ny_0 = X_train_cp[X_train_cp['not.fully.paid'] == 0]\ny_1 = X_train_cp[X_train_cp['not.fully.paid'] == 1]\ny_0_undersample = y_0.sample(y_1.shape[0])\nloan_data_undersample = pd.concat([y_0_undersample, y_1], axis = 0)\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data_undersample)\n\n\n\n\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_train_SMOTE, y_train_SMOTE = smote.fit_resample(X_train,y_train)\n# Visualize the proportion of borrowers\nshow_loan_distrib(y_train_SMOTE)\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nX = loan_data_undersample.drop('not.fully.paid', axis = 1)\ny = loan_data_undersample['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state=2022)\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(X_train, y_train)\ny_pred = logistic_classifier.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n[[91 70]\n [67 94]]\n              precision    recall  f1-score   support\n\n           0       0.58      0.57      0.57       161\n           1       0.57      0.58      0.58       161\n\n    accuracy                           0.57       322\n   macro avg       0.57      0.57      0.57       322\nweighted avg       0.57      0.57      0.57       322\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
  },
  {
    "objectID": "posts/outlier-detection/index.html",
    "href": "posts/outlier-detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Contents:\n\nIntroduction to Anomaly or Outlier Detection.\nExample of Anomaly or Outlier Detection manually created data.\nExample of Anomaly or Outlier Detection with real data Credit Card dataset.\nData Visualization\nData processing\nAnomaly Detection\nClustering Model implementation\nEvaluation metrics implementation\n\n\nAnomaly or Outlier Detection\nAnomaly Detection involves identifying uncommon occurrences within a dataset that stand out as statistically different from the majority of observations. These anomalies often indicate potential problems such as credit card fraud, server malfunctions, or cyber attacks.\nAnomalies fall into three primary categories:\n\nPoint Anomaly: This occurs when a data point significantly deviates from the rest of the dataset.\nContextual Anomaly: An observation is flagged as a Contextual Anomaly due to its abnormality within a specific context.\nCollective Anomaly: This involves a group of data instances that collectively indicate an anomaly.\n\nMachine Learning concepts are employed to perform anomaly detection using various approaches:\n\nSupervised Anomaly Detection: This method relies on labeled datasets containing both normal and anomalous samples. Predictive models, such as supervised Neural Networks, Support Vector Machines, or K-Nearest Neighbors Classifiers, are utilized to classify future data points.\nUnsupervised Anomaly Detection: This approach doesn’t require labeled training data. It operates on the assumptions that only a small fraction of the data is anomalous and that anomalies significantly differ from normal samples. Unsupervised methods cluster data based on similarity measures, identifying data points that fall far from the established clusters as anomalies.\n\nAnomaly Detection with manually generated data:\n\nimport numpy as np \nfrom scipy import stats \nimport matplotlib.pyplot as plt \nimport matplotlib.font_manager \nfrom pyod.models.knn import KNN  \nfrom pyod.utils.data import generate_data, get_outliers_inliers \n\n\n# generating a random dataset with two features \nX_train, y_train = generate_data(n_train = 300, train_only = True, \n                                                   n_features = 2) \n  \n# Setting the percentage of outliers \noutlier_fraction = 0.1\n  \n# Storing the outliers and inliners in different numpy arrays \nX_outliers, X_inliers = get_outliers_inliers(X_train, y_train) \nn_inliers = len(X_inliers) \nn_outliers = len(X_outliers) \n  \n# Separating the two features \nf1 = X_train[:, [0]].reshape(-1, 1) \nf2 = X_train[:, [1]].reshape(-1, 1) \n\n\nxx, yy = np.meshgrid(np.linspace(-10, 10, 200), \n                     np.linspace(-10, 10, 200)) \n  \n# scatter plot \nplt.scatter(f1, f2) \nplt.xlabel('Feature 1') \nplt.ylabel('Feature 2') \n\nText(0, 0.5, 'Feature 2')\n\n\n\n\n\n\nclf = KNN(contamination = outlier_fraction) \nclf.fit(X_train, y_train) \n  \n# You can print this to see all the prediction scores \nscores_pred = clf.decision_function(X_train)*-1\n  \ny_pred = clf.predict(X_train) \nn_errors = (y_pred != y_train).sum() \n# Counting the number of errors \n  \nprint('The number of prediction errors are ' + str(n_errors)) \n\nThe number of prediction errors are 8\n\n\n/home/tpriya/CS5525/MLBlog/env/lib/python3.10/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n  warnings.warn(\n\n\n\nthreshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction) \n  \n# decision function calculates the raw  \n# anomaly score for every point \nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\nZ = Z.reshape(xx.shape) \n  \n# fill blue colormap from minimum anomaly \n# score to threshold value \nsubplot = plt.subplot(1, 2, 1) \nsubplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),  \n                  threshold, 10), cmap = plt.cm.Blues_r) \n  \n# draw red contour line where anomaly  \n# score is equal to threshold \na = subplot.contour(xx, yy, Z, levels =[threshold], \n                     linewidths = 2, colors ='red') \n  \n# fill orange contour lines where range of anomaly \n# score is from threshold to maximum anomaly score \nsubplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange') \n  \n# scatter plot of inliers with white dots \nb = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1], \n                                    c ='white', s = 20, edgecolor ='k')  \n  \n# scatter plot of outliers with black dots \nc = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],  \n                                    c ='black', s = 20, edgecolor ='k') \nsubplot.axis('tight') \n  \nsubplot.legend( \n    [a.collections[0], b, c], \n    ['learned decision function', 'true inliers', 'true outliers'], \n    prop = matplotlib.font_manager.FontProperties(size = 10), \n    loc ='lower right') \n  \nsubplot.set_title('K-Nearest Neighbours') \nsubplot.set_xlim((-10, 10)) \nsubplot.set_ylim((-10, 10)) \nplt.show()  \n\n/tmp/ipykernel_25173/1166639198.py:33: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n  [a.collections[0], b, c],\n\n\n\n\n\nAnomaly Detection with real data:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.ensemble import IsolationForest\n\n\nfrom sklearn.metrics import silhouette_score\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/CC GENERAL.csv')\n\nprint('The shape of the dataset is:', df.shape)\n\nThe shape of the dataset is: (8950, 18)\n\n\n\n# check number of nulls in each column \ndf.isnull().sum().sort_values(ascending=False)\n# konw the ratio of null in each column \nround(df.isnull().sum(axis=0)*100/df.shape[0],2).sort_values(ascending=False)\n\nMINIMUM_PAYMENTS                    3.50\nCREDIT_LIMIT                        0.01\nCUST_ID                             0.00\nBALANCE                             0.00\nPRC_FULL_PAYMENT                    0.00\nPAYMENTS                            0.00\nPURCHASES_TRX                       0.00\nCASH_ADVANCE_TRX                    0.00\nCASH_ADVANCE_FREQUENCY              0.00\nPURCHASES_INSTALLMENTS_FREQUENCY    0.00\nONEOFF_PURCHASES_FREQUENCY          0.00\nPURCHASES_FREQUENCY                 0.00\nCASH_ADVANCE                        0.00\nINSTALLMENTS_PURCHASES              0.00\nONEOFF_PURCHASES                    0.00\nPURCHASES                           0.00\nBALANCE_FREQUENCY                   0.00\nTENURE                              0.00\ndtype: float64\n\n\n\n# save numeric columns and objects in separeted list to handle each one of them\nnumeric_columns = df.select_dtypes(exclude=['object']).columns.to_list() \nobject_columns = df.select_dtypes(include=['object']).columns.to_list()\n\n\ndf[numeric_columns].hist(bins=15, figsize=(20,15))\n\narray([[&lt;Axes: title={'center': 'BALANCE'}&gt;,\n        &lt;Axes: title={'center': 'BALANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES'}&gt;],\n       [&lt;Axes: title={'center': 'INSTALLMENTS_PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES_FREQUENCY'}&gt;],\n       [&lt;Axes: title={'center': 'PURCHASES_INSTALLMENTS_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_TRX'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_TRX'}&gt;],\n       [&lt;Axes: title={'center': 'CREDIT_LIMIT'}&gt;,\n        &lt;Axes: title={'center': 'PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'MINIMUM_PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'PRC_FULL_PAYMENT'}&gt;],\n       [&lt;Axes: title={'center': 'TENURE'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\nOutliers:\n\nplt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(numeric_columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df[column], orient='h')\n\n\n\n\nData Preprocessing:\n\ndf_pre=df.copy()\ndf_pre.drop(object_columns, axis=1, inplace=True)\ndf_pre.head(3)\ncolumns_names = list(df_pre.columns)\ncolumns_names\n\n['BALANCE',\n 'BALANCE_FREQUENCY',\n 'PURCHASES',\n 'ONEOFF_PURCHASES',\n 'INSTALLMENTS_PURCHASES',\n 'CASH_ADVANCE',\n 'PURCHASES_FREQUENCY',\n 'ONEOFF_PURCHASES_FREQUENCY',\n 'PURCHASES_INSTALLMENTS_FREQUENCY',\n 'CASH_ADVANCE_FREQUENCY',\n 'CASH_ADVANCE_TRX',\n 'PURCHASES_TRX',\n 'CREDIT_LIMIT',\n 'PAYMENTS',\n 'MINIMUM_PAYMENTS',\n 'PRC_FULL_PAYMENT',\n 'TENURE']\n\n\n\nfrom sklearn.impute import SimpleImputer\ndf_NoNull = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(df_pre), columns=columns_names)\ndf_NoNull \n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\n0\n40.900749\n0.818182\n95.40\n0.00\n95.40\n0.000000\n0.166667\n0.000000\n0.083333\n0.000000\n0.0\n2.0\n1000.0\n201.802084\n139.509787\n0.000000\n12.0\n\n\n1\n3202.467416\n0.909091\n0.00\n0.00\n0.00\n6442.945483\n0.000000\n0.000000\n0.000000\n0.250000\n4.0\n0.0\n7000.0\n4103.032597\n1072.340217\n0.222222\n12.0\n\n\n2\n2495.148862\n1.000000\n773.17\n773.17\n0.00\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.0\n12.0\n7500.0\n622.066742\n627.284787\n0.000000\n12.0\n\n\n3\n1666.670542\n0.636364\n1499.00\n1499.00\n0.00\n205.788017\n0.083333\n0.083333\n0.000000\n0.083333\n1.0\n1.0\n7500.0\n0.000000\n312.343947\n0.000000\n12.0\n\n\n4\n817.714335\n1.000000\n16.00\n16.00\n0.00\n0.000000\n0.083333\n0.083333\n0.000000\n0.000000\n0.0\n1.0\n1200.0\n678.334763\n244.791237\n0.000000\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n28.493517\n1.000000\n291.12\n0.00\n291.12\n0.000000\n1.000000\n0.000000\n0.833333\n0.000000\n0.0\n6.0\n1000.0\n325.594462\n48.886365\n0.500000\n6.0\n\n\n8946\n19.183215\n1.000000\n300.00\n0.00\n300.00\n0.000000\n1.000000\n0.000000\n0.833333\n0.000000\n0.0\n6.0\n1000.0\n275.861322\n312.343947\n0.000000\n6.0\n\n\n8947\n23.398673\n0.833333\n144.40\n0.00\n144.40\n0.000000\n0.833333\n0.000000\n0.666667\n0.000000\n0.0\n5.0\n1000.0\n81.270775\n82.418369\n0.250000\n6.0\n\n\n8948\n13.457564\n0.833333\n0.00\n0.00\n0.00\n36.558778\n0.000000\n0.000000\n0.000000\n0.166667\n2.0\n0.0\n500.0\n52.549959\n55.755628\n0.250000\n6.0\n\n\n8949\n372.708075\n0.666667\n1093.25\n1093.25\n0.00\n127.040008\n0.666667\n0.666667\n0.000000\n0.333333\n2.0\n23.0\n1200.0\n63.165404\n88.288956\n0.000000\n6.0\n\n\n\n\n8950 rows × 17 columns\n\n\n\nLog Transform for handling outliers:\n\n# will add 1 to all values because log transform get error for numbers between 0 and 1\ndf_pre2 = (df_NoNull + 1) \ndf_log = np.log(df_pre2)\ndf_log.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBALANCE\n8950.0\n6.161637\n2.013303\n0.000000\n4.861995\n6.773521\n7.628099\n9.854515\n\n\nBALANCE_FREQUENCY\n8950.0\n0.619940\n0.148590\n0.000000\n0.635989\n0.693147\n0.693147\n0.693147\n\n\nPURCHASES\n8950.0\n4.899647\n2.916872\n0.000000\n3.704627\n5.892417\n7.013133\n10.800403\n\n\nONEOFF_PURCHASES\n8950.0\n3.204274\n3.246365\n0.000000\n0.000000\n3.663562\n6.360274\n10.615512\n\n\nINSTALLMENTS_PURCHASES\n8950.0\n3.352403\n3.082973\n0.000000\n0.000000\n4.499810\n6.151961\n10.021315\n\n\nCASH_ADVANCE\n8950.0\n3.319086\n3.566298\n0.000000\n0.000000\n0.000000\n7.016449\n10.760839\n\n\nPURCHASES_FREQUENCY\n8950.0\n0.361268\n0.277317\n0.000000\n0.080042\n0.405465\n0.650588\n0.693147\n\n\nONEOFF_PURCHASES_FREQUENCY\n8950.0\n0.158699\n0.216672\n0.000000\n0.000000\n0.080042\n0.262364\n0.693147\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n8950.0\n0.270072\n0.281852\n0.000000\n0.000000\n0.154151\n0.559616\n0.693147\n\n\nCASH_ADVANCE_FREQUENCY\n8950.0\n0.113512\n0.156716\n0.000000\n0.000000\n0.000000\n0.200671\n0.916291\n\n\nCASH_ADVANCE_TRX\n8950.0\n0.817570\n1.009316\n0.000000\n0.000000\n0.000000\n1.609438\n4.820282\n\n\nPURCHASES_TRX\n8950.0\n1.894731\n1.373856\n0.000000\n0.693147\n2.079442\n2.890372\n5.883322\n\n\nCREDIT_LIMIT\n8950.0\n8.094825\n0.819629\n3.931826\n7.378384\n8.006701\n8.779711\n10.308986\n\n\nPAYMENTS\n8950.0\n6.624540\n1.591763\n0.000000\n5.951361\n6.754489\n7.550732\n10.834125\n\n\nMINIMUM_PAYMENTS\n8950.0\n5.916079\n1.169929\n0.018982\n5.146667\n5.747301\n6.671670\n11.243832\n\n\nPRC_FULL_PAYMENT\n8950.0\n0.117730\n0.211617\n0.000000\n0.000000\n0.000000\n0.133531\n0.693147\n\n\nTENURE\n8950.0\n2.519680\n0.130367\n1.945910\n2.564949\n2.564949\n2.564949\n2.564949\n\n\n\n\n\n\n\n\nf, axs = plt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(df_log.columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df_log[column], orient='h')\n\n\n\n\n\ndf_pre2 = df_NoNull.copy()\n\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer \ndf_power = PowerTransformer(method=\"yeo-johnson\").fit_transform(df_pre2)\n\n\ndf_power= pd.DataFrame(df_power, columns=columns_names)\ndf_power.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBALANCE\n8950.0\n-4.445854e-17\n1.000056\n-2.120305\n-0.816807\n0.151681\n0.717913\n2.731227\n\n\nBALANCE_FREQUENCY\n8950.0\n-4.699903e-16\n1.000056\n-1.997716\n-0.598992\n0.628612\n0.628612\n0.628612\n\n\nPURCHASES\n8950.0\n1.238488e-16\n1.000056\n-1.505149\n-0.654799\n0.160770\n0.710251\n3.559070\n\n\nONEOFF_PURCHASES\n8950.0\n6.986342e-17\n1.000056\n-1.000488\n-1.000488\n0.237516\n0.990945\n1.959134\n\n\nINSTALLMENTS_PURCHASES\n8950.0\n7.343598e-17\n1.000056\n-1.085422\n-1.085422\n0.361196\n0.906100\n2.212015\n\n\nCASH_ADVANCE\n8950.0\n1.381390e-16\n1.000056\n-0.944538\n-0.944538\n-0.944538\n1.059052\n1.729923\n\n\nPURCHASES_FREQUENCY\n8950.0\n6.668781e-17\n1.000056\n-1.278866\n-1.015926\n0.119657\n1.050283\n1.218860\n\n\nONEOFF_PURCHASES_FREQUENCY\n8950.0\n-2.540488e-17\n1.000056\n-0.903315\n-0.903315\n-0.092230\n0.978077\n1.732554\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n8950.0\n-7.462683e-17\n1.000056\n-1.004445\n-1.004445\n-0.273845\n1.069884\n1.377136\n\n\nCASH_ADVANCE_FREQUENCY\n8950.0\n-9.804696e-17\n1.000056\n-0.883204\n-0.883204\n-0.883204\n1.016910\n1.902718\n\n\nCASH_ADVANCE_TRX\n8950.0\n-1.254366e-16\n1.000056\n-0.905801\n-0.905801\n-0.905801\n1.044342\n1.921254\n\n\nPURCHASES_TRX\n8950.0\n-2.889805e-16\n1.000056\n-1.387624\n-0.872718\n0.143257\n0.729118\n2.838620\n\n\nCREDIT_LIMIT\n8950.0\n-4.826927e-16\n1.000056\n-4.573150\n-0.880590\n-0.129660\n0.830085\n2.851804\n\n\nPAYMENTS\n8950.0\n-5.716098e-17\n1.000056\n-2.782085\n-0.608905\n-0.064284\n0.569453\n4.568553\n\n\nMINIMUM_PAYMENTS\n8950.0\n-6.668781e-16\n1.000056\n-5.869902\n-0.643745\n-0.115812\n0.665890\n4.036562\n\n\nPRC_FULL_PAYMENT\n8950.0\n-6.986342e-17\n1.000056\n-0.677889\n-0.677889\n-0.677889\n0.854117\n1.873638\n\n\nTENURE\n8950.0\n7.621464e-16\n1.000056\n-2.526612\n0.422252\n0.422252\n0.422252\n0.422252\n\n\n\n\n\n\n\n\nf, axs = plt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(df_power.columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df_power[column], orient='h')\n\n\n\n\n\ndf_power.hist(bins=20, figsize=(20,15))\n\narray([[&lt;Axes: title={'center': 'BALANCE'}&gt;,\n        &lt;Axes: title={'center': 'BALANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES'}&gt;],\n       [&lt;Axes: title={'center': 'INSTALLMENTS_PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES_FREQUENCY'}&gt;],\n       [&lt;Axes: title={'center': 'PURCHASES_INSTALLMENTS_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_TRX'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_TRX'}&gt;],\n       [&lt;Axes: title={'center': 'CREDIT_LIMIT'}&gt;,\n        &lt;Axes: title={'center': 'PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'MINIMUM_PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'PRC_FULL_PAYMENT'}&gt;],\n       [&lt;Axes: title={'center': 'TENURE'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\nFeature Transform\n\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\nscale_MinMax = MinMaxScaler()\ndf_transformed = pd.DataFrame(scale_MinMax.fit_transform(df_NoNull), columns=columns_names)\nscale_MinMax = MinMaxScaler()\ndf_transformed_Log = pd.DataFrame(scale_MinMax.fit_transform(df_log), columns=columns_names)\nscale_MinMax = MinMaxScaler()\ndf_transformed_Power = pd.DataFrame(scale_MinMax.fit_transform(df_power), columns=columns_names)\nscale_Standard = StandardScaler()\ndf_transformed_Power = pd.DataFrame(scale_Standard.fit_transform(df_power), columns=columns_names)\ndf_log.to_csv(\"./Data_Log.csv\",index=False)\n\n\n\nData with Log Transformation to Clustering and Anomaly detection\n\ndf_transformed = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/Data_Log.csv')\ndf_transformed\n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\n0\n3.735304\n0.597837\n4.568506\n0.000000\n4.568506\n0.000000\n0.154151\n0.000000\n0.080042\n0.000000\n0.000000\n1.098612\n6.908755\n5.312231\n4.945277\n0.000000\n2.564949\n\n\n1\n8.071989\n0.646627\n0.000000\n0.000000\n0.000000\n8.770896\n0.000000\n0.000000\n0.000000\n0.223144\n1.609438\n0.000000\n8.853808\n8.319725\n6.978531\n0.200671\n2.564949\n\n\n2\n7.822504\n0.693147\n6.651791\n6.651791\n0.000000\n0.000000\n0.693147\n0.693147\n0.000000\n0.000000\n0.000000\n2.564949\n8.922792\n6.434654\n6.442994\n0.000000\n2.564949\n\n\n3\n7.419183\n0.492477\n7.313220\n7.313220\n0.000000\n5.331694\n0.080042\n0.080042\n0.000000\n0.080042\n0.693147\n0.693147\n8.922792\n0.000000\n5.747301\n0.000000\n2.564949\n\n\n4\n6.707735\n0.693147\n2.833213\n2.833213\n0.000000\n0.000000\n0.080042\n0.080042\n0.000000\n0.000000\n0.000000\n0.693147\n7.090910\n6.521114\n5.504483\n0.000000\n2.564949\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n3.384170\n0.693147\n5.677165\n0.000000\n5.677165\n0.000000\n0.693147\n0.000000\n0.606136\n0.000000\n0.000000\n1.945910\n6.908755\n5.788719\n3.909748\n0.405465\n1.945910\n\n\n8946\n3.004851\n0.693147\n5.707110\n0.000000\n5.707110\n0.000000\n0.693147\n0.000000\n0.606136\n0.000000\n0.000000\n1.945910\n6.908755\n5.623517\n5.747301\n0.000000\n1.945910\n\n\n8947\n3.194529\n0.606136\n4.979489\n0.000000\n4.979489\n0.000000\n0.606136\n0.000000\n0.510826\n0.000000\n0.000000\n1.791759\n6.908755\n4.410016\n4.423869\n0.223144\n1.945910\n\n\n8948\n2.671218\n0.606136\n0.000000\n0.000000\n0.000000\n3.625907\n0.000000\n0.000000\n0.000000\n0.154151\n1.098612\n0.000000\n6.216606\n3.980615\n4.038755\n0.223144\n1.945910\n\n\n8949\n5.923475\n0.510826\n6.997824\n6.997824\n0.000000\n4.852343\n0.510826\n0.510826\n0.000000\n0.287682\n1.098612\n3.178054\n7.090910\n4.161464\n4.491878\n0.000000\n1.945910\n\n\n\n\n8950 rows × 17 columns\n\n\n\nClustering\n\n# To plot Elbow With Inertia \ninertia = []\nRange = [*range(1,11)]\n\nfor k in Range: \n    kmean = KMeans(n_clusters=k, max_iter=300, random_state=42)\n    kmean.fit(df_transformed)\n    inertia.append(kmean.inertia_)\n    \nplt.figure(figsize=(10,4))\nplt.plot(Range, inertia, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method ')\nplt.show()\n\n\n\n\n\ndrop_variation = []\ndrop_variation.append(0) #add 0 in the first element \n\nfor i in range(len(inertia) -1):\n    dropValue = inertia[i] - inertia[i+1]\n    drop_variation.append(dropValue) \n\n# select suitable k that have large drop in the variation\nk = Range[np.argmax(drop_variation)]\nprint(\"Suitable number of clusters = \",k)\n\nSuitable number of clusters =  2\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=42).fit(df_transformed)\n\nlabels = kmeans.labels_\niner = kmeans.inertia_\ncent = kmeans.cluster_centers_\n\nprint(\"\\t~~ THIS RESULT OF K-mean SKLEARN  ~~\")\nprint('~'*50)\nprint(\"sum of elements that contain in cluster 0 :\",(labels == 0).sum())\nprint(\"sum of elements that contain in cluster 1 :\",(labels == 1).sum())\nprint(\"sum of elements that contain in cluster 2 :\",(labels == 2).sum())\nprint(\"sum of elements that contain in cluster 3 :\",(labels == 3).sum())\n\n    ~~ THIS RESULT OF K-mean SKLEARN  ~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nsum of elements that contain in cluster 0 : 2086\nsum of elements that contain in cluster 1 : 2133\nsum of elements that contain in cluster 2 : 1976\nsum of elements that contain in cluster 3 : 2755\n\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.metrics import silhouette_score\n\n\nscore = silhouette_score(df_transformed,  labels, metric='euclidean')\nprint('Silhouett Score: %.3f' % score)\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\nvisualizer.fit(df_transformed)\nvisualizer.show()  \n\nSilhouett Score: 0.400\n\n\n\n\n\n&lt;Axes: title={'center': 'Silhouette Plot of KMeans Clustering for 8950 Samples in 4 Centers'}, xlabel='silhouette coefficient values', ylabel='cluster label'&gt;\n\n\n\nfrom sklearn.decomposition import KernelPCA\nkpca = KernelPCA(n_components=10, kernel='rbf')\ndf_kpca = pd.DataFrame(kpca.fit_transform(df_transformed))\ndf_kpca\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.322024\n0.516047\n0.085217\n0.036215\n-0.015608\n-0.013188\n-0.345512\n0.052362\n0.067164\n0.204233\n\n\n1\n0.571038\n0.078448\n0.114536\n0.005173\n-0.033539\n0.586146\n-0.004310\n0.012051\n0.002466\n0.001755\n\n\n2\n-0.097995\n-0.181926\n-0.298064\n0.500376\n-0.085155\n0.017446\n0.030665\n0.070862\n-0.147242\n0.267388\n\n\n3\n-0.031012\n-0.038757\n-0.127563\n-0.017882\n0.049901\n-0.002018\n0.004615\n0.049781\n0.032890\n-0.013316\n\n\n4\n-0.042195\n-0.045667\n-0.203879\n0.215753\n-0.010849\n-0.072370\n0.016403\n0.047356\n0.017663\n0.074450\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n-0.376092\n0.612240\n0.150075\n0.016713\n-0.014451\n0.003618\n-0.282473\n0.008226\n-0.007720\n0.018540\n\n\n8946\n-0.373469\n0.605441\n0.145621\n0.015209\n-0.013303\n0.002802\n-0.247563\n0.017263\n-0.006531\n0.050644\n\n\n8947\n-0.326389\n0.532047\n0.093867\n0.027570\n-0.015974\n-0.005210\n-0.444072\n0.050256\n0.070342\n0.165628\n\n\n8948\n0.052839\n-0.013372\n-0.091298\n-0.008970\n0.022265\n-0.223196\n-0.010937\n0.064317\n0.052705\n-0.028263\n\n\n8949\n-0.046451\n-0.076523\n-0.235957\n0.025098\n0.191674\n0.006375\n-0.012778\n-0.017814\n-0.028892\n-0.009289\n\n\n\n\n8950 rows × 10 columns\n\n\n\n\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest(random_state=0, \n                      max_features=2,\n                      n_estimators=100,\n                      contamination=0.1).fit(df_kpca)\n\nanom_pred = clf.predict(df_kpca)\nanom_pred\n\narray([1, 1, 1, ..., 1, 1, 1])\n\n\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, \n            perplexity=50,  \n            random_state=42,\n            n_iter=300).fit_transform(df_transformed)\n\n\ndf_embed_Iso = pd.DataFrame(tsne, columns=['feature1', 'feature2'])\ndf_embed_Iso['Labels']= pd.DataFrame(anom_pred)\ndf_embed_Iso\n\n\n\n\n\n\n\n\nfeature1\nfeature2\nLabels\n\n\n\n\n0\n0.407689\n-5.521269\n1\n\n\n1\n-5.836224\n2.789712\n1\n\n\n2\n0.094771\n3.217455\n1\n\n\n3\n-1.684396\n6.508462\n1\n\n\n4\n3.838952\n4.256968\n1\n\n\n...\n...\n...\n...\n\n\n8945\n0.704921\n-7.746036\n1\n\n\n8946\n0.093061\n-7.773806\n1\n\n\n8947\n-0.642122\n-5.856036\n1\n\n\n8948\n-5.276070\n-6.167468\n1\n\n\n8949\n-2.545392\n7.006695\n1\n\n\n\n\n8950 rows × 3 columns\n\n\n\nTSNE Visualization\n\nplt.figure(figsize=(9,5))\nsns.scatterplot(\n    x='feature1', y='feature2',\n    data=df_embed_Iso,    \n    hue=df_embed_Iso['Labels'],\n    palette=sns.color_palette(\"hls\", 2)\n)\n\n&lt;Axes: xlabel='feature1', ylabel='feature2'&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Samheeta",
    "section": "",
    "text": "Hi! My name is Samheeta, and I am from Hyderabad, India.\nI am a MEng student in the Computer Science department at Virginia Tech, my academic path is firmly anchored in the dynamic realm of Machine Learning (ML). My studies lies in dissecting and mastering the intricate algorithms and methodologies that constitute the backbone of ML. This intense focus is fueled by my deep interest in the transformative capabilities of ML, particularly in how it can alter the landscape of data analysis and decision-making in diverse sectors. My educational journey is centered around a comprehensive understanding of these complex algorithms, coupled with their practical deployment in various real-life situations."
  },
  {
    "objectID": "index.html#blogs-related-to-machine-learning-concept",
    "href": "index.html#blogs-related-to-machine-learning-concept",
    "title": "Machine Learning is Everywhere",
    "section": "Blogs Related to Machine Learning Concept",
    "text": "Blogs Related to Machine Learning Concept\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nProbability theory deals with uncertainty and randomness. Random variables represent uncertain quantities. They come in two types: discrete (distinct values) and continuous (any value within a range). Probability distributions describe how random variables behave, either assigning probabilities to specific values (PMF for discrete) or intervals (PDF for continuous). These concepts are crucial in fields like statistics and machine learning for modeling and decision-making in uncertain scenarios.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nNon-linear regression is a statistical approach for modeling and analyzing relationships between variables that don’t follow a straight-line pattern, allowing for curved and complex relationships in data analysis.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClustering in machine learning is a grouping technique that organizes data points with similar characteristics into clusters. It’s an unsupervised learning method used for various tasks such as customer segmentation and anomaly detection. Common clustering algorithms include K-means, hierarchical clustering, and DBSCAN.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClassification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image Source: https://data-flair.training/blogs/clustering-in-machine-learning/\nContents:"
  },
  {
    "objectID": "posts/post-with-code/index.html#clustering",
    "href": "posts/post-with-code/index.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\nClustering is a data analysis technique used in machine learning and statistics. It involves grouping data points into sets, or clusters, such that points in the same cluster are more similar to each other than to those in other clusters. This similarity is usually based on features or characteristics of the data points. Clustering is a method of unsupervised learning, meaning it’s used to find patterns or structures within the data without using predefined labels or categories. It’s commonly used in various fields such as marketing, biology, and image processing, to identify and categorize groups in complex datasets."
  },
  {
    "objectID": "posts/post-with-code/index.html#kmodes-clustering-using-cardio-data",
    "href": "posts/post-with-code/index.html#kmodes-clustering-using-cardio-data",
    "title": "Clustering",
    "section": "KModes Clustering using Cardio Data",
    "text": "KModes Clustering using Cardio Data\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/post-with-code/Mall_Customers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\n\n\n\n\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\n\n\n\n\n\nplt.figure(1 , figsize = (15 , 7))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n\n\n\n\nMean Arterial Pressure (MAP) = 2 Diastollic Blood Pressure + Sistolic Blood Pressure / 3\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n    cpu_info = subprocess.run(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nApplying KMeans for k=4\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\nApplying KMeans for k=5\n\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n2D Clustering based on Annual Income and Spending Score\n\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n\n\n\n\n3D Clustering Age , Annual Income and Spending Score\n\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n\n\n                                                \n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\ndf.to_csv(\"segmented_customers.csv\", index = False)"
  },
  {
    "objectID": "posts/nlinregression/index.html",
    "href": "posts/nlinregression/index.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "href": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlinear Regression",
    "text": "Example of Nonlinear Regression\nI.Introduction\nIf the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear.\nImporting required libraries\n\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization"
  },
  {
    "objectID": "posts/nlinregression/index.html#linear",
    "href": "posts/nlinregression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#polynomial",
    "href": "posts/nlinregression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#quadratic",
    "href": "posts/nlinregression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "3. Quadratic",
    "text": "3. Quadratic\n\nx = np.arange(-6.0, 6.0, 0.1)\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#exponential",
    "href": "posts/nlinregression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "4. Exponential",
    "text": "4. Exponential\n\nX = np.arange(-6.0, 6.0, 0.1)\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#logarithmic",
    "href": "posts/nlinregression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "5. Logarithmic",
    "text": "5. Logarithmic\n\nX = np.arange(1.0, 10.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#sigmoidallogistic",
    "href": "posts/nlinregression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "6. Sigmoidal/Logistic",
    "text": "6. Sigmoidal/Logistic\n\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "href": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "title": "Nonlinear Regression",
    "section": "Non-Linear Regression example with Dataset",
    "text": "Non-Linear Regression example with Dataset\n\ndf1 = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nlinregression/gdp.csv')\ndf1.head()\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nlinregression/gdp1.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#section",
    "href": "posts/nlinregression/index.html#section",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Choosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\nBuilding The Model\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451712, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.33\nResidual sum of squares (MSE): 0.31\nR2-score: -1743414647518835609117433949444898816.00\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:1010: OptimizeWarning:\n\nCovariance of the parameters could not be estimated"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "MLBlog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability and Random Variables",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/probability/index.html#random-variable",
    "href": "posts/probability/index.html#random-variable",
    "title": "Probability and Random Variables",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable is a mathematical concept used to represent uncertain or random quantities in a probabilistic model. It assigns a numerical value to each possible outcome of a random experiment. Random variables can be classified into two main types:\n\nDiscrete Random Variables: Discrete random variables take on a countable set of distinct values. These values can often be enumerated. Common examples include the number of heads obtained when flipping a coin multiple times or the number of customers arriving at a store in a given hour.\nContinuous Random Variables: Continuous random variables can take on any value within a certain range, often an interval of real numbers. These values are typically associated with measurements and can have an infinite number of possible outcomes. Examples include the height of individuals in a population or the time it takes for a computer to complete a task.\n\nRandom variables are denoted by letters, typically uppercase, such as X, Y, or Z. The possible values that a random variable can take on are called its “range” or “sample space.” For discrete random variables, the range is a set of distinct values, while for continuous random variables, the range is a continuous interval."
  },
  {
    "objectID": "posts/probability/index.html#probability",
    "href": "posts/probability/index.html#probability",
    "title": "Probability and Random Variables",
    "section": "Probability:",
    "text": "Probability:\nProbability is a fundamental concept in mathematics, statistics, and everyday life. It quantifies the likelihood or chance of an event or outcome occurring in uncertain or random situations. In essence, probability helps us understand and measure uncertainty. Here are some key aspects of probability:\n\n\nEvent and Outcome: In probability, an “event” is a specific result or occurrence of interest, while an “outcome” is a particular result of a random experiment or process. Events can range from simple, like the outcome of a coin flip (heads or tails), to complex, like the probability of a stock market crash.\nSample Space: The sample space is the set of all possible outcomes of a random experiment. For example, when rolling a six-sided die, the sample space consists of the numbers 1 through 6.\nProbability as a Number: Probabilities are expressed as numbers between 0 and 1, where 0 represents an impossible event, and 1 represents a certain event. A probability of 0.5 (or 50%) indicates a 50-50 chance of an event occurring.\nEvents and Probability: The probability of an event is typically denoted as P(E), where E is the event. The probability of an event can be calculated using various methods, depending on the type of probability distribution and the nature of the problem.\n\nNow, let’s delve into the two specific probability distributions"
  },
  {
    "objectID": "posts/probability/index.html#why-probability",
    "href": "posts/probability/index.html#why-probability",
    "title": "Probability and Random Variables",
    "section": "Why Probability?",
    "text": "Why Probability?\nIn the realm of machine learning, uncertainty and stochastic elements often emerge due to incomplete observability, leading us to work primarily with sampled data.\nConsider a scenario where we aim to make reliable inferences about the behavior of a random variable, despite having access only to limited data, leaving the entire population characteristics unknown.\n\n\n\nEstimating the data-generating process (src: https://towardsdatascience.com/understanding-random-variables-and-probability-distributions-1ed1daf2e66)\n\n\nTherefore, we require a method to extrapolate from the sampled data to represent the entire population or, in simpler terms, estimate the true process generating the data. Understanding the probability distribution becomes crucial as it allows us to gauge the likelihood of specific outcomes while accommodating the variability observed in the results. This comprehension enables us to extend conclusions from the sample to the broader population, approximate the function generating the data, and enhance the accuracy of predicting the behavior of a random variable.\n\nDiscrete Probability Distribution\nDiscrete probability distributions stem from discrete data and aim to model predictions or outcomes, such as pricing options or forecasting market shocks. These distributions illustrate the possible values of a discrete random variable along with their corresponding probabilities. Examples include the Bernoulli, geometric, and binomial distributions. Here are some common examples of discrete probability distributions:\n\nBernoulli Distribution: Models a single trial with two outcomes, often used in success/failure experiments.\nBinomial Distribution: Describes the number of successes in a fixed number of independent trials with a constant probability of success.\nPoisson Distribution: Models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence.\n\n\n\nContinuous Probability Distribution\nIn contrast, continuous probability distributions cover an infinite range of values, making them uncountable, like time extending from 0 seconds indefinitely. Examples involve continuous measurements like annual rainfall in a city or the weight of newborn babies, where the range of values is limitless and not countable in a finite manner. Here are some common examples of continuous probability distributions:\n\nNormal (Gaussian) Distribution: Symmetric bell-shaped curve that describes many natural phenomena like heights or test scores.\nUniform Distribution: All values in an interval have equal probability, forming a rectangle in the probability density function.\nExponential Distribution: Models the time between events in a Poisson process, such as the time between phone calls at a call center.\n\n\nExample of discrete probability distribution\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset\ntitanic_data = sns.load_dataset('titanic')\n\n# Filter data for relevant columns (class and survival)\nsurvival_data = titanic_data[['pclass', 'survived']]\n\n# Calculate survival probabilities based on passenger class\nsurvival_probabilities = survival_data.groupby('pclass')['survived'].mean()\n\n# Plotting the probability distribution\nplt.figure(figsize=(8, 6))\nsurvival_probabilities.plot(kind='bar', color='skyblue')\nplt.title('Survival Probability by Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survival Probability')\nplt.xticks(rotation=0)\nplt.ylim(0, 1)  # Setting y-axis limits to probability range (0 to 1)\nplt.show()\n\n\n\n\n\n\nExample of continuous probability distribution\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom scipy.stats import norm, expon\n\n# Load breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Consider a specific feature for demonstration (e.g., mean radius)\nfeature_index = 0\nselected_feature = X[:, feature_index]\n\n# Summary statistics and visualization\nmean_value = np.mean(selected_feature)\nstd_dev = np.std(selected_feature)\n\nprint(f\"Mean: {mean_value}, Standard Deviation: {std_dev}\")\n\n# Plotting the histogram of the selected feature\nplt.figure(figsize=(8, 6))\nplt.hist(selected_feature, bins=30, density=True, alpha=0.6, color='blue')\n\n# Plot Gaussian distribution based on the observed mean and standard deviation\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_value, std_dev)\nplt.plot(x, p, 'k', linewidth=2, label='Gaussian PDF')\nplt.title('Histogram and Gaussian PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\n# Plotting an exponential distribution\nplt.figure(figsize=(8, 6))\n\n# Generate random data following an exponential distribution with the observed mean\nexponential_data = np.random.exponential(scale=mean_value, size=1000)\n\n# Plot histogram\nplt.hist(exponential_data, bins=30, density=True, alpha=0.6, color='green')\n\n# Plot exponential distribution PDF\nx_exp = np.linspace(0, np.max(exponential_data), 100)\np_exp = expon.pdf(x_exp, scale=mean_value)\nplt.plot(x_exp, p_exp, 'k', linewidth=2, label='Exponential PDF')\nplt.title('Histogram and Exponential PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\nMean: 14.127291739894552, Standard Deviation: 3.520950760711062\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Select a few features for analysis (e.g., mean radius, mean texture, mean perimeter)\nselected_features = ['mean radius', 'mean texture', 'mean perimeter']\n\n# Descriptive statistics\nprint(df[selected_features].describe())\n\n# Visualize the data using pair plots\nsns.pairplot(df[selected_features])\nplt.show()\n\n# Create probability distribution plots for selected features\nplt.figure(figsize=(15, 5))\nfor i, feature in enumerate(selected_features, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[feature], kde=True, stat=\"probability\", bins=20)\n    plt.title(f'Probability Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Probability')\nplt.tight_layout()\nplt.show()\n\n       mean radius  mean texture  mean perimeter\ncount   569.000000    569.000000      569.000000\nmean     14.127292     19.289649       91.969033\nstd       3.524049      4.301036       24.298981\nmin       6.981000      9.710000       43.790000\n25%      11.700000     16.170000       75.170000\n50%      13.370000     18.840000       86.240000\n75%      15.780000     21.800000      104.100000\nmax      28.110000     39.280000      188.500000"
  },
  {
    "objectID": "posts/classification/index.html#examples-of-machine-learning-classification-in-real-life",
    "href": "posts/classification/index.html#examples-of-machine-learning-classification-in-real-life",
    "title": "Classification",
    "section": "Examples of Machine Learning Classification in Real Life",
    "text": "Examples of Machine Learning Classification in Real Life\nSupervised Machine Learning Classification has different applications in multiple domains of our day-to-day life. Below are some examples.\nApplication in Healthcare :\nSupervised machine learning classification plays a significant role in various aspects of everyday life. Here are some key examples, particularly in the healthcare domain.\n\nDuring the COVID-19 pandemic, supervised machine learning models were crucial in predicting whether individuals were infected with the virus\nMachine learning models are also utilized by researchers to forecast the likelihood of new diseases emerging in the future.\n\nEducation :\n\nUtilizing machine learning for categorizing various documents like text, video, and audio.\nAutomatically determining the language of students’ application documents.\nAnalyzing students’ feedback about professors to gauge sentiments.\n\nTransportation :\n\nPredicting areas likely to experience increased traffic volume.\nAnticipating potential transportation problems in specific areas due to weather conditions.\n\nSustainable Agriculture :\n\nUsing classification models to identify the most suitable land types for different seeds.\nForecasting weather conditions to aid farmers in taking appropriate preventive actions."
  },
  {
    "objectID": "posts/classification/index.html#different-types-of-classification-tasks-in-machine-learning",
    "href": "posts/classification/index.html#different-types-of-classification-tasks-in-machine-learning",
    "title": "Classification",
    "section": "Different Types of Classification Tasks in Machine Learning",
    "text": "Different Types of Classification Tasks in Machine Learning\nThere are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications.\n\nBinary Classification: In a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat. Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification.\nMulti-Class Classification: The multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. In the following case, the model correctly classified the image to be a plane. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include but are not limited to:\nNaive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\nK-Nearest Neighbors: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\nSupport Vector Machines: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data.\nLogistic Regression: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.\nMulti-Label Classification: In multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. Such a scenario can be observed in different domains, such as auto-tagging in Natural Language Processing, where a given text can contain multiple topics. Similarly to computer vision, an image can contain multiple objects, as illustrated below: the model predicted that the image contains: a plane, a boat, a truck, and a dog. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification. We can cite:\nMulti-label Decision Trees\nMulti-label Gradient Boosting\nMulti-label Random Forests\nImbalanced Classification: For the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data. Let’s consider the following 3-class classification scenario where the training data contains: 60% of trucks, 25% of planes, and 15% of boats.The imbalanced classification problem could occur in the following scenario:\nFraudulent transaction detections in financial industries\nRare disease diagnosis\nCustomer churn analysis\n\nUsing conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise.\nSo, does that mean that such problems are left behind?\nOf course not! We can use multiple approaches to tackle the imbalance problem in a dataset. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms.\n\nSampling Techniques These techniques aim to balance the distribution of the original by:\nCluster-based Oversampling:\nRandom undersampling: random elimination of examples from the majority class.\nSMOTE Oversampling: random replication of examples from the minority class.\nCost-Sensitive Algorithms These algorithms take into consideration the cost of misclassification. They aim to minimize the total cost generated by the models.\nCost-sensitive Decision Trees.\nCost-sensitive Logistic Regression.\nCost-sensitive Support Vector Machines."
  },
  {
    "objectID": "posts/classification/index.html#example-distribution-of-loans-in-the-dataset",
    "href": "posts/classification/index.html#example-distribution-of-loans-in-the-dataset",
    "title": "Classification",
    "section": "Example: Distribution of Loans in the Dataset",
    "text": "Example: Distribution of Loans in the Dataset\n\nLook at the first five observations in the dataset.\n\n\nimport pandas as pd\nloan_data = pd.read_csv(\"loan_data.csv\")\nloan_data.head()\n\n\n\n\n\n\n\n\ncredit.policy\npurpose\nint.rate\ninstallment\nlog.annual.inc\ndti\nfico\ndays.with.cr.line\nrevol.bal\nrevol.util\ninq.last.6mths\ndelinq.2yrs\npub.rec\nnot.fully.paid\n\n\n\n\n0\n1\ndebt_consolidation\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0\n0\n0\n0\n\n\n1\n1\ncredit_card\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0\n0\n0\n0\n\n\n2\n1\ndebt_consolidation\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1\n0\n0\n0\n\n\n3\n1\ndebt_consolidation\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1\n0\n0\n0\n\n\n4\n1\ncredit_card\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0\n1\n0\n0\n\n\n\n\n\n\n\n\nBorrowers profile in the dataset.\n\n\nimport matplotlib.pyplot as plt\n# Helper function for data distribution\n# Visualize the proportion of borrowers\ndef show_loan_distrib(data):\n  count = \"\"\n  if isinstance(data, pd.DataFrame):\n      count = data[\"not.fully.paid\"].value_counts()\n  else:\n      count = data.value_counts()\n\n\n  count.plot(kind = 'pie', explode = [0, 0.1], \n\n              figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n  plt.ylabel(\"Loan: Fully Paid Vs. Not Fully Paid\")\n  plt.legend([\"Fully Paid\", \"Not Fully Paid\"])\n  plt.show()\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data)\n\n\n\n\nFrom the graphic above, we notice that 84% of the borrowers paid their loans back, and only 16% didn’t pay them back, which makes the dataset really imbalanced.\n\nVariable Types Before further, we need to check the variables’ type so that we can encode those that need to be encoded.\n\nWe notice that all the columns are continuous variables, except the purpose attribute, which needs to be encoded.\n\n# Check column types\nprint(loan_data.dtypes)\n\ncredit.policy          int64\npurpose               object\nint.rate             float64\ninstallment          float64\nlog.annual.inc       float64\ndti                  float64\nfico                   int64\ndays.with.cr.line    float64\nrevol.bal              int64\nrevol.util           float64\ninq.last.6mths         int64\ndelinq.2yrs            int64\npub.rec                int64\nnot.fully.paid         int64\ndtype: object\n\n\n\nencoded_loan_data = pd.get_dummies(loan_data, prefix=\"purpose\",   \n\n                                   drop_first=True)\nprint(encoded_loan_data.dtypes)\n\ncredit.policy                   int64\nint.rate                      float64\ninstallment                   float64\nlog.annual.inc                float64\ndti                           float64\nfico                            int64\ndays.with.cr.line             float64\nrevol.bal                       int64\nrevol.util                    float64\ninq.last.6mths                  int64\ndelinq.2yrs                     int64\npub.rec                         int64\nnot.fully.paid                  int64\npurpose_credit_card              bool\npurpose_debt_consolidation       bool\npurpose_educational              bool\npurpose_home_improvement         bool\npurpose_major_purchase           bool\npurpose_small_business           bool\ndtype: object"
  },
  {
    "objectID": "posts/classification/index.html#separate-data-into-train-and-test",
    "href": "posts/classification/index.html#separate-data-into-train-and-test",
    "title": "Classification",
    "section": "Separate data into train and test",
    "text": "Separate data into train and test\n\nfrom sklearn.model_selection import train_test_split\n\nX = encoded_loan_data.drop('not.fully.paid', axis = 1)\ny = encoded_loan_data['not.fully.paid']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, \n\n                                           stratify = y, random_state=2022)"
  },
  {
    "objectID": "posts/classification/index.html#application-of-the-sampling-strategies",
    "href": "posts/classification/index.html#application-of-the-sampling-strategies",
    "title": "Classification",
    "section": "Application of the Sampling Strategies",
    "text": "Application of the Sampling Strategies\nWe will explore two sampling strategies here: random undersampling, and SMOTE oversampling."
  },
  {
    "objectID": "posts/classification/index.html#random-undersampling",
    "href": "posts/classification/index.html#random-undersampling",
    "title": "Classification",
    "section": "Random Undersampling",
    "text": "Random Undersampling\nWe will undersample the majority class, which corresponds to the “fully paid” (class 0).\n\nX_train_cp = X_train.copy()\nX_train_cp['not.fully.paid'] = y_train\ny_0 = X_train_cp[X_train_cp['not.fully.paid'] == 0]\ny_1 = X_train_cp[X_train_cp['not.fully.paid'] == 1]\ny_0_undersample = y_0.sample(y_1.shape[0])\nloan_data_undersample = pd.concat([y_0_undersample, y_1], axis = 0)\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data_undersample)"
  },
  {
    "objectID": "posts/classification/index.html#smote-oversampling",
    "href": "posts/classification/index.html#smote-oversampling",
    "title": "Classification",
    "section": "SMOTE Oversampling",
    "text": "SMOTE Oversampling\nPerform oversampling on the minority class\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_train_SMOTE, y_train_SMOTE = smote.fit_resample(X_train,y_train)\n# Visualize the proportion of borrowers\nshow_loan_distrib(y_train_SMOTE)\n\nC:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning:\n\nCould not find the number of physical cores for the following reason:\n[WinError 2] The system cannot find the file specified\nReturning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n    cpu_info = subprocess.run(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n\n\n\nAfter applying the sampling strategies, we observe that the dataset is equally distributed across the different types of borrowers."
  },
  {
    "objectID": "posts/classification/index.html#application-of-some-machine-learning-classification-algorithms",
    "href": "posts/classification/index.html#application-of-some-machine-learning-classification-algorithms",
    "title": "Classification",
    "section": "Application of Some Machine Learning Classification Algorithms",
    "text": "Application of Some Machine Learning Classification Algorithms\nThis section will apply these two classification algorithms to the SMOTE smote sampled dataset. The same training approach can be applied to undersampled data as well."
  },
  {
    "objectID": "posts/classification/index.html#logistic-regression",
    "href": "posts/classification/index.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThis is an explainable algorithm. It classifies a data point by modeling its probability of belonging to a given class using the sigmoid function.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nX = loan_data_undersample.drop('not.fully.paid', axis = 1)\ny = loan_data_undersample['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state=2022)\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(X_train, y_train)\ny_pred = logistic_classifier.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n[[104  57]\n [ 74  87]]\n              precision    recall  f1-score   support\n\n           0       0.58      0.65      0.61       161\n           1       0.60      0.54      0.57       161\n\n    accuracy                           0.59       322\n   macro avg       0.59      0.59      0.59       322\nweighted avg       0.59      0.59      0.59       322\n\n\n\nThese results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data."
  },
  {
    "objectID": "posts/classification/index.html#support-vector-machines",
    "href": "posts/classification/index.html#support-vector-machines",
    "title": "Classification",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nThis algorithm can be used for both classification and regression. It learns to draw the hyperplane (decision boundary) by using the margin to maximization principle. This decision boundary is drawn through the two closest support vectors.\nSVM provides a transformation strategy called kernel tricks used to project non-learner separable data onto a higher dimension space to make them linearly separable.\n\nfrom sklearn.svm import SVC\nsvc_classifier = SVC(kernel='linear')\nsvc_classifier.fit(X_train, y_train)\n\n\n# Make Prediction & print the result\ny_pred = svc_classifier.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.60      0.49      0.54       161\n           1       0.57      0.67      0.62       161\n\n    accuracy                           0.58       322\n   macro avg       0.58      0.58      0.58       322\nweighted avg       0.58      0.58      0.58       322\n\n\n\nThese results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data."
  },
  {
    "objectID": "posts/classification/index.html#summary",
    "href": "posts/classification/index.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\nIn summary, this blog has thoroughly explored the fundamental aspects of classification in machine learning, offering insights into its diverse applications across various fields. Additionally, it delved into the practical implementation of key machine learning models like Logistic Regression and Support Vector Machine, highlighting their application in scenarios involving both undersampling and SMOTE oversampling techniques to achieve a balanced dataset for model training."
  },
  {
    "objectID": "posts/post-with-code/index.html#different-types-of-clustering.",
    "href": "posts/post-with-code/index.html#different-types-of-clustering.",
    "title": "Clustering",
    "section": "Different types of clustering.",
    "text": "Different types of clustering.\n\n1. KMeans Clustering:\n\n\n2. Hierarchical Clustering\n\n\n3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):"
  },
  {
    "objectID": "posts/post-with-code/index.html#k-means-clustering",
    "href": "posts/post-with-code/index.html#k-means-clustering",
    "title": "Clustering",
    "section": "1. K-Means Clustering",
    "text": "1. K-Means Clustering\nThis algorithm categorizes data into a predetermined number (k) of clusters. The process involves calculating the mean (centroid) of each cluster and assigning data points to the nearest cluster based on distance. It’s an iterative process, with re-calculation of centroids and reassignment of points until the clusters stabilize. K-Means is widely used for market segmentation and organizing large data sets."
  },
  {
    "objectID": "posts/post-with-code/index.html#hierarchical-clustering-1",
    "href": "posts/post-with-code/index.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "2. Hierarchical Clustering",
    "text": "2. Hierarchical Clustering\nThis technique builds a hierarchy of clusters using a tree-like structure (dendrogram). It can be agglomerative (bottom-up) where each point starts as a separate cluster and pairs of clusters are merged as one moves up the hierarchy, or divisive (top-down), where all points start in one cluster that splits progressively. It’s useful for data with inherent hierarchical relationships and for visualizing data structure.\n\nDendrogram The sole concept of hierarchical clustering lies in just the construction and analysis of a dendrogram. A dendrogram is a tree-like structure that explains the relationship between all the data points in the system.\n\n\n\n\nSrc: https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8\n\n\nHowever, like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical direction (y-axis) in it represents the distance between clusters in some metric. As you keep going down in a path, you keep breaking the clusters into smaller and smaller units until your granularity level reaches the data sample. In the vice versa situation, when you traverse in up direction, at each level, you are subsuming smaller clusters into larger ones till the point you reach the entire system. As a result, hierarchical clustering is also known as clustering of clustering.\n\n\n\nSrc: https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8"
  },
  {
    "objectID": "posts/post-with-code/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise-1",
    "href": "posts/post-with-code/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise-1",
    "title": "Clustering",
    "section": "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)",
    "text": "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN forms clusters based on the density of data points. It identifies ‘core’ points with many nearby neighbors and groups them into clusters, while marking less densely located points as outliers. This method is effective for data with clusters of similar density and is especially adept at identifying outliers and handling noise."
  },
  {
    "objectID": "posts/post-with-code/index.html#kmodes-clustering-for-customer-data",
    "href": "posts/post-with-code/index.html#kmodes-clustering-for-customer-data",
    "title": "Clustering",
    "section": "KModes Clustering for Customer Data",
    "text": "KModes Clustering for Customer Data\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nData Exploration\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/post-with-code/Mall_Customers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\nChecking for null values\n\ndf.isnull().sum()\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\n\n\n\n\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x7fce331b5d90&gt;\n2D Clustering based on Age and Spending Score\n\nplt.figure(1 , figsize = (15 , 7))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n\n\n\n\nDeciding K value\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nApplying KMeans for k=4\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\nApplying KMeans for k=5\n\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n2D Clustering based on Annual Income and Spending Score\n\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n\n\n\n\n3D Clustering Age , Annual Income and Spending Score\n\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n\n\n                                                \n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\ndf.to_csv(\"segmented_customers.csv\", index = False)\n\nWe have successfully used 2D and 3D K Means clustering on customer data. This approach has proven vital in formulating more effective customer acquisition strategies, significantly contributing to business growth. By identifying distinct patterns and segments in customer behavior and preferences, this method provides invaluable insights for targeted marketing and strategic business decisions."
  },
  {
    "objectID": "posts/post-with-code/index.html#kmeans-clustering-for-customer-data",
    "href": "posts/post-with-code/index.html#kmeans-clustering-for-customer-data",
    "title": "Clustering",
    "section": "KMeans Clustering for Customer Data",
    "text": "KMeans Clustering for Customer Data\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nData Exploration\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/post-with-code/Mall_Customers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\nChecking for null values\n\ndf.isnull().sum()\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\n\n\n\n\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x7fce331b5d90&gt;\n2D Clustering based on Age and Spending Score\n\nplt.figure(1 , figsize = (15 , 7))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n\n\n\n\nDeciding K value\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nApplying KMeans for k=4\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\nApplying KMeans for k=5\n\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n2D Clustering based on Annual Income and Spending Score\n\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n\n\n\n\n3D Clustering Age , Annual Income and Spending Score\n\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n\n\n                                                \n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\ndf.to_csv(\"segmented_customers.csv\", index = False)\n\nWe have successfully used 2D and 3D K Means clustering on customer data. This approach has proven vital in formulating more effective customer acquisition strategies, significantly contributing to business growth. By identifying distinct patterns and segments in customer behavior and preferences, this method provides invaluable insights for targeted marketing and strategic business decisions."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image Source: https://data-flair.training/blogs/clustering-in-machine-learning/\nContents:"
  },
  {
    "objectID": "posts/clustering/index.html#clustering",
    "href": "posts/clustering/index.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\nClustering is a data analysis technique used in machine learning and statistics. It involves grouping data points into sets, or clusters, such that points in the same cluster are more similar to each other than to those in other clusters. This similarity is usually based on features or characteristics of the data points. Clustering is a method of unsupervised learning, meaning it’s used to find patterns or structures within the data without using predefined labels or categories. It’s commonly used in various fields such as marketing, biology, and image processing, to identify and categorize groups in complex datasets."
  },
  {
    "objectID": "posts/clustering/index.html#different-types-of-clustering.",
    "href": "posts/clustering/index.html#different-types-of-clustering.",
    "title": "Clustering",
    "section": "Different types of clustering.",
    "text": "Different types of clustering.\n\n1. KMeans Clustering:\n\n\n2. Hierarchical Clustering\n\n\n3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):"
  },
  {
    "objectID": "posts/clustering/index.html#k-means-clustering",
    "href": "posts/clustering/index.html#k-means-clustering",
    "title": "Clustering",
    "section": "1. K-Means Clustering",
    "text": "1. K-Means Clustering\nThis algorithm categorizes data into a predetermined number (k) of clusters. The process involves calculating the mean (centroid) of each cluster and assigning data points to the nearest cluster based on distance. It’s an iterative process, with re-calculation of centroids and reassignment of points until the clusters stabilize. K-Means is widely used for market segmentation and organizing large data sets."
  },
  {
    "objectID": "posts/clustering/index.html#hierarchical-clustering-1",
    "href": "posts/clustering/index.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "2. Hierarchical Clustering",
    "text": "2. Hierarchical Clustering\nThis technique builds a hierarchy of clusters using a tree-like structure (dendrogram). It can be agglomerative (bottom-up) where each point starts as a separate cluster and pairs of clusters are merged as one moves up the hierarchy, or divisive (top-down), where all points start in one cluster that splits progressively. It’s useful for data with inherent hierarchical relationships and for visualizing data structure.\n\nDendrogram The sole concept of hierarchical clustering lies in just the construction and analysis of a dendrogram. A dendrogram is a tree-like structure that explains the relationship between all the data points in the system.\n\n\n\n\nSrc: https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8\n\n\nHowever, like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical direction (y-axis) in it represents the distance between clusters in some metric. As you keep going down in a path, you keep breaking the clusters into smaller and smaller units until your granularity level reaches the data sample. In the vice versa situation, when you traverse in up direction, at each level, you are subsuming smaller clusters into larger ones till the point you reach the entire system. As a result, hierarchical clustering is also known as clustering of clustering.\n\n\n\nSrc: https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8"
  },
  {
    "objectID": "posts/clustering/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise-1",
    "href": "posts/clustering/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise-1",
    "title": "Clustering",
    "section": "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)",
    "text": "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN forms clusters based on the density of data points. It identifies ‘core’ points with many nearby neighbors and groups them into clusters, while marking less densely located points as outliers. This method is effective for data with clusters of similar density and is especially adept at identifying outliers and handling noise."
  },
  {
    "objectID": "posts/clustering/index.html#kmeans-clustering-for-customer-data",
    "href": "posts/clustering/index.html#kmeans-clustering-for-customer-data",
    "title": "Clustering",
    "section": "KMeans Clustering for Customer Data",
    "text": "KMeans Clustering for Customer Data\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly as py\nimport plotly.graph_objs as go\n\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nData Exploration\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/clustering/Mall_Customers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\nChecking for null values\n\ndf.isnull().sum()\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\n\n\n\n\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x7fce331b5d90&gt;\n2D Clustering based on Age and Spending Score\n\nplt.figure(1 , figsize = (15 , 7))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n\n\n\n\nDeciding K value\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n    cpu_info = subprocess.run(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nApplying KMeans for k=4\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\nApplying KMeans for k=5\n\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n2D Clustering based on Annual Income and Spending Score\n\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n\n\n\n\n3D Clustering Age , Annual Income and Spending Score\n\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n\n\n                                                \n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\ncluster\n\n\n\n\n0\n1\nMale\n19\n15\n39\n4\n\n\n1\n2\nMale\n21\n15\n81\n5\n\n\n2\n3\nFemale\n20\n16\n6\n4\n\n\n3\n4\nFemale\n23\n16\n77\n5\n\n\n4\n5\nFemale\n31\n17\n40\n4\n\n\n\n\n\n\n\n\ndf.to_csv(\"segmented_customers.csv\", index = False)\n\nWe have successfully used 2D and 3D K Means clustering on customer data. This approach has proven vital in formulating more effective customer acquisition strategies, significantly contributing to business growth. By identifying distinct patterns and segments in customer behavior and preferences, this method provides invaluable insights for targeted marketing and strategic business decisions."
  },
  {
    "objectID": "posts/linregression/index.html#residual-plot-shows-residual-error-vs.-true-y-value.",
    "href": "posts/linregression/index.html#residual-plot-shows-residual-error-vs.-true-y-value.",
    "title": "Linear Regression",
    "section": "Residual plot shows residual error VS. true y value.",
    "text": "Residual plot shows residual error VS. true y value.\n\nsns.scatterplot(x=y_test, y=test_residual)\n\nplt.axhline(y=0, color='r', ls='--')\n\n&lt;matplotlib.lines.Line2D at 0x2d09dd19310&gt;"
  },
  {
    "objectID": "posts/linregression/index.html#residualplot-showing-a-clear-pattern-indicating-linear-regression-no-valid",
    "href": "posts/linregression/index.html#residualplot-showing-a-clear-pattern-indicating-linear-regression-no-valid",
    "title": "Linear Regression",
    "section": "Residualplot showing a clear pattern, indicating Linear Regression no valid!",
    "text": "Residualplot showing a clear pattern, indicating Linear Regression no valid!\nIn summary, this example demonstrates the application of linear regression in predicting real estate prices while emphasizing the importance of each step in the machine learning process, from data preparation to model evaluation and residual analysis."
  },
  {
    "objectID": "posts/linregression/index.html#exploratory-data-analysis-eda",
    "href": "posts/linregression/index.html#exploratory-data-analysis-eda",
    "title": "Linear Regression",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nsns.pairplot(df)\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x7fcf18e2e910&gt;\n\nX=df.drop('Y house price of unit area', axis=1)\n\ny=df['X4 number of convenience stores']\n\n\nprint(\"X=\",X.shape,\"\\ny=\", y.shape)\n\nX= (414, 7) \ny= (414,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n\nX_train.shape\n\n(289, 7)\n\n\n\nX_test.shape\n\n(125, 7)\n\n\nLinear Regression\n\nmodel = LinearRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.coef_\n\narray([-1.33431232e-17, -8.23993651e-16, -1.24336375e-16,  6.64430818e-19,\n        1.00000000e+00,  1.56601898e-14,  8.92570291e-15])\n\n\n\npd.DataFrame(model.coef_, X.columns, columns=['Coedicients'])\n\n\n\n\n\n\n\n\nCoedicients\n\n\n\n\nNo\n-1.334312e-17\n\n\nX1 transaction date\n-8.239937e-16\n\n\nX2 house age\n-1.243364e-16\n\n\nX3 distance to the nearest MRT station\n6.644308e-19\n\n\nX4 number of convenience stores\n1.000000e+00\n\n\nX5 latitude\n1.566019e-14\n\n\nX6 longitude\n8.925703e-15"
  },
  {
    "objectID": "posts/linregression/index.html#predictions-from-our-model",
    "href": "posts/linregression/index.html#predictions-from-our-model",
    "title": "Linear Regression",
    "section": "Predictions from our Model",
    "text": "Predictions from our Model\n\ny_pred = model.predict(X_test)"
  },
  {
    "objectID": "posts/linregression/index.html#regression-evaluation-metrics",
    "href": "posts/linregression/index.html#regression-evaluation-metrics",
    "title": "Linear Regression",
    "section": "Regression Evaluation Metrics",
    "text": "Regression Evaluation Metrics\nHere are three common evaluation metrics for regression problems:\n\nMean Absolute Error (MAE) is the mean of the absolute value of the errors: 1n∑i=1n|yi−y^i|\nMean Squared Error (MSE) is the mean of the squared errors: 1n∑i=1n(yi−y^i)2\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors: 1n∑i=1n(yi−y^i)2−−−−−−−−−−−−√"
  },
  {
    "objectID": "posts/linregression/index.html#comparing-these-metrics",
    "href": "posts/linregression/index.html#comparing-these-metrics",
    "title": "Linear Regression",
    "section": "Comparing these metrics:",
    "text": "Comparing these metrics:\n\nMAE is the easiest to understand, because it’s the average error.\nMSE is more popular than MAE, because MSE “punishes” larger errors, which tends to be useful in the real world.\nRMSE is even more popular than MSE, because RMSE is interpretable in the “y” units. All of these are loss functions, because we want to minimize them.\n\n\nMAE= metrics.mean_absolute_error(y_test, y_pred)\nMSE=metrics.mean_squared_error(y_test, y_pred)\nRMSE= np.sqrt(MSE)\n\n\nMAE\n\n2.9990405857085223e-15\n\n\n\nMSE\n\n1.340362315924368e-29\n\n\n\nRMSE\n\n3.661095895936582e-15\n\n\n\ndf['X4 number of convenience stores'].mean()\n\n4.094202898550725"
  },
  {
    "objectID": "posts/linregression/index.html#residual-histogram",
    "href": "posts/linregression/index.html#residual-histogram",
    "title": "Linear Regression",
    "section": "Residual Histogram",
    "text": "Residual Histogram\n\nOften for Linear Regression it is a good idea to separately evaluate residuals (y−y^)\n\nand not just calculate performance metrics (e.g. RMSE).\n\nLet’s explore why this is important…\nThe residual eerors should be random and close to a normal distribution."
  },
  {
    "objectID": "posts/linregression/index.html#example-real-estate-price-prediction",
    "href": "posts/linregression/index.html#example-real-estate-price-prediction",
    "title": "Linear Regression",
    "section": "Example Real estate price prediction",
    "text": "Example Real estate price prediction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\ndf=pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/linregression/Real estate.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\n0\n1\n2012.917\n32.0\n84.87882\n10\n24.98298\n121.54024\n37.9\n\n\n1\n2\n2012.917\n19.5\n306.59470\n9\n24.98034\n121.53951\n42.2\n\n\n2\n3\n2013.583\n13.3\n561.98450\n5\n24.98746\n121.54391\n47.3\n\n\n3\n4\n2013.500\n13.3\n561.98450\n5\n24.98746\n121.54391\n54.8\n\n\n4\n5\n2012.833\n5.0\n390.56840\n5\n24.97937\n121.54245\n43.1\n\n\n\n\n\n\n\n\ndf.shape\n\n(414, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 414 entries, 0 to 413\nData columns (total 8 columns):\n #   Column                                  Non-Null Count  Dtype  \n---  ------                                  --------------  -----  \n 0   No                                      414 non-null    int64  \n 1   X1 transaction date                     414 non-null    float64\n 2   X2 house age                            414 non-null    float64\n 3   X3 distance to the nearest MRT station  414 non-null    float64\n 4   X4 number of convenience stores         414 non-null    int64  \n 5   X5 latitude                             414 non-null    float64\n 6   X6 longitude                            414 non-null    float64\n 7   Y house price of unit area              414 non-null    float64\ndtypes: float64(6), int64(2)\nmemory usage: 26.0 KB\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\nNo\n1.000000\n-0.048658\n-0.032808\n-0.013573\n-0.012699\n-0.010110\n-0.011059\n-0.028587\n\n\nX1 transaction date\n-0.048658\n1.000000\n0.017549\n0.060880\n0.009635\n0.035058\n-0.041082\n0.087491\n\n\nX2 house age\n-0.032808\n0.017549\n1.000000\n0.025622\n0.049593\n0.054420\n-0.048520\n-0.210567\n\n\nX3 distance to the nearest MRT station\n-0.013573\n0.060880\n0.025622\n1.000000\n-0.602519\n-0.591067\n-0.806317\n-0.673613\n\n\nX4 number of convenience stores\n-0.012699\n0.009635\n0.049593\n-0.602519\n1.000000\n0.444143\n0.449099\n0.571005\n\n\nX5 latitude\n-0.010110\n0.035058\n0.054420\n-0.591067\n0.444143\n1.000000\n0.412924\n0.546307\n\n\nX6 longitude\n-0.011059\n-0.041082\n-0.048520\n-0.806317\n0.449099\n0.412924\n1.000000\n0.523287\n\n\nY house price of unit area\n-0.028587\n0.087491\n-0.210567\n-0.673613\n0.571005\n0.546307\n0.523287\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True,cmap='Reds')\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/linearRegression/index.html",
    "href": "posts/linearRegression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Contents:\n\nWhat Is Linear Regression?\nExample of Linear Regression with Real estate price prediction dataset.\nData Exploration\nModel Evaluation\nResidual Analysis"
  },
  {
    "objectID": "posts/linearRegression/index.html#example-real-estate-price-prediction",
    "href": "posts/linearRegression/index.html#example-real-estate-price-prediction",
    "title": "Linear Regression",
    "section": "Example Real estate price prediction",
    "text": "Example Real estate price prediction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\ndf=pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/linearRegression/Real estate.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\n0\n1\n2012.917\n32.0\n84.87882\n10\n24.98298\n121.54024\n37.9\n\n\n1\n2\n2012.917\n19.5\n306.59470\n9\n24.98034\n121.53951\n42.2\n\n\n2\n3\n2013.583\n13.3\n561.98450\n5\n24.98746\n121.54391\n47.3\n\n\n3\n4\n2013.500\n13.3\n561.98450\n5\n24.98746\n121.54391\n54.8\n\n\n4\n5\n2012.833\n5.0\n390.56840\n5\n24.97937\n121.54245\n43.1\n\n\n\n\n\n\n\n\ndf.shape\n\n(414, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 414 entries, 0 to 413\nData columns (total 8 columns):\n #   Column                                  Non-Null Count  Dtype  \n---  ------                                  --------------  -----  \n 0   No                                      414 non-null    int64  \n 1   X1 transaction date                     414 non-null    float64\n 2   X2 house age                            414 non-null    float64\n 3   X3 distance to the nearest MRT station  414 non-null    float64\n 4   X4 number of convenience stores         414 non-null    int64  \n 5   X5 latitude                             414 non-null    float64\n 6   X6 longitude                            414 non-null    float64\n 7   Y house price of unit area              414 non-null    float64\ndtypes: float64(6), int64(2)\nmemory usage: 26.0 KB\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\nNo\n1.000000\n-0.048658\n-0.032808\n-0.013573\n-0.012699\n-0.010110\n-0.011059\n-0.028587\n\n\nX1 transaction date\n-0.048658\n1.000000\n0.017549\n0.060880\n0.009635\n0.035058\n-0.041082\n0.087491\n\n\nX2 house age\n-0.032808\n0.017549\n1.000000\n0.025622\n0.049593\n0.054420\n-0.048520\n-0.210567\n\n\nX3 distance to the nearest MRT station\n-0.013573\n0.060880\n0.025622\n1.000000\n-0.602519\n-0.591067\n-0.806317\n-0.673613\n\n\nX4 number of convenience stores\n-0.012699\n0.009635\n0.049593\n-0.602519\n1.000000\n0.444143\n0.449099\n0.571005\n\n\nX5 latitude\n-0.010110\n0.035058\n0.054420\n-0.591067\n0.444143\n1.000000\n0.412924\n0.546307\n\n\nX6 longitude\n-0.011059\n-0.041082\n-0.048520\n-0.806317\n0.449099\n0.412924\n1.000000\n0.523287\n\n\nY house price of unit area\n-0.028587\n0.087491\n-0.210567\n-0.673613\n0.571005\n0.546307\n0.523287\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True,cmap='Reds')\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/linearRegression/index.html#exploratory-data-analysis-eda",
    "href": "posts/linearRegression/index.html#exploratory-data-analysis-eda",
    "title": "Linear Regression",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nsns.pairplot(df)\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x7fcf18e2e910&gt;\n\nX=df.drop('Y house price of unit area', axis=1)\n\ny=df['X4 number of convenience stores']\n\n\nprint(\"X=\",X.shape,\"\\ny=\", y.shape)\n\nX= (414, 7) \ny= (414,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n\nX_train.shape\n\n(289, 7)\n\n\n\nX_test.shape\n\n(125, 7)\n\n\nLinear Regression\n\nmodel = LinearRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.coef_\n\narray([-1.33431232e-17, -8.23993651e-16, -1.24336375e-16,  6.64430818e-19,\n        1.00000000e+00,  1.56601898e-14,  8.92570291e-15])\n\n\n\npd.DataFrame(model.coef_, X.columns, columns=['Coedicients'])\n\n\n\n\n\n\n\n\nCoedicients\n\n\n\n\nNo\n-1.334312e-17\n\n\nX1 transaction date\n-8.239937e-16\n\n\nX2 house age\n-1.243364e-16\n\n\nX3 distance to the nearest MRT station\n6.644308e-19\n\n\nX4 number of convenience stores\n1.000000e+00\n\n\nX5 latitude\n1.566019e-14\n\n\nX6 longitude\n8.925703e-15"
  },
  {
    "objectID": "posts/linearRegression/index.html#predictions-from-our-model",
    "href": "posts/linearRegression/index.html#predictions-from-our-model",
    "title": "Linear Regression",
    "section": "Predictions from our Model",
    "text": "Predictions from our Model\n\ny_pred = model.predict(X_test)"
  },
  {
    "objectID": "posts/linearRegression/index.html#regression-evaluation-metrics",
    "href": "posts/linearRegression/index.html#regression-evaluation-metrics",
    "title": "Linear Regression",
    "section": "Regression Evaluation Metrics",
    "text": "Regression Evaluation Metrics\nHere are three common evaluation metrics for regression problems:\n\nMean Absolute Error (MAE) is the mean of the absolute value of the errors: 1n∑i=1n|yi−y^i|\nMean Squared Error (MSE) is the mean of the squared errors: 1n∑i=1n(yi−y^i)2\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors: 1n∑i=1n(yi−y^i)2−−−−−−−−−−−−√"
  },
  {
    "objectID": "posts/linearRegression/index.html#comparing-these-metrics",
    "href": "posts/linearRegression/index.html#comparing-these-metrics",
    "title": "Linear Regression",
    "section": "Comparing these metrics:",
    "text": "Comparing these metrics:\n\nMAE is the easiest to understand, because it’s the average error.\nMSE is more popular than MAE, because MSE “punishes” larger errors, which tends to be useful in the real world.\nRMSE is even more popular than MSE, because RMSE is interpretable in the “y” units. All of these are loss functions, because we want to minimize them.\n\n\nMAE= metrics.mean_absolute_error(y_test, y_pred)\nMSE=metrics.mean_squared_error(y_test, y_pred)\nRMSE= np.sqrt(MSE)\n\n\nMAE\n\n2.9990405857085223e-15\n\n\n\nMSE\n\n1.340362315924368e-29\n\n\n\nRMSE\n\n3.661095895936582e-15\n\n\n\ndf['X4 number of convenience stores'].mean()\n\n4.094202898550725"
  },
  {
    "objectID": "posts/linearRegression/index.html#residual-histogram",
    "href": "posts/linearRegression/index.html#residual-histogram",
    "title": "Linear Regression",
    "section": "Residual Histogram",
    "text": "Residual Histogram\n\nOften for Linear Regression it is a good idea to separately evaluate residuals (y−y^)\n\nand not just calculate performance metrics (e.g. RMSE).\n\nLet’s explore why this is important…\nThe residual eerors should be random and close to a normal distribution."
  },
  {
    "objectID": "posts/linearRegression/index.html#residual-plot-shows-residual-error-vs.-true-y-value.",
    "href": "posts/linearRegression/index.html#residual-plot-shows-residual-error-vs.-true-y-value.",
    "title": "Linear Regression",
    "section": "Residual plot shows residual error VS. true y value.",
    "text": "Residual plot shows residual error VS. true y value.\n\nsns.scatterplot(x=y_test, y=test_residual)\n\nplt.axhline(y=0, color='r', ls='--')\n\n&lt;matplotlib.lines.Line2D at 0x25714585df0&gt;"
  },
  {
    "objectID": "posts/linearRegression/index.html#residualplot-showing-a-clear-pattern-indicating-linear-regression-no-valid",
    "href": "posts/linearRegression/index.html#residualplot-showing-a-clear-pattern-indicating-linear-regression-no-valid",
    "title": "Linear Regression",
    "section": "Residualplot showing a clear pattern, indicating Linear Regression no valid!",
    "text": "Residualplot showing a clear pattern, indicating Linear Regression no valid!\nIn summary, this example demonstrates the application of linear regression in predicting real estate prices while emphasizing the importance of each step in the machine learning process, from data preparation to model evaluation and residual analysis."
  },
  {
    "objectID": "posts/nonlinregression/index.html",
    "href": "posts/nonlinregression/index.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/nonlinregression/index.html#example-of-nonlinear-regression",
    "href": "posts/nonlinregression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlinear Regression",
    "text": "Example of Nonlinear Regression\nI.Introduction\nIf the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear.\nImporting required libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/nonlinregression/index.html#linear",
    "href": "posts/nonlinregression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#polynomial",
    "href": "posts/nonlinregression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#quadratic",
    "href": "posts/nonlinregression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "Quadratic",
    "text": "Quadratic\nY=X2\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\nydata = y \nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('ydata:', ydata[0:5])\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\nydata: [25.   24.01 23.04 22.09 21.16]\n\n\n\n\n\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [ 0.36  0.3  -2.75 -0.36  0.8 ]\nydata: [25.36 24.31 20.29 21.73 21.96]\n\n\n\n\n\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [ 0.54 -1.87  1.47 -1.94  2.05]\nydata: [25.54 22.14 24.51 20.15 23.21]\n\n\n\n\n\nAs can be seen, this function has x2 as independent variables. Also, the graphic of this function is not a straight line over the 2D plane. So this is a non-linear function."
  },
  {
    "objectID": "posts/nonlinregression/index.html#exponential",
    "href": "posts/nonlinregression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "Exponential",
    "text": "Exponential\nAn exponential function with base c is defined by Y=a+bcX\nwhere b ≠0, c &gt; 0 , c ≠1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.\n\nX = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\nY= np.exp(X)\nY_noise = 7 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(3))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [0.007 0.007 0.008 0.009 0.01 ]\nY_noise: [ 10.54   8.59   5.27   7.3  -13.51]\nYdata: [ 10.55   8.6    5.28   7.31 -13.5 ]"
  },
  {
    "objectID": "posts/nonlinregression/index.html#logarithmic",
    "href": "posts/nonlinregression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "Logarithmic",
    "text": "Logarithmic\nThe response y is a results of applying logarithmic map from input x ’s to output variable y . It is one of the simplest form of log(): i.e. y=log(x)\nAlso, consider that instead of x , X can be, which can be polynomial representation of the x ’s. In general form it would be written as y=log(X)\n\nX = np.arange(0.01, 5, 0.1)\nY = np.log(X)\nY_noise = 0.25*np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [0.01 0.11 0.21 0.31 0.41]\nY: [-4.61 -2.21 -1.56 -1.17 -0.89]\nY_noise: [-0.04 -0.11  0.27  0.18 -0.15]\nYdata: [-4.65 -2.31 -1.29 -1.   -1.05]\n\n\n\n\n\n\nX = np.arange(0.01, 5, 0.1)\nX = np.power(X,2)\nY = np.log(X)\nY_noise = 0.5 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5].round(2))\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [0.   0.01 0.04 0.1  0.17]\nY: [-9.21 -4.41 -3.12 -2.34 -1.78]\nY_noise: [-0.68  0.02 -0.1  -0.27  0.13]\nYdata: [-9.89 -4.39 -3.22 -2.61 -1.66]"
  },
  {
    "objectID": "posts/nonlinregression/index.html#sigmoidallogistic",
    "href": "posts/nonlinregression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "Sigmoidal/Logistic",
    "text": "Sigmoidal/Logistic\nY=a+b1+c(X−d)\n\nX = np.arange(-5.0, 5.0, 0.1)\nY = 1-4/(1+np.power(3, X-2))\nY_noise = 0.25 * np.random.normal(size=X.size) \nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [-3. -3. -3. -3. -3.]\nY_noise: [-0.44  0.23 -0.22  0.46  0.4 ]\nYdata: [-3.44 -2.77 -3.22 -2.54 -2.59]"
  },
  {
    "objectID": "posts/nonlinregression/index.html#non-linear-regression-example-with-dataset",
    "href": "posts/nonlinregression/index.html#non-linear-regression-example-with-dataset",
    "title": "Nonlinear Regression",
    "section": "Non-Linear Regression example with Dataset",
    "text": "Non-Linear Regression example with Dataset\n\ndf1 = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/gdp.csv')\ndf1.head()\ndf = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/gdp1.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#section",
    "href": "posts/nonlinregression/index.html#section",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Choosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\nBuilding The Model\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451712, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.18\nResidual sum of squares (MSE): 0.11\nR2-score: -306193127611360907644976064626688.00\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:1010: OptimizeWarning:\n\nCovariance of the parameters could not be estimated"
  },
  {
    "objectID": "posts/nonlinregression/index.html#load-the-data",
    "href": "posts/nonlinregression/index.html#load-the-data",
    "title": "Nonlinear Regression",
    "section": "Load the data",
    "text": "Load the data\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 5*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\nydata = y \nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('ydata:', ydata[0:5])\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\nydata: [25.   24.01 23.04 22.09 21.16]\n\n\n\n\n\nThere are 20640 observations and 9 features + 1 target variable(median_house_value).\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [ 0.25 -0.73 -1.57  1.18 -0.45]\nydata: [25.25 23.28 21.47 23.27 20.71]\n\n\n\n\n\nThe above info function provide the information about the dataset . For example:\n\nMissing values(no missing values in our dataset)\ndatatype(9 of them are floats and 1 is categorical)\n\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [-1.71  0.78  0.89  3.37 -0.17]\nydata: [23.29 24.79 23.93 25.46 20.99]\n\n\n\n\n\nIf we have to select a single variable for the regression analysis then higher possibility is to pick the most correlated feature with the target variable(median_house_value).\n\nIn our case it is the median_income with correlation coefficent of 0.69\n\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 1*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [-102.     -95.539  -89.352  -83.433  -77.776]\ny_noise: [ -2.75  -8.94 -13.49  -6.11   0.94]\nydata: [-104.75 -104.48 -102.84  -89.55  -76.84]\n\n\n\n\n\n\nX = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\nY= np.exp(X)\nY_noise = 7 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(3))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [0.007 0.007 0.008 0.009 0.01 ]\nY_noise: [-8.86 -7.8   8.68  4.7   0.79]\nYdata: [-8.86 -7.79  8.69  4.71  0.8 ]\n\n\n\n\n\n\nX = np.arange(0.01, 5, 0.1)\nY = np.log(X)\nY_noise = 0.25*np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [0.01 0.11 0.21 0.31 0.41]\nY: [-4.61 -2.21 -1.56 -1.17 -0.89]\nY_noise: [-0.19 -0.43 -0.12 -0.18  0.1 ]\nYdata: [-4.8  -2.64 -1.68 -1.36 -0.79]\n\n\n\n\n\nUsing this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house. There is somewhat positive linear relationship between them."
  },
  {
    "objectID": "posts/nonlinregression/index.html#split-the-data",
    "href": "posts/nonlinregression/index.html#split-the-data",
    "title": "Nonlinear Regression",
    "section": "Split the data",
    "text": "Split the data\n\nX = np.arange(0.01, 5, 0.1)\nX = np.power(X,2)\nY = np.log(X)\nY_noise = 0.5 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5].round(2))\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [0.   0.01 0.04 0.1  0.17]\nY: [-9.21 -4.41 -3.12 -2.34 -1.78]\nY_noise: [ 0.41 -0.33  0.02  0.25  0.46]\nYdata: [-8.8  -4.74 -3.11 -2.09 -1.32]"
  },
  {
    "objectID": "posts/nonlinregression/index.html#model-1",
    "href": "posts/nonlinregression/index.html#model-1",
    "title": "Nonlinear Regression",
    "section": "Model 1",
    "text": "Model 1\n\nLinear regression model\n\nX = np.arange(-5.0, 5.0, 0.1)\nY = 1-4/(1+np.power(3, X-2))\nY_noise = 0.25 * np.random.normal(size=X.size) \nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [-3. -3. -3. -3. -3.]\nY_noise: [-0.22 -0.19 -0.42  0.3  -0.27]\nYdata: [-3.22 -3.18 -3.42 -2.7  -3.26]\n\n\n\n\n\n\ncountries = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/Countries GDP 1960-2020.csv')\ncountries.head()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n...\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n\n\n\n\n0\nAfrica Eastern and Southern\nAFE\n1.931311e+10\n1.972349e+10\n2.149392e+10\n2.573321e+10\n2.352744e+10\n2.681057e+10\n2.915216e+10\n3.017317e+10\n...\n9.430000e+11\n9.510000e+11\n9.640000e+11\n9.850000e+11\n9.200000e+11\n8.730000e+11\n9.850000e+11\n1.010000e+12\n1.010000e+12\n9.210000e+11\n\n\n1\nAfrica Western and Central\nAFW\n1.040428e+10\n1.112805e+10\n1.194335e+10\n1.267652e+10\n1.383858e+10\n1.486247e+10\n1.583285e+10\n1.442643e+10\n...\n6.710000e+11\n7.280000e+11\n8.210000e+11\n8.650000e+11\n7.610000e+11\n6.910000e+11\n6.840000e+11\n7.420000e+11\n7.950000e+11\n7.850000e+11\n\n\n2\nAustralia\nAUS\n1.860679e+10\n1.968306e+10\n1.992272e+10\n2.153993e+10\n2.380110e+10\n2.597715e+10\n2.730989e+10\n3.044462e+10\n...\n1.400000e+12\n1.550000e+12\n1.580000e+12\n1.470000e+12\n1.350000e+12\n1.210000e+12\n1.330000e+12\n1.430000e+12\n1.390000e+12\n1.330000e+12\n\n\n3\nAustria\nAUT\n6.592694e+09\n7.311750e+09\n7.756110e+09\n8.374175e+09\n9.169984e+09\n9.994071e+09\n1.088768e+10\n1.157943e+10\n...\n4.310000e+11\n4.090000e+11\n4.300000e+11\n4.420000e+11\n3.820000e+11\n3.960000e+11\n4.160000e+11\n4.550000e+11\n4.450000e+11\n4.330000e+11\n\n\n4\nBurundi\nBDI\n1.960000e+08\n2.030000e+08\n2.135000e+08\n2.327500e+08\n2.607500e+08\n1.589950e+08\n1.654446e+08\n1.782971e+08\n...\n2.235821e+09\n2.333308e+09\n2.451625e+09\n2.705783e+09\n3.104395e+09\n2.732809e+09\n2.748180e+09\n2.668496e+09\n2.631434e+09\n2.841786e+09\n\n\n\n\n5 rows × 63 columns\n\n\n\n\nchina = countries[countries[\"Country Name\"]== 'China']\nchina = china.drop(['Country Name', 'Country Code'], axis = 1)\nchina = china.T\nchina.columns = ['GDP in US$']\ndf_gdp = china.reset_index()\ndf_gdp.columns = ['Year', 'GDP in US$']\ndf_gdp.head()\n\n\n\n\n\n\n\n\nYear\nGDP in US$\n\n\n\n\n0\n1960\n5.971647e+10\n\n\n1\n1961\n5.005687e+10\n\n\n2\n1962\n4.720936e+10\n\n\n3\n1963\n5.070680e+10\n\n\n4\n1964\n5.970834e+10"
  },
  {
    "objectID": "posts/nonlinregression/index.html#interpretation",
    "href": "posts/nonlinregression/index.html#interpretation",
    "title": "Nonlinear Regression",
    "section": "Interpretation:",
    "text": "Interpretation:\nThis simple linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : [42032.17769894]\nIntercept (b) : 44320.63\nR2 score: 0.4466 (For R2 score more is better in the range [0,1])\nRoot mean squared error: 84941.0515 (Lower is better)"
  },
  {
    "objectID": "posts/nonlinregression/index.html#the-plot-of-simple-linear-regression",
    "href": "posts/nonlinregression/index.html#the-plot-of-simple-linear-regression",
    "title": "Nonlinear Regression",
    "section": "The plot of simple linear regression :",
    "text": "The plot of simple linear regression :\n\ndf_gdp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 61 entries, 0 to 60\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Year        61 non-null     object \n 1   GDP in US$  61 non-null     float64\ndtypes: float64(1), object(1)\nmemory usage: 1.1+ KB"
  },
  {
    "objectID": "posts/nonlinregression/index.html#residual-plot-from-linear-regression",
    "href": "posts/nonlinregression/index.html#residual-plot-from-linear-regression",
    "title": "Nonlinear Regression",
    "section": "Residual plot from linear regression",
    "text": "Residual plot from linear regression\n\ndf_gdp['Year'] = df_gdp['Year'].astype('int32')"
  },
  {
    "objectID": "posts/nonlinregression/index.html#model-2",
    "href": "posts/nonlinregression/index.html#model-2",
    "title": "Nonlinear Regression",
    "section": "Model 2",
    "text": "Model 2\n\nApplying transformation\n\nplt.figure(figsize=(8,5))\nX = df_gdp['Year']\ny = df_gdp[\"GDP in US$\"]\nplt.plot(X, y, 'bo')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#fitting-a-model",
    "href": "posts/nonlinregression/index.html#fitting-a-model",
    "title": "Nonlinear Regression",
    "section": "Fitting a model",
    "text": "Fitting a model\n\nX, y = (df_gdp[\"Year\"].values, df_gdp[\"GDP in US$\"].values)\n\n\n#Define a function named sigmoid\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n\n\n# Develop the sigmoid model with a random beta_1 and beta_2 values\nbeta_1 = 0.10\nbeta_2 = 1990.0\n#logistic function\ny_pred = sigmoid(X, beta_1 , beta_2)\n#plot initial prediction against datapoints\nplt.plot(X, y_pred*15000000000000)\nplt.plot(X, y, 'ro');\n\n\n\n\n\n# Normalizing the data\nx =X/max(X)\ny =y/max(y)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x, y)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 565.120191, beta_2 = 0.995458\n\n\n\nplt.figure(figsize=(8,5))\ny_pred = sigmoid(x, *popt)\nplt.plot(X, y, 'ro', label='original data')\nplt.plot(X,y_pred, linewidth=3.0, label='predicted fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df_gdp)) &lt; 0.8\nx_train = x[msk]\nx_test = x[~msk]\ny_train = y[msk]\ny_test = y[~msk]\n\n# build the model using train set\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x_train, y_train)\n\n# predict using test set\ny_hat = sigmoid(x_test, *popt)\n\n# Evaluation metrics\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , y_test))\n\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.99\n\n\n\nk = df_gdp[['GDP in US$']].shape[1]\nn = len(x_test)\n\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\nprint('Mean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is:', metrics.mean_absolute_error(y_test, y_hat))\nprint('Mean Squared Error(MSE) of Logistic (Sigmoid)Regression model is:', metrics.mean_squared_error(y_test, y_hat))\nprint('Root Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))\n# Explained variance score: 1 is perfect prediction\nprint('Explained Variance Score (EVS) of Logistic (Sigmoid)Regression model is:',explained_variance_score(y_test, y_hat))\n#Residual sum of squares (rss)\nprint(\"Residual sum of squares of Logistic (Sigmoid)Regression model is: %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint('R2 of Logistic (Sigmoid)Regression model is:',metrics.r2_score(y_test, y_hat))\nprint('R2 rounded of Logistic (Sigmoid)Regression model is:',(metrics.r2_score(y_test, y_hat)).round(2))\nr2 = r2_score(y_test, y_hat)\nr2_rounded = r2_score(y_test, y_hat).round(2)\nadjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)\nprint('Adjusted_r2 of Logistic (Sigmoid)Regression model is: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))\n\nMean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is: 0.01826421502174514\nMean Squared Error(MSE) of Logistic (Sigmoid)Regression model is: 0.0004353198095936058\nRoot Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is: 0.020864319054155728\nExplained Variance Score (EVS) of Logistic (Sigmoid)Regression model is: 0.994435059805147\nResidual sum of squares of Logistic (Sigmoid)Regression model is: 0.00\nR2 of Logistic (Sigmoid)Regression model is: 0.9934079607274738\nR2 rounded of Logistic (Sigmoid)Regression model is: 0.99\nAdjusted_r2 of Logistic (Sigmoid)Regression model is:  0.993"
  },
  {
    "objectID": "posts/nonlinregression/index.html#types-of-non-linear-regressions",
    "href": "posts/nonlinregression/index.html#types-of-non-linear-regressions",
    "title": "Nonlinear Regression",
    "section": "Types of Non-Linear Regressions",
    "text": "Types of Non-Linear Regressions\nNon-linear regression encompasses a wide range of mathematical functions that can be used to model complex relationships between variables. In this discussion, we’ll explore various forms of non-linear functions commonly used in regression analysis. We’ll cover quadratic, cubic, exponential, logarithmic, and sigmoidal (logistic) functions."
  },
  {
    "objectID": "posts/nonlinregression/index.html#cubic-functions-graph",
    "href": "posts/nonlinregression/index.html#cubic-functions-graph",
    "title": "Nonlinear Regression",
    "section": "Cubic function’s graph",
    "text": "Cubic function’s graph\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 1*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [-102.     -95.539  -89.352  -83.433  -77.776]\ny_noise: [-46.17 -11.98  -2.75 -13.89  15.62]\nydata: [-148.17 -107.52  -92.1   -97.33  -62.16]\n\n\n\n\n\nThis function has x3 and x2 as independent variables. Also, the graphic of this function is also not a straight line over the 2D plane. So this too is a non-linear function.\nSome other types of non-linear functions are:"
  },
  {
    "objectID": "posts/nonlinregression/index.html#non-linear-regression-example",
    "href": "posts/nonlinregression/index.html#non-linear-regression-example",
    "title": "Nonlinear Regression",
    "section": "Non-Linear Regression example",
    "text": "Non-Linear Regression example"
  },
  {
    "objectID": "posts/nonlinregression/index.html#load-the-country-gdp-dataset",
    "href": "posts/nonlinregression/index.html#load-the-country-gdp-dataset",
    "title": "Nonlinear Regression",
    "section": "Load the Country GDP dataset",
    "text": "Load the Country GDP dataset\n\ncountries = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/Countries GDP 1960-2020.csv')\ncountries.head()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n...\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n\n\n\n\n0\nAfrica Eastern and Southern\nAFE\n1.931311e+10\n1.972349e+10\n2.149392e+10\n2.573321e+10\n2.352744e+10\n2.681057e+10\n2.915216e+10\n3.017317e+10\n...\n9.430000e+11\n9.510000e+11\n9.640000e+11\n9.850000e+11\n9.200000e+11\n8.730000e+11\n9.850000e+11\n1.010000e+12\n1.010000e+12\n9.210000e+11\n\n\n1\nAfrica Western and Central\nAFW\n1.040428e+10\n1.112805e+10\n1.194335e+10\n1.267652e+10\n1.383858e+10\n1.486247e+10\n1.583285e+10\n1.442643e+10\n...\n6.710000e+11\n7.280000e+11\n8.210000e+11\n8.650000e+11\n7.610000e+11\n6.910000e+11\n6.840000e+11\n7.420000e+11\n7.950000e+11\n7.850000e+11\n\n\n2\nAustralia\nAUS\n1.860679e+10\n1.968306e+10\n1.992272e+10\n2.153993e+10\n2.380110e+10\n2.597715e+10\n2.730989e+10\n3.044462e+10\n...\n1.400000e+12\n1.550000e+12\n1.580000e+12\n1.470000e+12\n1.350000e+12\n1.210000e+12\n1.330000e+12\n1.430000e+12\n1.390000e+12\n1.330000e+12\n\n\n3\nAustria\nAUT\n6.592694e+09\n7.311750e+09\n7.756110e+09\n8.374175e+09\n9.169984e+09\n9.994071e+09\n1.088768e+10\n1.157943e+10\n...\n4.310000e+11\n4.090000e+11\n4.300000e+11\n4.420000e+11\n3.820000e+11\n3.960000e+11\n4.160000e+11\n4.550000e+11\n4.450000e+11\n4.330000e+11\n\n\n4\nBurundi\nBDI\n1.960000e+08\n2.030000e+08\n2.135000e+08\n2.327500e+08\n2.607500e+08\n1.589950e+08\n1.654446e+08\n1.782971e+08\n...\n2.235821e+09\n2.333308e+09\n2.451625e+09\n2.705783e+09\n3.104395e+09\n2.732809e+09\n2.748180e+09\n2.668496e+09\n2.631434e+09\n2.841786e+09\n\n\n\n\n5 rows × 63 columns"
  },
  {
    "objectID": "posts/nonlinregression/index.html#information-about-the-data",
    "href": "posts/nonlinregression/index.html#information-about-the-data",
    "title": "Nonlinear Regression",
    "section": "Information about the data",
    "text": "Information about the data\nThe dataset is a subset of data obtained from World Bank national accounts data, and OECD National Accounts data files. It contains Country names, Country codes and GDP for years from 1960 - 2020 as annual gross domestic income in US dollars for that year.\n\nInformation from the website:\nGDP at purchaser’s prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars. Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used.\n\nPeriodicity: Annual\nStatistical Concept and Methodology: Gross domestic product (GDP) represents the sum of value added by all its producers. Value added is the value of the gross output of producers less the value of intermediate goods and services consumed in production, before accounting for consumption of fixed capital in production. The United Nations System of National Accounts calls for value added to be valued at either basic prices (excluding net taxes on products) or producer prices (including net taxes on products paid by producers but excluding sales or value added taxes). Both valuations exclude transport charges that are invoiced separately by producers. Total GDP is measured at purchaser prices. Value added by industry is normally measured at basic prices."
  },
  {
    "objectID": "posts/nonlinregression/index.html#data-preprocessing",
    "href": "posts/nonlinregression/index.html#data-preprocessing",
    "title": "Nonlinear Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nchina = countries[countries[\"Country Name\"]== 'China']\nchina = china.drop(['Country Name', 'Country Code'], axis = 1)\nchina = china.T\nchina.columns = ['GDP in US$']\ndf_gdp = china.reset_index()\ndf_gdp.columns = ['Year', 'GDP in US$']\ndf_gdp.head()\n\n\n\n\n\n\n\n\nYear\nGDP in US$\n\n\n\n\n0\n1960\n5.971647e+10\n\n\n1\n1961\n5.005687e+10\n\n\n2\n1962\n4.720936e+10\n\n\n3\n1963\n5.070680e+10\n\n\n4\n1964\n5.970834e+10\n\n\n\n\n\n\n\n\ndf_gdp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 61 entries, 0 to 60\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Year        61 non-null     object \n 1   GDP in US$  61 non-null     float64\ndtypes: float64(1), object(1)\nmemory usage: 1.1+ KB\n\n\n\ndf_gdp['Year'] = df_gdp['Year'].astype('int32')\n\n\nplt.figure(figsize=(8,5))\nX = df_gdp['Year']\ny = df_gdp[\"GDP in US$\"]\nplt.plot(X, y, 'bo')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#method-development-logistic-sigmoid-regression",
    "href": "posts/nonlinregression/index.html#method-development-logistic-sigmoid-regression",
    "title": "Nonlinear Regression",
    "section": "Method Development : Logistic (Sigmoid) Regression",
    "text": "Method Development : Logistic (Sigmoid) Regression\nFrom an initial look at the plot, the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\nThe formula for the logistic function is the following:\nY^=11+eβ1(X−β2)\n\nβ1\n\nControls the curve’s steepness,\n\nβ2\n\nSlides the curve on the x-axis.\n\n\n\nBuilding The Model\n\nX, y = (df_gdp[\"Year\"].values, df_gdp[\"GDP in US$\"].values)\n\n\n#Define a function named sigmoid\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n\n\n# Develop the sigmoid model with a random beta_1 and beta_2 values\nbeta_1 = 0.10\nbeta_2 = 1990.0\n#logistic function\ny_pred = sigmoid(X, beta_1 , beta_2)\n#plot initial prediction against datapoints\nplt.plot(X, y_pred*15000000000000)\nplt.plot(X, y, 'ro');"
  },
  {
    "objectID": "posts/nonlinregression/index.html#find-the-best-parameters-for-the-fit-line",
    "href": "posts/nonlinregression/index.html#find-the-best-parameters-for-the-fit-line",
    "title": "Nonlinear Regression",
    "section": "Find the best parameters for the fit line",
    "text": "Find the best parameters for the fit line\nTo find the best parameters beta_1 and beta_2 values for the model. For this first normalize X and y:\nThen use curve_fit which uses non-linear least squares to fit the sigmoid function, to data to find Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.\npopt are optimized parameters.\n\n# Normalizing the data\nx =X/max(X)\ny =y/max(y)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x, y)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 565.120191, beta_2 = 0.995458\n\n\nPlot the resulting regression model with the optimized parameters popt\n\nplt.figure(figsize=(8,5))\ny_pred = sigmoid(x, *popt)\nplt.plot(X, y, 'ro', label='original data')\nplt.plot(X,y_pred, linewidth=3.0, label='predicted fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinregression/index.html#model-evaluation---logistic-sigmoid-regression",
    "href": "posts/nonlinregression/index.html#model-evaluation---logistic-sigmoid-regression",
    "title": "Nonlinear Regression",
    "section": "Model Evaluation - Logistic (Sigmoid) Regression",
    "text": "Model Evaluation - Logistic (Sigmoid) Regression\n\n# split data into train/test\nmsk = np.random.rand(len(df_gdp)) &lt; 0.8\nx_train = x[msk]\nx_test = x[~msk]\ny_train = y[msk]\ny_test = y[~msk]\n\n# build the model using train set\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x_train, y_train)\n\n# predict using test set\ny_hat = sigmoid(x_test, *popt)\n\n# Evaluation metrics\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , y_test))\n\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.99\n\n\n\nk = df_gdp[['GDP in US$']].shape[1]\nn = len(x_test)\n\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\nprint('Mean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is:', metrics.mean_absolute_error(y_test, y_hat))\nprint('Mean Squared Error(MSE) of Logistic (Sigmoid)Regression model is:', metrics.mean_squared_error(y_test, y_hat))\nprint('Root Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))\n# Explained variance score: 1 is perfect prediction\nprint('Explained Variance Score (EVS) of Logistic (Sigmoid)Regression model is:',explained_variance_score(y_test, y_hat))\n#Residual sum of squares (rss)\nprint(\"Residual sum of squares of Logistic (Sigmoid)Regression model is: %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint('R2 of Logistic (Sigmoid)Regression model is:',metrics.r2_score(y_test, y_hat))\nprint('R2 rounded of Logistic (Sigmoid)Regression model is:',(metrics.r2_score(y_test, y_hat)).round(2))\nr2 = r2_score(y_test, y_hat)\nr2_rounded = r2_score(y_test, y_hat).round(2)\nadjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)\nprint('Adjusted_r2 of Logistic (Sigmoid)Regression model is: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))\n\nMean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is: 0.01691289541887371\nMean Squared Error(MSE) of Logistic (Sigmoid)Regression model is: 0.0004875880086879474\nRoot Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is: 0.02208139508020151\nExplained Variance Score (EVS) of Logistic (Sigmoid)Regression model is: 0.9927396411660685\nResidual sum of squares of Logistic (Sigmoid)Regression model is: 0.00\nR2 of Logistic (Sigmoid)Regression model is: 0.9879656354848502\nR2 rounded of Logistic (Sigmoid)Regression model is: 0.99\nAdjusted_r2 of Logistic (Sigmoid)Regression model is:  0.986"
  },
  {
    "objectID": "posts/nonlinregression/index.html#quadratic-regression",
    "href": "posts/nonlinregression/index.html#quadratic-regression",
    "title": "Nonlinear Regression",
    "section": "Quadratic Regression:",
    "text": "Quadratic Regression:\nDescription: Quadratic regression is used when the relationship between the independent variable (x) and the dependent variable (y) follows a U-shaped or inverted U-shaped curve. It models a parabolic relationship. This means that as x changes, the rate of change in y is not constant; it varies with x in a quadratic manner."
  },
  {
    "objectID": "posts/nonlinregression/index.html#cubic-regression",
    "href": "posts/nonlinregression/index.html#cubic-regression",
    "title": "Nonlinear Regression",
    "section": "Cubic Regression:",
    "text": "Cubic Regression:\n\nDescription: Cubic regression is an extension of quadratic regression. It models a cubic relationship between x and y, allowing for capturing more complex curvature in the data. This type of regression is used when the relationship between the variables shows more pronounced S-shaped or inverted S-shaped behavior."
  },
  {
    "objectID": "posts/nonlinregression/index.html#exponential-regression",
    "href": "posts/nonlinregression/index.html#exponential-regression",
    "title": "Nonlinear Regression",
    "section": "Exponential Regression:",
    "text": "Exponential Regression:\n\nDescription: Exponential regression is used when the dependent variable (y) exhibits exponential growth or decay concerning the independent variable (x). In this type of regression, the rate of change in y is proportional to the current value of y. It is suitable for modeling processes like population growth or the decay of radioactive substances."
  },
  {
    "objectID": "posts/nonlinregression/index.html#logarithmic-regression",
    "href": "posts/nonlinregression/index.html#logarithmic-regression",
    "title": "Nonlinear Regression",
    "section": "Logarithmic Regression:",
    "text": "Logarithmic Regression:\n\nDescription: Logarithmic regression is suitable when the relationship between x and y can be expressed as a logarithmic function. It is often used for modeling diminishing returns or saturation effects. In this type of regression, the rate of change in y slows down as x increases, and it eventually reaches a plateau."
  },
  {
    "objectID": "posts/nonlinregression/index.html#sigmoidal-logistic-regression",
    "href": "posts/nonlinregression/index.html#sigmoidal-logistic-regression",
    "title": "Nonlinear Regression",
    "section": "Sigmoidal (Logistic) Regression:",
    "text": "Sigmoidal (Logistic) Regression:\n\nDescription: Sigmoidal regression, often referred to as logistic regression, models an S-shaped curve. It is commonly used for binary classification problems, where y represents probabilities ranging from 0 to 1. This type of regression is ideal for situations where you want to model the probability of an event occurring based on one or more predictor variables.\n\nThese types of non-linear regressions offer a diverse set of tools to model relationships in data that don’t follow a straight line. The choice of which type to use depends on the specific characteristics of your data and the underlying behavior you want to capture. Non-linear regression is valuable for capturing complex data patterns in various fields of study."
  },
  {
    "objectID": "posts/nonlinregression/index.html#example-of-nonlnear-regression",
    "href": "posts/nonlinregression/index.html#example-of-nonlnear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlnear Regression",
    "text": "Example of Nonlnear Regression\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/nonlinregression/index.html#types-of-non-linear-regressions-1",
    "href": "posts/nonlinregression/index.html#types-of-non-linear-regressions-1",
    "title": "Nonlinear Regression",
    "section": "Types of Non-Linear regressions",
    "text": "Types of Non-Linear regressions\nLinear regression models a linear relation between a dependent variable y and independent variable x. It had a simple equation, of degree 1, for example y = 5x + 2.\n\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 5*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\nNon-linear regressions are a relationship between independent variables x and a dependent variable y which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of k degrees (maximum power of x ).\ny=ax3+bx2+cx+d\nNon-linear functions can have elements like exponentials, logarithms, fractions, and others. For example: y=log(x)\nOr even, more complicated such as : y=log(ax3+bx2+cx+d)"
  },
  {
    "objectID": "posts/anomaly-detection/index.html",
    "href": "posts/anomaly-detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#train-validation-test-split",
    "href": "posts/anomaly-detection/index.html#train-validation-test-split",
    "title": "Anomaly/Outlier detection",
    "section": "Train-Validation-Test Split",
    "text": "Train-Validation-Test Split\n\n# Splitting the data by target class\ndata_0, data_1 = data[data['Class'] == 0], data[data['Class'] == 1]\n\n# Feature-target split\nX_0, y_0 = data_0.drop('Class', axis = 1), data_0['Class']\nX_1, y_1 = data_1.drop('Class', axis = 1), data_1['Class']\n\n# Splitting the authentic class and constructing the training set\nX_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size = 0.2, random_state = 40)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)\ndata_val_1, data_test_1 = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n\n# Splitting the fraudulent class\nX_val, X_test, y_val, y_test = train_test_split(X_1, y_1, test_size = 0.5, random_state = 40)\ndata_val_2, data_test_2 = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)\n\n# Merging data to construct the validation set and the test set\ndata_val, data_test = pd.concat([data_val_1, data_val_2], axis = 0), pd.concat([data_test_1, data_test_2], axis = 0)\nX_val, y_val = data_val.drop('Class', axis = 1), data_val['Class']\nX_test, y_test = data_test.drop('Class', axis = 1), data_test['Class']\n\n\n# Distribution of authentic and fraudulent transactions over training, validation and test set\nlabels = ['Train', 'Validation', 'Test']\nvalues_0 = [len(y_train[y_train == 0]), len(y_val[y_val == 0]), len(y_test[y_test == 0])]\nvalues_1 = [len(y_train[y_train == 1]), len(y_val[y_val == 1]), len(y_test[y_test == 1])]\nfig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'domain'}, {'type': 'domain'}]])\nfig.add_trace(go.Pie(values = values_0, labels = labels, hole = 0.5, textinfo = 'percent', title = \"Authentic\"),\n              row = 1, col = 1)\nfig.add_trace(go.Pie(values = values_1, labels = labels, hole = 0.5, textinfo = 'percent', title = \"Fraudulent\"),\n              row = 1, col = 2)\ntext_title = \"Distribution of authentic and fraudulent transactions over training, validation and test set\"\nfig.update_layout(height = 500, width = 800, showlegend = True, title = dict(text = text_title, x = 0.5, y = 0.95)) \nfig.show()\n\n\n                                                \n\n\n\n# Setting the number of bins\nbins_train = math.floor(len(X_train)**(1/3))"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#feature-engineering",
    "href": "posts/anomaly-detection/index.html#feature-engineering",
    "title": "Anomaly/Outlier detection",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nTime\n\n# Decomposing time\nfor df in [X_train, X_val, X_test]:\n    df['Day'], temp = df['Time'] // (24*60*60), df['Time'] % (24*60*60)\n    df['Hour'], temp = temp // (60*60), temp % (60*60)\n    df['Minute'], df['Second'] = temp // 60, temp % 60\nX_train[['Time', 'Day', 'Hour', 'Minute', 'Second']].head()\n\n\n\n\n\n\n\n\nTime\nDay\nHour\nMinute\nSecond\n\n\n\n\n19594\n30401.0\n0.0\n8.0\n26.0\n41.0\n\n\n124712\n77397.0\n0.0\n21.0\n29.0\n57.0\n\n\n167920\n118964.0\n1.0\n9.0\n2.0\n44.0\n\n\n47377\n43191.0\n0.0\n11.0\n59.0\n51.0\n\n\n41731\n40804.0\n0.0\n11.0\n20.0\n4.0\n\n\n\n\n\n\n\n\n# Visualization\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = False)\nsns.histplot(data = X_train, x = 'Time', bins = bins_train, ax = ax[0])\nsns.histplot(data = X_train, x = 'Hour', bins = 24, ax = ax[1])\nax[1].set_ylabel(\" \")\nplt.suptitle(\"Histograms of Time and Hour\", size = 14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#amount",
    "href": "posts/anomaly-detection/index.html#amount",
    "title": "Anomaly/Outlier detection",
    "section": "Amount",
    "text": "Amount\nThe distribution of Amount has extreme positive skewness. We apply the transformation x↦log(x+0.001) to this column and form the new column Amount_transformed. The positive constant 0.001 is added to deal with the zero-amount transactions, which leads to log 0, an undefined quantity.\n\n# Transformation of 'Amount'\nfor df in [X_train, X_val, X_test]:\n    df['Amount_transformed'] = np.log10(df['Amount'] + 0.001)\n\n\n# Visualization\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = False)\nsns.histplot(data = X_train, x = 'Amount', bins = bins_train, ax = ax[0])\nsns.histplot(data = X_train, x = 'Amount_transformed', bins = bins_train, ax = ax[1])\nax[1].set_ylabel(\" \")\nplt.suptitle(\"Histograms of Amount and Amount_transformed\", size = 14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Discarding unnecessary columns\nfor df in [X_train, X_val, X_test]:\n    df.drop(['Time', 'Day', 'Minute', 'Second', 'Amount'], axis = 1, inplace = True)"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#implementing-anomaly-detection",
    "href": "posts/anomaly-detection/index.html#implementing-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Implementing Anomaly Detection",
    "text": "Implementing Anomaly Detection\n\n# Normal pdf\ndef normal_density(x, mu, sigma):\n    \"\"\"\n    Computes univariate normal probability density function (pdf) with mean mu, standard deviation sigma\n    Args:\n      x (scalar)    : input observation\n      mu (scalar)   : mean\n      sigma (scalar): standard deviation (&gt; 0)\n    Returns:\n      f (scalar): value of the univariate normal pdf\n    \"\"\"\n    assert sigma &gt; 0, \"Standard deviation must be positive\"\n    f = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (1 / 2) * ((x - mu) / sigma)**2)\n    return f\n\n\n# Product of normal pdfs\ndef normal_product(x_vec, mu_vec, sigma_vec):\n    \"\"\"\n    Computes product of univariate normal densities\n    Args:\n      x_vec (array_like, shape (n,))    : vector of input observations\n      mu_vec (array_like, shape (n,))   : vector of means\n      sigma_vec (array_like, shape (n,)): vector of standard deviations (&gt; 0)\n    Returns:\n      f (scalar): product of univariate normal densities\n    \"\"\"\n    assert min(sigma_vec) &gt; 0, \"Standard deviation must be positive\"\n    assert len(mu_vec) == len(x_vec), \"Length of mean vector does not match length of input vector\"\n    assert len(sigma_vec) == len(x_vec), \"Length of standard deviation vector does not match length of input vector\"\n    f = 1\n    for i in range(len(x_vec)):\n        f = f * normal_density(x_vec[i], mu_vec[i], sigma_vec[i])\n    return f\n\nNext, we compute the vector of means and vector of standard deviations for the features in the training set. These estimates characterize the joint probability density function of the features, which will be used to detect anomalous observations.\n\n# Model fitting\nmu_train, sigma_train = X_train_fs.mean(), X_train_fs.std()\n\n\n# Function to predict anomaly based on probability density threshold\ndef model_normal(X, epsilon):\n    \"\"\"\n    Anomaly detection model\n    Args:\n      X (DataFrame, shape (m, n)): DataFrame of features\n      epsilon (scalar)           : threshold density value (&gt; 0)\n    Returns:\n      y (array_like, shape (m,)): predicted class labels\n    \"\"\"\n    y = []\n    for i in X.index:\n        prob_density = normal_product(X.loc[i].tolist(), mu_train, sigma_train)\n        y.append((prob_density &lt; epsilon).astype(int))\n    return y"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#threshold-tuning-on-validation-set",
    "href": "posts/anomaly-detection/index.html#threshold-tuning-on-validation-set",
    "title": "Anomaly/Outlier detection",
    "section": "Threshold Tuning on Validation Set",
    "text": "Threshold Tuning on Validation Set\nFirst, we construct some functions to compute and display the confusion matrix and to compute the F2-score, given the true labels and the predicted labels of the target.\n\n# Function to compute confusion matrix\ndef conf_mat(y_test, y_pred):\n    \"\"\"\n    Computes confusion matrix\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      confusion_mat (array): A 2D array representing a 2x2 confusion matrix\n    \"\"\"\n    y_test, y_pred = list(y_test), list(y_pred)\n    count, labels, confusion_mat = len(y_test), [0, 1], np.zeros(shape = (2, 2), dtype = int)\n    for i in range(2):\n        for j in range(2):\n            confusion_mat[i][j] = len([k for k in range(count) if y_test[k] == labels[i] and y_pred[k] == labels[j]])\n    return confusion_mat\n\n\n# Function to print confusion matrix\ndef conf_mat_heatmap(y_test, y_pred):\n    \"\"\"\n    Prints confusion matrix\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      Nothing, prints a heatmap representing a 2x2 confusion matrix\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    labels, confusion_mat_df = [0, 1], pd.DataFrame(confusion_mat, range(2), range(2))\n    plt.figure(figsize = (6, 4.75))\n    sns.heatmap(confusion_mat_df, annot = True, annot_kws = {\"size\": 16}, fmt = 'd')\n    plt.xticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.yticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.xlabel(\"Predicted label\", fontsize = 14)\n    plt.ylabel(\"True label\", fontsize = 14)\n    plt.title(\"Confusion Matrix\", fontsize = 14)\n    plt.grid(False)\n    plt.show()\n\n\n# Function to compute and return f2_score\ndef f2_score(y_test, y_pred):\n    \"\"\"\n    Computes accuracy, given true and predicted binary (0 or 1) labels\n    Args:\n      y_test (array_like): true binary (0 or 1) labels\n      y_pred (array_like): predicted binary (0 or 1) labels\n    Returns:\n      f2 (float): accuracy obtained from y_test and y_pred\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]\n    f2 = (5 * tp) / ((5 * tp) + (4 * fn) + fp)\n    return f2\n\n\n# Tuning the threshold of density value\nalpha_list, f2_list, f2_max, alpha_opt, y_val_pred_opt = [], [], 0.0, 0.0, np.zeros(len(y_val))\nfor alpha, j in itertools.product(np.arange(0.001, 0.051, 0.001), range(1)):\n    y_val_pred = model_normal(X_val_fs, epsilon = alpha**X_val_fs.shape[1])\n    f2 = f2_score(y_val, y_val_pred)\n    alpha_list.append(alpha)\n    f2_list.append(f2)\n    if f2 &gt; f2_max:\n        alpha_opt = alpha\n        y_val_pred_opt = y_val_pred\n        f2_max = f2\n\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_47740\\516892871.py:17: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n# Plotting F2-score over alpha\nplt.figure(figsize = (9, 6))\nplt.plot(alpha_list, f2_list)\nplt.xlabel(\"alpha\", fontsize = 14)\nplt.ylabel(\"F2-score\", fontsize = 14)\nplt.title(\"F2-score vs alpha\", fontsize = 14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Tuning summary\nprint(pd.Series({\n    \"Optimal alpha\": alpha_opt,\n    \"Optimal F2-score\": f2_score(y_val, y_val_pred_opt)\n}).to_string())\n\nOptimal alpha       0.009000\nOptimal F2-score    0.834671\n\n\n\n# Confusion matrix for predictions on the validation set\nconf_mat_heatmap(y_val, y_val_pred_opt)"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#prediction-and-evaluation-on-test-set",
    "href": "posts/anomaly-detection/index.html#prediction-and-evaluation-on-test-set",
    "title": "Anomaly/Outlier detection",
    "section": "Prediction and Evaluation on Test Set",
    "text": "Prediction and Evaluation on Test Set\n\n# Function to compute and print evaluation metrics\ndef evaluation(y_test, y_pred):\n    confusion_mat = conf_mat(y_test, y_pred)\n    tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]\n    print(pd.Series({\n        \"Accuracy\": (tp + tn) / (tn + fp + fn + tp),\n        \"Precision\": tp / (tp + fp),\n        \"Recall\": tp / (tp + fn),\n        \"F1-score\": (2 * tp) / ((2 * tp) + fn + fp),\n        \"F2-score\": (5 * tp) / ((5 * tp) + (4 * fn) + fp),\n        \"MCC\": ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n    }).to_string())\n\n\n# Prediction and evaluation on the test set\ny_test_normal = model_normal(X_test_fs, epsilon = alpha_opt**X_test_fs.shape[1])\nevaluation(y_test, y_test_normal)\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_47740\\516892871.py:17: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_47740\\2892512474.py:11: RuntimeWarning:\n\noverflow encountered in scalar multiply\n\n\n\nAccuracy       0.996687\nPrecision      0.798419\nRecall         0.821138\nF1-score       0.809619\nF2-score       0.816492\nMCC          171.242437\n\n\n\n# Confusion matrix for predictions on the test set\nconf_mat_heatmap(y_test, y_test_normal)"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#importing-libraries",
    "href": "posts/anomaly-detection/index.html#importing-libraries",
    "title": "Anomaly/Outlier detection",
    "section": "Importing libraries",
    "text": "Importing libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\n# Sample DataFrame for demonstration\ndata = {\n    'Feature1': [0.1, 0.2, 0.1, 0.3, 12.3, 0.2, 0.3],\n    'Feature2': [0.3, 0.1, 0.2, 0.1, 0.2, 15.1, 0.2],\n    # Add more features as needed\n}\ndf = pd.DataFrame(data)\n\n\n# Data visualization before applying Isolation Forest\nsns.pairplot(df)\nplt.title('Original Data Distribution')\nplt.show()\n\n\n\n\n\n# Data preprocessing\nscaler = StandardScaler()\nscaled_df = scaler.fit_transform(df)\n\n\n# Split the dataset\nX_train, X_test = train_test_split(scaled_df, test_size=0.2, random_state=42)\n\n\n# Initialize and fit the Isolation Forest model\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\niso_forest.fit(X_train)\n\nIsolationForest(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(random_state=42)\n\n\n\n# Predictions\npredictions = iso_forest.predict(X_test)\n\n\n# Map predictions to -1 for anomalies and 1 for normal\ndf_test = pd.DataFrame(X_test, columns=['Feature1', 'Feature2'])\ndf_test['anomaly'] = predictions\n\n\n# Visualizing the anomalies detected\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_test, x='Feature1', y='Feature2', hue='anomaly', palette={-1:'red', 1:'blue'})\nplt.title('Anomaly Detection with Isolation Forest')\nplt.show()"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#runtime-and-memory-usage",
    "href": "posts/anomaly-detection/index.html#runtime-and-memory-usage",
    "title": "Anomaly/Outlier detection",
    "section": "Runtime and memory usage",
    "text": "Runtime and memory usage\n\n# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\nstart = time.time()\n\n# Class representing the OS process and having memory_info() method to compute process memory usage\nprocess = psutil.Process(os.getpid())\n\nThe Anomaly Detection in credit card transactions is aiming to identify fraudulent transactions among a highly imbalanced dataset. Anomalies, or outliers, are rare observations that deviate significantly from the majority of data points. Anomaly detection techniques, including machine learning, are employed to automate this process. The project’s objective is to fit a probability distribution based on authentic transactions and use it to identify new transactions as authentic or fraudulent, with the target variable playing no role in constructing the distribution.\nThe evaluation metric considers True Positives (correctly predicting positive outcomes), True Negatives (correctly predicting negative outcomes), False Positives (incorrectly predicting positive outcomes), and False Negatives (incorrectly predicting negative outcomes). Metrics like Precision, Recall, F1-Score, and MCC are used to evaluate model performance. F2-Score is given special importance due to the higher cost associated with false negatives in this context.\nFeature selection is crucial due to high dimensionality, with 30 features in the dataset. Features are selected based on their ability to distinguish between authentic and fraudulent transactions, considering the distributions of each feature for both target classes. Features exhibiting distinct distributions are retained for classification purposes.\nThe dataset contains information on credit card transactions made by European cardholders and can be accessed via Kaggle. The data includes transaction time, PCA-transformed features (V1 to V28), transaction amount, and a binary class variable indicating authenticity (0 for authentic, 1 for fraudulent).\n\n# Loading the data\ndata = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/anomaly-detection/creditcard.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data.shape)}).to_string())\ndata.head()\n\nMemory usage         67.36 MB\nDataset shape    (284807, 31)\n\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#conclusion",
    "href": "posts/anomaly-detection/index.html#conclusion",
    "title": "Anomaly/Outlier detection",
    "section": "Conclusion",
    "text": "Conclusion\nHere, we addressed the challenge of a highly imbalanced dataset in credit card transactions, where fraudulent transactions are significantly less frequent compared to legitimate ones. The approach involved sophisticated feature engineering, where we extracted ‘Hour’ from the ‘Time’ attribute and log-transformed the ‘Amount’ feature to correct for skewness, resulting in a new feature called ‘Amount_transformed’.\nA crucial step in the analysis was the selection of key features that show distinct distribution patterns across the two classes of transactions. Out of 30 features engineered, 9 (V4, V11, V12, V14, V16, V17, V18, V19, and Hour) were identified as significantly influential in distinguishing between fraudulent and legitimate transactions. These features were used to fit a multivariate normal distribution to the training data, under the assumption of statistical independence among features, a reasonable assumption given that most of the features were obtained through PCA.\nThe final phase of the project involved optimizing a threshold for anomaly detection. This was achieved by iterating over a range of values and evaluating their performance using the F2-score on the validation set. The optimal threshold was found to be approximately 0.0099. With this threshold, the model achieved an F2-score of 0.834671 on the validation set. When applied to the test set, the model demonstrated a consistent performance with an F2-score of 0.816492. This method proved effective in flagging transactions as fraudulent based on their density values in the fitted distribution."
  },
  {
    "objectID": "posts/probability/index.html#probability-theory-and-random-variables",
    "href": "posts/probability/index.html#probability-theory-and-random-variables",
    "title": "Probability and Random Variables",
    "section": "Probability Theory and Random Variables",
    "text": "Probability Theory and Random Variables\nProbability theory is a branch of mathematics that deals with uncertainty and randomness. It provides a framework for understanding and quantifying uncertainty, making predictions in uncertain situations, and analyzing random phenomena. One of the fundamental concepts in probability theory is that of a random variable."
  },
  {
    "objectID": "posts/probability/index.html#probability-distribution",
    "href": "posts/probability/index.html#probability-distribution",
    "title": "Probability and Random Variables",
    "section": "Probability Distribution",
    "text": "Probability Distribution\nThe behavior of a random variable is described by its probability distribution. The probability distribution of a random variable specifies the probabilities associated with each possible outcome. There are two main types of probability distributions:\n\nProbability Mass Function (PMF): The PMF is used for discrete random variables and assigns a probability to each possible outcome. It is denoted as P(X = x), where X is the random variable and x is a specific value in its range.\nProbability Density Function (PDF): The PDF is used for continuous random variables. Unlike the PMF, which assigns probabilities to specific values, the PDF assigns probabilities to intervals. It is denoted as f(x), where x is a value in the range of the continuous random variable."
  },
  {
    "objectID": "posts/probability/index.html#properties-of-probability-distributions",
    "href": "posts/probability/index.html#properties-of-probability-distributions",
    "title": "Probability and Random Variables",
    "section": "Properties of Probability Distributions:",
    "text": "Properties of Probability Distributions:\nProbability distributions must satisfy certain properties:\n\nNon-negativity: Probabilities must be non-negative, meaning P(X = x) ≥ 0 for all x.\nNormalization: The sum of probabilities for all possible outcomes in a discrete distribution equals 1, while the integral of the PDF over its entire range equals 1 in a continuous distribution.\nProbability Rule: The total probability rule states that the probability of any outcome occurring is 1. For a discrete random variable, this is expressed as ∑ P(X = x) = 1, where the sum is taken over all possible values of X. For a continuous random variable, it is expressed as ∫ f(x) dx = 1, where the integral is taken over the entire range of X.\n\n\nExpected Value and Variance:\nTwo important measures of a random variable’s behavior are the expected value (mean) and variance:\n\nExpected Value (Mean): The expected value of a random variable X, denoted as E(X) or μ, represents the long-term average or center of its distribution. For a discrete random variable, it is calculated as E(X) = ∑ x * P(X = x), where the sum is taken over all possible values of X. For a continuous random variable, it is calculated as E(X) = ∫ x * f(x) dx.\nVariance: The variance of a random variable X, denoted as Var(X) or σ², measures the spread or dispersion of its distribution. It is calculated as Var(X) = E((X - μ)²), where μ is the mean.\n\nThese concepts are fundamental to understanding and working with random variables and probability distributions. They play a crucial role in various fields, including statistics, finance, engineering, and science, where uncertainty and randomness are inherent in data and observations. Probability theory provides a rigorous framework for making decisions and drawing conclusions in uncertain situations.\n\n\nExample of discrete probability distribution\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n# import seaborn\nimport seaborn as sns\n# settings for seaborn plotting style\nsns.set(color_codes=True)\n# settings for seaborn plot sizes\nsns.set(rc={'figure.figsize':(9.5,5)})\n\n\n\nExample of continuous probability distribution\n\nfrom scipy.stats import binom\n# set the parameters\nparam_n = 10 # number of trials\nparam_p = 0.3 # probability of success in a single trial\nk_value = 5 #NOTE: k = the number of successes\n\n# compute the probability/likelihood\np = binom.pmf(k=k_value, n=param_n, p=param_p) \n\nprint('For the discrete binomial distribution:')\nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {p}')\n\nFor the discrete binomial distribution:\nL(n=10,p=0.3|k=5) = p(k=5|n=10,p=0.3) = 0.10291934519999989\n\n\n\nfrom scipy.stats import norm\n# set the parameters\nparam_loc = 3 # mean\nparam_scale = 2 # standard deviation\nx_value = 5\n\n# compute the probability/likelihood\np = norm.pdf(x=x_value, loc=param_loc,scale=param_scale)\n\nprint('For the continuous gaussian distribution:')\nprint(f'L(\\u03BC={param_loc},\\u03C3={param_scale}|x={x_value}) = f(x={x_value}|\\u03BC={param_loc},\\u03C3={param_scale}) = {p}')\n\nFor the continuous gaussian distribution:\nL(μ=3,σ=2|x=5) = f(x=5|μ=3,σ=2) = 0.12098536225957168\n\n\n\nfrom scipy.stats import binom\n\n# compute the likelihoods\nk_value, param_n, param_p = 21, 50, 0.5,\nL1 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L1}')\nk_value, param_n, param_p = 21, 50, 0.1 \nL2 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L2}')\n\nL(n=50,p=0.5|k=21) = p(k=21|n=50,p=0.5) = 0.05979878464650326\nL(n=50,p=0.1|k=21) = p(k=21|n=50,p=0.1) = 3.17120935812694e-09\n\n\n\nN1 = L1*0.5\nprint(f'Numerator for p=0.5: {N1}')\nN2 = L2*0.5\nprint(f'Numerator for p=0.1: {N2}')\nD = L1*0.5 + L2*0.5\nprint(f'Denominator: {D}')\nprint(f'Probability that the coin is fair = P(n=50,p=0.5|k=21) = {N1/D}')\nprint(f'Probability that the coin is not fair with p=0.1 = P(n=50,p=0.1|k=21) = {N2/D}')\n\nNumerator for p=0.5: 0.02989939232325163\nNumerator for p=0.1: 1.58560467906347e-09\nDenominator: 0.02989939390885631\nProbability that the coin is fair = P(n=50,p=0.5|k=21) = 0.9999999469686682\nProbability that the coin is not fair with p=0.1 = P(n=50,p=0.1|k=21) = 5.303133180214092e-08\n\n\n\nfrom scipy.stats import binom\n\n# create an array of possible values for p\nx = np.arange(0, 1, 0.01)\nprint(f'x = {x}')\n\n# compute the likelihoods for each of these\nL = binom.pmf(k=21, n=50, p=x)\nprint(f'L = {L}')\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor)\nprior_prob = 1/len(L)\nD = np.sum(L*prior_prob)\nprint(f'D = {D}')\n\n# now compute the probability for each x-vaue using Bayes Theorem\nP= L*prior_prob / D\nprint(f'P={P}')\n\nx = [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n 0.98 0.99]\nL = [0.00000000e+00 5.03051889e-29 7.85919826e-23 2.91151333e-19\n 9.06395450e-17 7.25348613e-15 2.45513938e-13 4.58412540e-12\n 5.53259475e-11 4.78063092e-10 3.17120936e-09 1.69726135e-08\n 7.60318924e-08 2.93130403e-07 9.93884633e-07 3.01479250e-06\n 8.29489842e-06 2.09355401e-05 4.89233815e-05 1.06680074e-04\n 2.18490054e-04 4.22652048e-04 7.75911013e-04 1.35739963e-03\n 2.27102750e-03 3.64509592e-03 5.62797708e-03 8.37903745e-03\n 1.20546109e-02 1.67896719e-02 2.26767938e-02 2.97448198e-02\n 3.79402470e-02 4.71144533e-02 5.70195164e-02 6.73144675e-02\n 7.75825236e-02 8.73583214e-02 9.61627036e-02 1.03541417e-01\n 1.09103384e-01 1.12554139e-01 1.13720576e-01 1.12564260e-01\n 1.09182021e-01 1.03794162e-01 9.67220830e-02 8.83583049e-02\n 7.91325373e-02 6.94776192e-02 5.97987846e-02 5.04489532e-02\n 4.17117337e-02 3.37927303e-02 2.68187305e-02 2.08435419e-02\n 1.58587271e-02 1.18072653e-02 8.59823730e-03 6.12091333e-03\n 4.25704408e-03 2.89062850e-03 1.91488158e-03 1.23649874e-03\n 7.77578272e-04 4.75713982e-04 2.82815613e-04 1.63180122e-04\n 9.12491187e-05 4.93755751e-05 2.58087736e-05 1.30064688e-05\n 6.30605094e-06 2.93441385e-06 1.30702937e-06 5.55570175e-07\n 2.24596812e-07 8.60210251e-08 3.10768178e-08 1.05370880e-08\n 3.33389365e-09 9.77759321e-10 2.63745883e-10 6.48411637e-11\n 1.43726432e-11 2.83554206e-12 4.90199260e-13 7.28540438e-14\n 9.09042925e-15 9.24208907e-16 7.36690109e-17 4.37616968e-18\n 1.80861504e-19 4.72255713e-21 6.76493032e-23 4.27088598e-25\n 8.23432081e-28 2.43733344e-31 2.36488270e-36 5.45169094e-45]\nD = 0.01960784313725491\nP=[0.00000000e+00 2.56556463e-29 4.00819111e-23 1.48487180e-19\n 4.62261679e-17 3.69927793e-15 1.25212109e-13 2.33790395e-12\n 2.82162332e-11 2.43812177e-10 1.61731677e-09 8.65603289e-09\n 3.87762651e-08 1.49496506e-07 5.06881163e-07 1.53754418e-06\n 4.23039820e-06 1.06771254e-05 2.49509246e-05 5.44068377e-05\n 1.11429928e-04 2.15552544e-04 3.95714616e-04 6.92273812e-04\n 1.15822402e-03 1.85899892e-03 2.87026831e-03 4.27330910e-03\n 6.14785154e-03 8.56273267e-03 1.15651648e-02 1.51698581e-02\n 1.93495260e-02 2.40283712e-02 2.90799533e-02 3.43303784e-02\n 3.95670870e-02 4.45527439e-02 4.90429788e-02 5.28061226e-02\n 5.56427257e-02 5.74026108e-02 5.79974940e-02 5.74077728e-02\n 5.56828308e-02 5.29350224e-02 4.93282623e-02 4.50627355e-02\n 4.03575940e-02 3.54335858e-02 3.04973802e-02 2.57289662e-02\n 2.12729842e-02 1.72342925e-02 1.36775525e-02 1.06302064e-02\n 8.08795082e-03 6.02170530e-03 4.38510102e-03 3.12166580e-03\n 2.17109248e-03 1.47422053e-03 9.76589607e-04 6.30614356e-04\n 3.96564919e-04 2.42614131e-04 1.44235963e-04 8.32218623e-05\n 4.65370505e-05 2.51815433e-05 1.31624745e-05 6.63329908e-06\n 3.21608598e-06 1.49655106e-06 6.66584978e-07 2.83340789e-07\n 1.14544374e-07 4.38707228e-08 1.58491771e-08 5.37391486e-09\n 1.70028576e-09 4.98657254e-10 1.34510400e-10 3.30689935e-11\n 7.33004804e-12 1.44612645e-12 2.50001623e-13 3.71555623e-14\n 4.63611892e-15 4.71346543e-16 3.75711956e-17 2.23184654e-18\n 9.22393670e-20 2.40850414e-21 3.45011446e-23 2.17815185e-25\n 4.19950361e-28 1.24304005e-31 1.20609018e-36 2.78036238e-45]\n\n\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor) approximating the integral\nprior_prob = 1/len(L)\ndelta_theta = 0.01\nD = np.sum(L*prior_prob*delta_theta)\nprint(f'D = {D}')\n\n# now compute the probability for each x-value using Bayes Theorem\nP= L*prior_prob / D\nprint(f'P={P}')\n\nD = 0.00019607843137254904\nP=[0.00000000e+00 2.56556463e-27 4.00819111e-21 1.48487180e-17\n 4.62261679e-15 3.69927793e-13 1.25212109e-11 2.33790395e-10\n 2.82162332e-09 2.43812177e-08 1.61731677e-07 8.65603289e-07\n 3.87762651e-06 1.49496506e-05 5.06881163e-05 1.53754418e-04\n 4.23039820e-04 1.06771254e-03 2.49509246e-03 5.44068377e-03\n 1.11429928e-02 2.15552544e-02 3.95714616e-02 6.92273812e-02\n 1.15822402e-01 1.85899892e-01 2.87026831e-01 4.27330910e-01\n 6.14785154e-01 8.56273267e-01 1.15651648e+00 1.51698581e+00\n 1.93495260e+00 2.40283712e+00 2.90799533e+00 3.43303784e+00\n 3.95670870e+00 4.45527439e+00 4.90429788e+00 5.28061226e+00\n 5.56427257e+00 5.74026108e+00 5.79974940e+00 5.74077728e+00\n 5.56828308e+00 5.29350224e+00 4.93282623e+00 4.50627355e+00\n 4.03575940e+00 3.54335858e+00 3.04973802e+00 2.57289662e+00\n 2.12729842e+00 1.72342925e+00 1.36775525e+00 1.06302064e+00\n 8.08795082e-01 6.02170530e-01 4.38510102e-01 3.12166580e-01\n 2.17109248e-01 1.47422053e-01 9.76589607e-02 6.30614356e-02\n 3.96564919e-02 2.42614131e-02 1.44235963e-02 8.32218623e-03\n 4.65370505e-03 2.51815433e-03 1.31624745e-03 6.63329908e-04\n 3.21608598e-04 1.49655106e-04 6.66584978e-05 2.83340789e-05\n 1.14544374e-05 4.38707228e-06 1.58491771e-06 5.37391486e-07\n 1.70028576e-07 4.98657254e-08 1.34510400e-08 3.30689935e-09\n 7.33004804e-10 1.44612645e-10 2.50001623e-11 3.71555623e-12\n 4.63611892e-13 4.71346543e-14 3.75711956e-15 2.23184654e-16\n 9.22393670e-18 2.40850414e-19 3.45011446e-21 2.17815185e-23\n 4.19950361e-26 1.24304005e-29 1.20609018e-34 2.78036238e-43]"
  },
  {
    "objectID": "posts/probability/index.html#discrete-probability-distribution",
    "href": "posts/probability/index.html#discrete-probability-distribution",
    "title": "Probability and Random Variables",
    "section": "Discrete Probability Distribution:",
    "text": "Discrete Probability Distribution:\nA discrete probability distribution deals with random variables that can take on a finite or countable number of distinct values. Each of these values has an associated probability of occurrence. Discrete probability distributions are used in situations where outcomes are countable and distinct.\n\nExamples:\n\nBernoulli Distribution: Models binary outcomes like success/failure, heads/tails. It has a single parameter, p, which represents the probability of success.\nBinomial Distribution: Models the number of successes (k) in a fixed number of independent Bernoulli trials (n) with the same probability of success (p).\nPoisson Distribution: Models the number of events that occur in a fixed interval of time or space."
  },
  {
    "objectID": "posts/probability/index.html#continuous-probability-distribution",
    "href": "posts/probability/index.html#continuous-probability-distribution",
    "title": "Probability and Random Variables",
    "section": "Continuous Probability Distribution:",
    "text": "Continuous Probability Distribution:\nA continuous probability distribution deals with random variables that can take on an infinite number of values within a given interval. These distributions are used when outcomes can take any value within a range, often due to measurement precision or inherent continuity.\n\nExamples:\n\nNormal Distribution (Gaussian): Characterized by a symmetric bell-shaped curve with parameters μ (mean) and σ (standard deviation). Widely used in various fields to model natural phenomena.\nExponential Distribution: Models the time between events in a Poisson process, such as arrival times at a service center.\nUniform Distribution: Assigns equal probability density to all values within a specified interval.\n\nIn both types of distributions, the probabilities are defined in different ways. Discrete distributions use probability mass functions (PMFs) to describe the likelihood of discrete outcomes, while continuous distributions use probability density functions (PDFs) to describe the likelihood of outcomes within a continuous range. Understanding these probability distributions and their associated mathematical functions is crucial in statistics, data analysis, and making informed decisions in situations involving uncertainty and randomness."
  },
  {
    "objectID": "posts/probability/index.html#example-for-bernoulli-distribution",
    "href": "posts/probability/index.html#example-for-bernoulli-distribution",
    "title": "Probability and Random Variables",
    "section": "Example for Bernoulli Distribution",
    "text": "Example for Bernoulli Distribution\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Probability of success\np = 0.5\n\n# Possible outcomes\noutcomes = [0, 1]\n\n# PMF values\npmf_values = [bernoulli.pmf(x, p) for x in outcomes]\n\n# Plotting\nplt.bar(outcomes, pmf_values)\nplt.xticks(outcomes, ['Failure', 'Success'])\nplt.ylabel('Probability')\nplt.title('Bernoulli Distribution PMF (p = 0.5)')\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#example-forbinomial-distribution",
    "href": "posts/probability/index.html#example-forbinomial-distribution",
    "title": "Probability and Random Variables",
    "section": "Example forBinomial Distribution",
    "text": "Example forBinomial Distribution\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Parameters\nn = 10\np = 0.5\n\n# Possible number of successes\nk_values = range(n+1)\n\n# PMF values\npmf_values = [binom.pmf(k, n, p) for k in k_values]\n\n# Plotting\nplt.bar(k_values, pmf_values)\nplt.xlabel('Number of successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution PMF (n=10, p=0.5)')\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#example-for-poisson-distribution",
    "href": "posts/probability/index.html#example-for-poisson-distribution",
    "title": "Probability and Random Variables",
    "section": "Example for Poisson Distribution",
    "text": "Example for Poisson Distribution\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Parameters\nn = 10\np = 0.5\n\n# Possible number of successes\nk_values = range(n+1)\n\n# PMF values\npmf_values = [binom.pmf(k, n, p) for k in k_values]\n\n# Plotting\nplt.bar(k_values, pmf_values)\nplt.xlabel('Number of successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution PMF (n=10, p=0.5)')\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#normal-distribution",
    "href": "posts/probability/index.html#normal-distribution",
    "title": "Probability and Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parameters for the Normal distribution\nmu = 0  # mean\nsigma = 1  # standard deviation\n\n# Generate values within a range\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 1000)\n\n# Calculate the PDF\npdf = norm.pdf(x, mu, sigma)\n\n# Plotting\nplt.figure(figsize=(8, 4))\nplt.plot(x, pdf, label=f'Normal Distribution (μ={mu}, σ={sigma})')\nplt.title('Normal Distribution PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#exponential-distribution",
    "href": "posts/probability/index.html#exponential-distribution",
    "title": "Probability and Random Variables",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Rate parameter for the exponential distribution\nlambda_exp = 1\n\n# Generate values for the x-axis\nx_exp = np.linspace(0, 10, 1000)\n\n# Calculate the PDF\npdf_exp = expon.pdf(x_exp, scale=1/lambda_exp)\n\n# Plotting\nplt.figure(figsize=(8, 4))\nplt.plot(x_exp, pdf_exp, label=f'Exponential Distribution (λ={lambda_exp})')\nplt.title('Exponential Distribution PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#uniform-distribution",
    "href": "posts/probability/index.html#uniform-distribution",
    "title": "Probability and Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nfrom scipy.stats import uniform\n\n# Parameters for the uniform distribution\na, b = 0, 5\n\n# Generate values within the range\nx_uniform = np.linspace(a, b, 1000)\n\n# Calculate the PDF\npdf_uniform = uniform.pdf(x_uniform, loc=a, scale=b-a)\n\n# Plotting\nplt.figure(figsize=(8, 4))\nplt.plot(x_uniform, pdf_uniform, label=f'Uniform Distribution (a={a}, b={b})')\nplt.title('Uniform Distribution PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/linearRegression/index.html#example-for-linear-regression",
    "href": "posts/linearRegression/index.html#example-for-linear-regression",
    "title": "Linear Regression",
    "section": "Example for Linear Regression",
    "text": "Example for Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\n\nfrom sklearn.linear_model import LinearRegression\n\n%matplotlib inline\n\n\ndf=pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/linearRegression/Real estate.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\n0\n1\n2012.917\n32.0\n84.87882\n10\n24.98298\n121.54024\n37.9\n\n\n1\n2\n2012.917\n19.5\n306.59470\n9\n24.98034\n121.53951\n42.2\n\n\n2\n3\n2013.583\n13.3\n561.98450\n5\n24.98746\n121.54391\n47.3\n\n\n3\n4\n2013.500\n13.3\n561.98450\n5\n24.98746\n121.54391\n54.8\n\n\n4\n5\n2012.833\n5.0\n390.56840\n5\n24.97937\n121.54245\n43.1\n\n\n\n\n\n\n\n\ndf.shape\n\n(414, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 414 entries, 0 to 413\nData columns (total 8 columns):\n #   Column                                  Non-Null Count  Dtype  \n---  ------                                  --------------  -----  \n 0   No                                      414 non-null    int64  \n 1   X1 transaction date                     414 non-null    float64\n 2   X2 house age                            414 non-null    float64\n 3   X3 distance to the nearest MRT station  414 non-null    float64\n 4   X4 number of convenience stores         414 non-null    int64  \n 5   X5 latitude                             414 non-null    float64\n 6   X6 longitude                            414 non-null    float64\n 7   Y house price of unit area              414 non-null    float64\ndtypes: float64(6), int64(2)\nmemory usage: 26.0 KB\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nNo\nX1 transaction date\nX2 house age\nX3 distance to the nearest MRT station\nX4 number of convenience stores\nX5 latitude\nX6 longitude\nY house price of unit area\n\n\n\n\nNo\n1.000000\n-0.048658\n-0.032808\n-0.013573\n-0.012699\n-0.010110\n-0.011059\n-0.028587\n\n\nX1 transaction date\n-0.048658\n1.000000\n0.017549\n0.060880\n0.009635\n0.035058\n-0.041082\n0.087491\n\n\nX2 house age\n-0.032808\n0.017549\n1.000000\n0.025622\n0.049593\n0.054420\n-0.048520\n-0.210567\n\n\nX3 distance to the nearest MRT station\n-0.013573\n0.060880\n0.025622\n1.000000\n-0.602519\n-0.591067\n-0.806317\n-0.673613\n\n\nX4 number of convenience stores\n-0.012699\n0.009635\n0.049593\n-0.602519\n1.000000\n0.444143\n0.449099\n0.571005\n\n\nX5 latitude\n-0.010110\n0.035058\n0.054420\n-0.591067\n0.444143\n1.000000\n0.412924\n0.546307\n\n\nX6 longitude\n-0.011059\n-0.041082\n-0.048520\n-0.806317\n0.449099\n0.412924\n1.000000\n0.523287\n\n\nY house price of unit area\n-0.028587\n0.087491\n-0.210567\n-0.673613\n0.571005\n0.546307\n0.523287\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True,cmap='Reds')\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "index.html#understanding-ml-key-methods-and-applications",
    "href": "index.html#understanding-ml-key-methods-and-applications",
    "title": "Getting to Know about Machine Learning",
    "section": "Understanding ML: Key Methods and Applications",
    "text": "Understanding ML: Key Methods and Applications\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\nProbability theory deals with uncertainty and randomness. Random variables represent uncertain quantities. They come in two types: discrete (distinct values) and continuous (any value within a range). Probability distributions describe how random variables behave, either assigning probabilities to specific values (PMF for discrete) or intervals (PDF for continuous). These concepts are crucial in fields like statistics and machine learning for modeling and decision-making in uncertain scenarios.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\nNon-linear regression is a statistical approach for modeling and analyzing relationships between variables that don’t follow a straight-line pattern, allowing for curved and complex relationships in data analysis.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\nClustering in machine learning is a grouping technique that organizes data points with similar characteristics into clusters. It’s an unsupervised learning method used for various tasks such as customer segmentation and anomaly detection. Common clustering algorithms include K-means, hierarchical clustering, and DBSCAN.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\nClassification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#machine-learning-blogs",
    "href": "index.html#machine-learning-blogs",
    "title": "Getting to Know about Machine Learning",
    "section": "Machine Learning Blogs:",
    "text": "Machine Learning Blogs:\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\nProbability theory deals with uncertainty and randomness. Random variables represent uncertain quantities. They come in two types: discrete (distinct values) and continuous (any value within a range). Probability distributions describe how random variables behave, either assigning probabilities to specific values (PMF for discrete) or intervals (PDF for continuous). These concepts are crucial in fields like statistics and machine learning for modeling and decision-making in uncertain scenarios.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\nNon-linear regression is a statistical approach for modeling and analyzing relationships between variables that don’t follow a straight-line pattern, allowing for curved and complex relationships in data analysis.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\nClustering in machine learning is a grouping technique that organizes data points with similar characteristics into clusters. It’s an unsupervised learning method used for various tasks such as customer segmentation and anomaly detection. Common clustering algorithms include K-means, hierarchical clustering, and DBSCAN.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\nClassification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nDec 6, 2023\n\n\nSamheeta\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  }
]