---
title: "Anomaly/Outlier detection"
author: "Samheeta"
date: "2023-12-6"
jupyter: python3
description: "Anomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns."
---

**Contents:**

-   Introduction to Anomaly or Outlier Detection.

-   Example of Anomaly Detection 

-   Data Visualization

-   Data processing

-   Anomaly Detection

#### **Anomaly or Outlier Detection**

Anomaly or outlier detection is a crucial aspect of data analysis and machine learning, involving the identification of data points or observations that deviate significantly from the expected or normal behavior within a dataset. These anomalies, often representing rare events, errors, or unusual patterns, are detected using various techniques such as statistical methods, machine learning algorithms, distance-based approaches, and domain-specific rules, enabling their recognition in applications like fraud detection, quality control, network security, and predictive maintenance, ultimately enhancing decision-making and problem-solving in diverse fields.

In the context of machine learning, anomaly detection can be approached using various techniques and algorithms, including:

- Statistical Methods: Statistical approaches involve using statistical measures such as mean, standard deviation, and probability distributions to identify outliers. Data points that fall outside predefined statistical thresholds are considered anomalies.

- Machine Learning Algorithms: Machine learning models can be trained to detect anomalies. Some popular algorithms for this purpose include:

One-Class SVM (Support Vector Machine): It is a supervised learning algorithm that learns to classify data points as either "normal" or "anomalous" based on the majority class (usually "normal" data).

Isolation Forest: This algorithm isolates anomalies by recursively partitioning the data into subsets, making it efficient for high-dimensional datasets.

Autoencoders: These neural networks are used for unsupervised learning and can learn to reconstruct normal data. Data points that are poorly reconstructed are considered anomalies.

- Distance-Based Methods: These methods compute distances or dissimilarities between data points and identify outliers as those with unusually large distances from their nearest neighbors. k-nearest neighbors (KNN) and DBSCAN (Density-Based Spatial Clustering of Applications with Noise) are examples of distance-based techniques.

- Time Series Anomaly Detection: In time series data, specialized methods like Seasonal Decomposition of Time Series (STL), Exponential Smoothing, or Recurrent Neural Networks (RNNs) can be used to detect anomalies over time.

- Rule-Based Systems: Domain-specific knowledge can be applied to define rules that identify anomalies based on specific criteria. This approach is often used in fields like cybersecurity.

- Unsupervised and Semi-Supervised Learning: Anomaly detection can be performed in an unsupervised manner, where the model learns from data without the need for labeled anomalies, or in a semi-supervised manner with a limited amount of labeled data.

The choice of method depends on the characteristics of the data, the specific problem, and the available resources. Anomaly detection is a valuable tool in machine learning for uncovering irregularities and potential issues within datasets, leading to improved decision-making and problem-solving in various domains.

## Importing libraries

```{python}

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

```


```{python}
# Sample DataFrame for demonstration
data = {
    'Feature1': [0.1, 0.2, 0.1, 0.3, 12.3, 0.2, 0.3],
    'Feature2': [0.3, 0.1, 0.2, 0.1, 0.2, 15.1, 0.2],
    # Add more features as needed
}
df = pd.DataFrame(data)

```

```{python}
# Data visualization before applying Isolation Forest
sns.pairplot(df)
plt.title('Original Data Distribution')
plt.show()

```

```{python}
# Data preprocessing
scaler = StandardScaler()
scaled_df = scaler.fit_transform(df)

```
```{python}

# Split the dataset
X_train, X_test = train_test_split(scaled_df, test_size=0.2, random_state=42)

```
```{python}

# Initialize and fit the Isolation Forest model
iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)
iso_forest.fit(X_train)
```

```{python}
# Predictions
predictions = iso_forest.predict(X_test)



```

```{python}

# Map predictions to -1 for anomalies and 1 for normal
df_test = pd.DataFrame(X_test, columns=['Feature1', 'Feature2'])
df_test['anomaly'] = predictions
```

```{python}

# Visualizing the anomalies detected
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_test, x='Feature1', y='Feature2', hue='anomaly', palette={-1:'red', 1:'blue'})
plt.title('Anomaly Detection with Isolation Forest')
plt.show()
```
