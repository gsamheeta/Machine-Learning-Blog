---
title: "Nonlinear Regression"
author: "Samheeta"
date: "2023-12-6"
jupyter: python3
description: "Non-linear regression is a statistical approach for modeling and analyzing relationships between variables that don't follow a straight-line pattern, allowing for curved and complex relationships in data analysis."
---

**Contents:**

-   What Is Nonlinear Regression?

-   Types of Non-Linear Regressions

-   Example of Nonlinear Regression with [Countries GDP dataset](https://www.kaggle.com/datasets/rinichristy/countries-gdp-19602020).

-   Model Development and Evaluation

-   Visual Representation:

# Nonlinear Regression

![Nonlinear Regression](image1.png){fig-alt="Nonlinear Regression"}

Non-linear regression is a statistical modeling technique used to analyze and make predictions based on data that exhibits a non-linear or curvilinear relationship between variables. In contrast to linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression is essential when the data follows a more complex pattern. In this introduction, we will explore why non-linear regression is needed and how it differs from linear regression, especially when dealing with data that shows a curvy trend.

## Types of Non-Linear Regressions

Non-linear regression encompasses a wide range of mathematical functions that can be used to model complex relationships between variables. In this discussion, we'll explore various forms of non-linear functions commonly used in regression analysis. We'll cover quadratic, cubic, exponential, logarithmic, and sigmoidal (logistic) functions.

## Quadratic Regression:

Description: Quadratic regression is used when the relationship between the independent variable (x) and the dependent variable (y) follows a U-shaped or inverted U-shaped curve. It models a parabolic relationship. This means that as 
x changes, the rate of change in y is not constant; it varies with x in a quadratic manner.

## Cubic Regression:

- Description: Cubic regression is an extension of quadratic regression. It models a cubic relationship between x and y, allowing for capturing more complex curvature in the data. This type of regression is used when the relationship between the variables shows more pronounced S-shaped or inverted S-shaped behavior.

## Exponential Regression:

- Description: Exponential regression is used when the dependent variable (y) exhibits exponential growth or decay concerning the independent variable (x). In this type of regression, the rate of change in y is proportional to the current value of y. It is suitable for modeling processes like population growth or the decay of radioactive substances.

## Logarithmic Regression:

- Description: Logarithmic regression is suitable when the relationship between 
x and y can be expressed as a logarithmic function. It is often used for modeling diminishing returns or saturation effects. In this type of regression, the rate of change in y slows down as x increases, and it eventually reaches a plateau.

## Sigmoidal (Logistic) Regression:

- Description: Sigmoidal regression, often referred to as logistic regression, models an S-shaped curve. It is commonly used for binary classification problems, where y represents probabilities ranging from 0 to 1. This type of regression is ideal for situations where you want to model the probability of an event occurring based on one or more predictor variables.

These types of non-linear regressions offer a diverse set of tools to model relationships in data that don't follow a straight line. The choice of which type to use depends on the specific characteristics of your data and the underlying behavior you want to capture. Non-linear regression is valuable for capturing complex data patterns in various fields of study.

## Example of Nonlnear Regression

```{python}
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```

## Types of Non-Linear regressions
Linear regression models a linear relation between a dependent variable y and independent variable x. It had a simple equation, of degree 1, for example y = 5x + 2.

```{python}

x = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
y = 5*(x) + 2
y_noise = 2 * np.random.normal(size=x.size)
ydata = y + y_noise
#plt.figure(figsize=(8,6))
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```

Non-linear regressions are a relationship between independent variables  x
  and a dependent variable  y
  which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of  k
  degrees (maximum power of  x
 ).

 y=ax3+bx2+cx+d 
 
Non-linear functions can have elements like exponentials, logarithms, fractions, and others. For example:
y=log(x)
 
Or even, more complicated such as :
y=log(ax3+bx2+cx+d)
 
## Quadratic
Y=X2

```{python}
x = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
y = np.power(x,2)
ydata = y 
print('x:', x[0:5])
print('y:', y[0:5])
print('ydata:', ydata[0:5])
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```



```{python}
x = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
y = np.power(x,2)
y_noise = np.random.normal(size=x.size)
ydata = y + y_noise
print('x:', x[0:5])
print('y:', y[0:5])
print('y_noise:', y_noise[0:5].round(2))
print('ydata:', ydata[0:5].round(2))
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()

```



```{python}
x = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
y = np.power(x,2)
y_noise = 2 * np.random.normal(size=x.size)
ydata = y + y_noise
print('x:', x[0:5])
print('y:', y[0:5])
print('y_noise:', y_noise[0:5].round(2))
print('ydata:', ydata[0:5].round(2)) 
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r')
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```

As can be seen, this function has x2 as independent variables. Also, the graphic of this function is not a straight line over the 2D plane. So this is a non-linear function.

## Cubic function's graph

```{python}
x = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
y = 1*(x**3) + 1*(x**2) + 1*x + 3
y_noise = 20 * np.random.normal(size=x.size)
ydata = y + y_noise
print('x:', x[0:5])
print('y:', y[0:5])
print('y_noise:', y_noise[0:5].round(2))
print('ydata:', ydata[0:5].round(2)) 
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```

This function has x3 and x2 as independent variables. Also, the graphic of this function is also not a straight line over the 2D plane. So this too is a non-linear function.

Some other types of non-linear functions are:

## Exponential
An exponential function with base c is defined by
Y=a+bcX
 
where b ≠0, c > 0 , c ≠1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.

```{python}
X = np.arange(-5.0, 5.0, 0.1)
##You can adjust the slope and intercept to verify the changes in the graph
Y= np.exp(X)
Y_noise = 7 * np.random.normal(size=X.size)
Ydata = Y + Y_noise
print('X:', X[0:5])
print('Y:', Y[0:5].round(3))
print('Y_noise:', Y_noise[0:5].round(2))
print('Ydata:', Ydata[0:5].round(2)) 
plt.plot(X,Y, '-r') 
plt.plot(X, Ydata,  'bo')
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```
## Logarithmic
The response  y
  is a results of applying logarithmic map from input  x
 's to output variable  y
 . It is one of the simplest form of log(): i.e.
y=log(x)
 
Also, consider that instead of  x
 ,  X
  can be, which can be polynomial representation of the  x
 's. In general form it would be written as
y=log(X)

```{python}
X = np.arange(0.01, 5, 0.1)
Y = np.log(X)
Y_noise = 0.25*np.random.normal(size=X.size)
Ydata = Y + Y_noise
print('X:', X[0:5])
print('Y:', Y[0:5].round(2))
print('Y_noise:', Y_noise[0:5].round(2))
print('Ydata:', Ydata[0:5].round(2)) 
plt.plot(X,Y, '-r') 
plt.plot(X, Ydata,  'bo')
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```


```{python}
X = np.arange(0.01, 5, 0.1)
X = np.power(X,2)
Y = np.log(X)
Y_noise = 0.5 * np.random.normal(size=X.size)
Ydata = Y + Y_noise
print('X:', X[0:5].round(2))
print('Y:', Y[0:5].round(2))
print('Y_noise:', Y_noise[0:5].round(2))
print('Ydata:', Ydata[0:5].round(2)) 
plt.plot(X,Y, '-r') 
plt.plot(X, Ydata,  'bo')
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```

## Sigmoidal/Logistic

Y=a+b1+c(X−d)

```{python}
X = np.arange(-5.0, 5.0, 0.1)
Y = 1-4/(1+np.power(3, X-2))
Y_noise = 0.25 * np.random.normal(size=X.size) 
Ydata = Y + Y_noise
print('X:', X[0:5])
print('Y:', Y[0:5].round(2))
print('Y_noise:', Y_noise[0:5].round(2))
print('Ydata:', Ydata[0:5].round(2)) 
plt.plot(X,Y, '-r') 
plt.plot(X, Ydata,  'bo')
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()
```

## Non-Linear Regression example

## Load the Country GDP dataset

```{python}
countries = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/Countries GDP 1960-2020.csv')
countries.head()
```

## Information about the data
The dataset is a subset of data obtained from World Bank national accounts data, and OECD National Accounts data files. It contains Country names, Country codes and GDP for years from 1960 - 2020 as annual gross domestic income in US dollars for that year.

### Information from the website:

GDP at purchaser's prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars. Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used.

#### Periodicity: Annual

Statistical Concept and Methodology: Gross domestic product (GDP) represents the sum of value added by all its producers. Value added is the value of the gross output of producers less the value of intermediate goods and services consumed in production, before accounting for consumption of fixed capital in production. The United Nations System of National Accounts calls for value added to be valued at either basic prices (excluding net taxes on products) or producer prices (including net taxes on products paid by producers but excluding sales or value added taxes). Both valuations exclude transport charges that are invoiced separately by producers. Total GDP is measured at purchaser prices. Value added by industry is normally measured at basic prices.

## Data Preprocessing

```{python}
china = countries[countries["Country Name"]== 'China']
china = china.drop(['Country Name', 'Country Code'], axis = 1)
china = china.T
china.columns = ['GDP in US$']
df_gdp = china.reset_index()
df_gdp.columns = ['Year', 'GDP in US$']
df_gdp.head()
```


```{python}
df_gdp.info()
```


```{python}
df_gdp['Year'] = df_gdp['Year'].astype('int32')
```



```{python}
plt.figure(figsize=(8,5))
X = df_gdp['Year']
y = df_gdp["GDP in US$"]
plt.plot(X, y, 'bo')
plt.ylabel('GDP')
plt.xlabel('Year')
plt.show()
```

## Method Development : Logistic (Sigmoid) Regression
From an initial look at the plot, the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:

The formula for the logistic function is the following:

Y^=11+eβ1(X−β2)
 
β1
 : Controls the curve's steepness,

β2
 : Slides the curve on the x-axis.

### Building The Model

```{python}
X, y = (df_gdp["Year"].values, df_gdp["GDP in US$"].values)
```



```{python}
#Define a function named sigmoid
def sigmoid(x, Beta_1, Beta_2):
     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
     return y
```




```{python}
# Develop the sigmoid model with a random beta_1 and beta_2 values
beta_1 = 0.10
beta_2 = 1990.0
#logistic function
y_pred = sigmoid(X, beta_1 , beta_2)
#plot initial prediction against datapoints
plt.plot(X, y_pred*15000000000000)
plt.plot(X, y, 'ro');
```

## Find the best parameters for the fit line

To find the best parameters beta_1 and beta_2 values for the model. For this first normalize X and y:

Then use curve_fit which uses non-linear least squares to fit the sigmoid function, to data to find Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.

popt are optimized parameters.

```{python}
# Normalizing the data
x =X/max(X)
y =y/max(y)
```

```{python}
from scipy.optimize import curve_fit
popt, pcov = curve_fit(sigmoid, x, y)
#print the final parameters
print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
```

Plot the resulting regression model with the optimized parameters popt

```{python}
plt.figure(figsize=(8,5))
y_pred = sigmoid(x, *popt)
plt.plot(X, y, 'ro', label='original data')
plt.plot(X,y_pred, linewidth=3.0, label='predicted fit')
plt.legend(loc='best')
plt.ylabel('GDP')
plt.xlabel('Year')
plt.show()
```

## Model Evaluation - Logistic (Sigmoid) Regression

```{python}
# split data into train/test
msk = np.random.rand(len(df_gdp)) < 0.8
x_train = x[msk]
x_test = x[~msk]
y_train = y[msk]
y_test = y[~msk]

# build the model using train set
from scipy.optimize import curve_fit
popt, pcov = curve_fit(sigmoid, x_train, y_train)

# predict using test set
y_hat = sigmoid(x_test, *popt)

# Evaluation metrics
print("Mean absolute error: %.2f" % np.mean(np.absolute(y_hat - y_test)))
print("Residual sum of squares (MSE): %.2f" % np.mean((y_hat - y_test) ** 2))
from sklearn.metrics import r2_score
print("R2-score: %.2f" % r2_score(y_hat , y_test))
```

```{python}
k = df_gdp[['GDP in US$']].shape[1]
n = len(x_test)


from sklearn import metrics
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score
print('Mean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is:', metrics.mean_absolute_error(y_test, y_hat))
print('Mean Squared Error(MSE) of Logistic (Sigmoid)Regression model is:', metrics.mean_squared_error(y_test, y_hat))
print('Root Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))
# Explained variance score: 1 is perfect prediction
print('Explained Variance Score (EVS) of Logistic (Sigmoid)Regression model is:',explained_variance_score(y_test, y_hat))
#Residual sum of squares (rss)
print("Residual sum of squares of Logistic (Sigmoid)Regression model is: %.2f" % np.mean((y_hat - y_test) ** 2))
print('R2 of Logistic (Sigmoid)Regression model is:',metrics.r2_score(y_test, y_hat))
print('R2 rounded of Logistic (Sigmoid)Regression model is:',(metrics.r2_score(y_test, y_hat)).round(2))
r2 = r2_score(y_test, y_hat)
r2_rounded = r2_score(y_test, y_hat).round(2)
adjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)
print('Adjusted_r2 of Logistic (Sigmoid)Regression model is: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))
```
