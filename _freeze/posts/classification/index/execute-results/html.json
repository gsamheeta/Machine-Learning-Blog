{
  "hash": "b8d8bb5bee339b2302a155fa746b676d",
  "result": {
    "markdown": "---\ntitle: Classification\nauthor: Samheeta\ndate: 2023-12-6\ndescription: 'Classification in machine learning is the technique of categorizing data points into predefined groups or labels based on their attributes. This process allows algorithms to understand and accurately predict the category of new, unobserved examples.'\n---\n\n**Contents:**\n\n-   Introduction to Classification.\n\n-   Different types of classification.\n\n-   Example of Linear Regression with [Loan dataset](https://www.kaggle.com/datasets/itssuru/loan-data/).\n\n-   Data Visualization\n\n-   Data processing\n\n-   Model implementation\n\n-   Evaluation metrics implementation\n\n## Introduction to Classification\n\nIn recent times, various industries are grappling with extremely large and diverse datasets. Processing this data manually is not only time-consuming but may also lack long-term value. To enhance return on investment, a range of strategies are being employed, from basic automation to advanced machine learning techniques. This blog will focus on a key concept in this field: classification in machine learning.\n\nWe'll begin by explaining the essence of classification within the realm of Machine Learning, followed by a discussion on the two types of learners in machine learning, as well as distinguishing between classification and regression. Next, the blog will explore various real-life applications of classification. Subsequently, we'll delve into the different kinds of classification methods and examine some specific examples of classification algorithms. To conclude, the blog will offer practical experience in implementing several of these algorithms.\n\n## Defining Classification in Machine Learning\n\nClassification is a type of supervised learning in machine learning where the goal is to accurately predict the label of given input data. In this process, the model is thoroughly trained with training data and then evaluated using test data before it is applied to predict labels for new, unseen data.\n\nFor example, a machine learning algorithm can be trained to determine whether an email is spam or not (ham). However, before delving into the nuances of classification, it's essential to understand the difference between two classifications of learners: lazy and eager learners, and to clear up common confusions between classification and regression.\n![Machine Learning Algorithms for Classification (src: <https://www.datacamp.com/blog/classification-machine-learning>)](classification(Img1).png){}\nLazy Learners vs. Eager Learners:\nIn the realm of machine learning classification, there are two distinct types of learners: lazy and eager learners.\n\nEager learners are those algorithms that construct a model based on the training dataset before making predictions on new data. These algorithms invest more time in the training phase to better generalize from the data by learning the weights, but they are quicker in making predictions. Examples of eager learners include:\n\nLogistic Regression.\nSupport Vector Machine.\nDecision Trees.\nArtificial Neural Networks.\nConversely, lazy learners, or instance-based learners, do not immediately generate a model from the training data. This is where the 'lazy' aspect comes in. They store the training data and, for each prediction, they search the entire training set for the nearest neighbor, resulting in slower prediction times. Examples of lazy learners are:\n\nK-Nearest Neighbor.\nCase-based reasoning.\nHowever, there are techniques like BallTrees and KDTrees that can enhance prediction speed in these algorithms.\n\n## Examples of Machine Learning Classification in Real Life \n\nSupervised Machine Learning Classification has different applications in multiple domains of our day-to-day life. Below are some examples. \n\n**Application in Healthcare :**\n\nSupervised machine learning classification plays a significant role in various aspects of everyday life. Here are some key examples, particularly in the healthcare domain. \n\n- During the COVID-19 pandemic, supervised machine learning models were crucial in predicting whether individuals were infected with the virus \n\n- Machine learning models are also utilized by researchers to forecast the likelihood of new diseases emerging in the future.\n\n**Education :**\n\n-  Utilizing machine learning for categorizing various documents like text, video, and audio.\n\n- Automatically determining the language of students' application documents.\n\n- Analyzing students' feedback about professors to gauge sentiments.\n\n\n**Transportation :**\n\n- Predicting areas likely to experience increased traffic volume.\n\n- Anticipating potential transportation problems in specific areas due to weather conditions.\n\n**Sustainable Agriculture :**\n\n- Using classification models to identify the most suitable land types for different seeds.\n\n- Forecasting weather conditions to aid farmers in taking appropriate preventive actions.\n\n## Different Types of Classification Tasks in Machine Learning \n\nThere are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications. \n\n-   **Binary Classification**: In a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat. Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification. \n\n-   **Multi-Class Classification**: The multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. In the following case, the model correctly classified the image to be a plane. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include but are not limited to:\n\n-   [Naive Bayes](https://monkeylearn.com/blog/classification-algorithms/#naive-bayes): Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It's particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\n\n-   [K-Nearest Neighbors](https://monkeylearn.com/blog/classification-algorithms/#knn): K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It's versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\n\n-   [Support Vector Machines](https://monkeylearn.com/blog/classification-algorithms/#svm): SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data.\n\n-   [Logistic Regression](https://monkeylearn.com/blog/classification-algorithms/#logistic-regression): Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It's simple, interpretable, and effective for binary and multiclass classification tasks.\n\n-   **Multi-Label Classification**: In multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. Such a scenario can be observed in different domains, such as auto-tagging in Natural Language Processing, where a given text can contain multiple topics. Similarly to computer vision, an image can contain multiple objects, as illustrated below: the model predicted that the image contains: a plane, a boat, a truck, and a dog. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification. We can cite: \n-   Multi-label Decision Trees\n-   Multi-label Gradient Boosting\n-   Multi-label Random Forests\n\n-   **Imbalanced Classification**: For the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data. Let’s consider the following 3-class classification scenario where the training data contains: 60% of trucks, 25% of planes, and 15% of boats.The imbalanced classification problem could occur in the following scenario:\n\n- Fraudulent transaction detections in financial industries\n- Rare disease diagnosis \n- Customer churn analysis\n\nUsing conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise. \n\nSo, does that mean that such problems are left behind?\n\nOf course not! We can use multiple approaches to tackle the imbalance problem in a dataset. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms. \n\n- **Sampling Techniques** \nThese techniques aim to balance the distribution of the original by: \n\n- Cluster-based Oversampling:\n- Random undersampling: random elimination of examples from the majority class. \n- SMOTE Oversampling: random replication of examples from the minority class. \n- **Cost-Sensitive Algorithms** \nThese algorithms take into consideration the cost of misclassification. They aim to minimize the total cost generated by the models.\n\n- Cost-sensitive Decision Trees.\n- Cost-sensitive Logistic Regression. \n- Cost-sensitive Support Vector Machines.\n\n## Example: Distribution of Loans in the Dataset \n- Look at the first five observations in the dataset. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nloan_data = pd.read_csv(\"loan_data.csv\")\nloan_data.head()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>credit.policy</th>\n      <th>purpose</th>\n      <th>int.rate</th>\n      <th>installment</th>\n      <th>log.annual.inc</th>\n      <th>dti</th>\n      <th>fico</th>\n      <th>days.with.cr.line</th>\n      <th>revol.bal</th>\n      <th>revol.util</th>\n      <th>inq.last.6mths</th>\n      <th>delinq.2yrs</th>\n      <th>pub.rec</th>\n      <th>not.fully.paid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>debt_consolidation</td>\n      <td>0.1189</td>\n      <td>829.10</td>\n      <td>11.350407</td>\n      <td>19.48</td>\n      <td>737</td>\n      <td>5639.958333</td>\n      <td>28854</td>\n      <td>52.1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>credit_card</td>\n      <td>0.1071</td>\n      <td>228.22</td>\n      <td>11.082143</td>\n      <td>14.29</td>\n      <td>707</td>\n      <td>2760.000000</td>\n      <td>33623</td>\n      <td>76.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>debt_consolidation</td>\n      <td>0.1357</td>\n      <td>366.86</td>\n      <td>10.373491</td>\n      <td>11.63</td>\n      <td>682</td>\n      <td>4710.000000</td>\n      <td>3511</td>\n      <td>25.6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>debt_consolidation</td>\n      <td>0.1008</td>\n      <td>162.34</td>\n      <td>11.350407</td>\n      <td>8.10</td>\n      <td>712</td>\n      <td>2699.958333</td>\n      <td>33667</td>\n      <td>73.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>credit_card</td>\n      <td>0.1426</td>\n      <td>102.92</td>\n      <td>11.299732</td>\n      <td>14.97</td>\n      <td>667</td>\n      <td>4066.000000</td>\n      <td>4740</td>\n      <td>39.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n- Borrowers profile in the dataset. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n# Helper function for data distribution\n# Visualize the proportion of borrowers\ndef show_loan_distrib(data):\n  count = \"\"\n  if isinstance(data, pd.DataFrame):\n      count = data[\"not.fully.paid\"].value_counts()\n  else:\n      count = data.value_counts()\n\n\n  count.plot(kind = 'pie', explode = [0, 0.1], \n\n              figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n  plt.ylabel(\"Loan: Fully Paid Vs. Not Fully Paid\")\n  plt.legend([\"Fully Paid\", \"Not Fully Paid\"])\n  plt.show()\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=481 height=463}\n:::\n:::\n\n\nFrom the graphic above, we notice that 84% of the borrowers paid their loans back, and only 16% didn’t pay them back, which makes the dataset really imbalanced.\n\n- **Variable Types**\nBefore further, we need to check the variables’ type so that we can encode those that need to be encoded. \n\nWe notice that all the columns are continuous variables, except the purpose attribute, which needs to be encoded. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Check column types\nprint(loan_data.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncredit.policy          int64\npurpose               object\nint.rate             float64\ninstallment          float64\nlog.annual.inc       float64\ndti                  float64\nfico                   int64\ndays.with.cr.line    float64\nrevol.bal              int64\nrevol.util           float64\ninq.last.6mths         int64\ndelinq.2yrs            int64\npub.rec                int64\nnot.fully.paid         int64\ndtype: object\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nencoded_loan_data = pd.get_dummies(loan_data, prefix=\"purpose\",   \n\n                                   drop_first=True)\nprint(encoded_loan_data.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncredit.policy                   int64\nint.rate                      float64\ninstallment                   float64\nlog.annual.inc                float64\ndti                           float64\nfico                            int64\ndays.with.cr.line             float64\nrevol.bal                       int64\nrevol.util                    float64\ninq.last.6mths                  int64\ndelinq.2yrs                     int64\npub.rec                         int64\nnot.fully.paid                  int64\npurpose_credit_card              bool\npurpose_debt_consolidation       bool\npurpose_educational              bool\npurpose_home_improvement         bool\npurpose_major_purchase           bool\npurpose_small_business           bool\ndtype: object\n```\n:::\n:::\n\n\n## Separate data into train and test\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX = encoded_loan_data.drop('not.fully.paid', axis = 1)\ny = encoded_loan_data['not.fully.paid']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, \n\n                                           stratify = y, random_state=2022)\n```\n:::\n\n\n## Application of the Sampling Strategies \nWe will explore two sampling strategies here: random undersampling, and SMOTE oversampling.\n\n## Random Undersampling \nWe will undersample the majority class, which corresponds to the “fully paid” (class 0). \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nX_train_cp = X_train.copy()\nX_train_cp['not.fully.paid'] = y_train\ny_0 = X_train_cp[X_train_cp['not.fully.paid'] == 0]\ny_1 = X_train_cp[X_train_cp['not.fully.paid'] == 1]\ny_0_undersample = y_0.sample(y_1.shape[0])\nloan_data_undersample = pd.concat([y_0_undersample, y_1], axis = 0)\n\n\n# Visualize the proportion of borrowers\nshow_loan_distrib(loan_data_undersample)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=481 height=463}\n:::\n:::\n\n\n## SMOTE Oversampling\nPerform oversampling on the minority class\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_train_SMOTE, y_train_SMOTE = smote.fit_resample(X_train,y_train)\n# Visualize the proportion of borrowers\nshow_loan_distrib(y_train_SMOTE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning:\n\nCould not find the number of physical cores for the following reason:\n[WinError 2] The system cannot find the file specified\nReturning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n    cpu_info = subprocess.run(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=481 height=463}\n:::\n:::\n\n\nAfter applying the sampling strategies, we observe that the dataset is equally distributed across the different types of borrowers.\n\n## Application of Some Machine Learning Classification Algorithms\nThis section will apply these two classification algorithms to the SMOTE smote sampled dataset. The same training approach can be applied to undersampled data as well. \n\n## Logistic Regression \nThis is an explainable algorithm. It classifies a data point by modeling its probability of belonging to a given class using the sigmoid function. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nX = loan_data_undersample.drop('not.fully.paid', axis = 1)\ny = loan_data_undersample['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify = y, random_state=2022)\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(X_train, y_train)\ny_pred = logistic_classifier.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[104  57]\n [ 74  87]]\n              precision    recall  f1-score   support\n\n           0       0.58      0.65      0.61       161\n           1       0.60      0.54      0.57       161\n\n    accuracy                           0.59       322\n   macro avg       0.59      0.59      0.59       322\nweighted avg       0.59      0.59      0.59       322\n\n```\n:::\n:::\n\n\nThese results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data. \n\n## Support Vector Machines \nThis algorithm can be used for both classification and regression. It learns to draw the hyperplane (decision boundary) by using the margin to maximization principle. This decision boundary is drawn through the two closest support vectors. \n\nSVM provides a transformation strategy called kernel tricks used to project non-learner separable data onto a higher dimension space to make them linearly separable. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\nsvc_classifier = SVC(kernel='linear')\nsvc_classifier.fit(X_train, y_train)\n\n\n# Make Prediction & print the result\ny_pred = svc_classifier.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.60      0.49      0.54       161\n           1       0.57      0.67      0.62       161\n\n    accuracy                           0.58       322\n   macro avg       0.58      0.58      0.58       322\nweighted avg       0.58      0.58      0.58       322\n\n```\n:::\n:::\n\n\nThese results can be of course improved with more feature engineering and fine-tuning. But they are better than using the original imbalanced data. \n\n## Summary\n\nIn summary, this blog has thoroughly explored the fundamental aspects of classification in machine learning, offering insights into its diverse applications across various fields. Additionally, it delved into the practical implementation of key machine learning models like Logistic Regression and Support Vector Machine, highlighting their application in scenarios involving both undersampling and SMOTE oversampling techniques to achieve a balanced dataset for model training.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}