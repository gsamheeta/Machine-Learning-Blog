{
  "hash": "d477e8b22b2eec7e37ce5502d2bb59a8",
  "result": {
    "markdown": "---\ntitle: Anomaly/Outlier detection\nauthor: Samheeta\ndate: 2023-12-6\ndescription: 'Anomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.'\n---\n\n**Contents:**\n\n-   Introduction to Anomaly or Outlier Detection.\n\n-   Example of Anomaly Detection \n\n-   Data Visualization\n\n-   Data processing\n\n-   Anomaly Detection\n\n#### **Anomaly or Outlier Detection**\n\nAnomaly or outlier detection is a crucial aspect of data analysis and machine learning, involving the identification of data points or observations that deviate significantly from the expected or normal behavior within a dataset. These anomalies, often representing rare events, errors, or unusual patterns, are detected using various techniques such as statistical methods, machine learning algorithms, distance-based approaches, and domain-specific rules, enabling their recognition in applications like fraud detection, quality control, network security, and predictive maintenance, ultimately enhancing decision-making and problem-solving in diverse fields.\n\nIn the context of machine learning, anomaly detection can be approached using various techniques and algorithms, including:\n\n- Statistical Methods: Statistical approaches involve using statistical measures such as mean, standard deviation, and probability distributions to identify outliers. Data points that fall outside predefined statistical thresholds are considered anomalies.\n\n- Machine Learning Algorithms: Machine learning models can be trained to detect anomalies. Some popular algorithms for this purpose include:\n\nOne-Class SVM (Support Vector Machine): It is a supervised learning algorithm that learns to classify data points as either \"normal\" or \"anomalous\" based on the majority class (usually \"normal\" data).\n\nIsolation Forest: This algorithm isolates anomalies by recursively partitioning the data into subsets, making it efficient for high-dimensional datasets.\n\nAutoencoders: These neural networks are used for unsupervised learning and can learn to reconstruct normal data. Data points that are poorly reconstructed are considered anomalies.\n\n- Distance-Based Methods: These methods compute distances or dissimilarities between data points and identify outliers as those with unusually large distances from their nearest neighbors. k-nearest neighbors (KNN) and DBSCAN (Density-Based Spatial Clustering of Applications with Noise) are examples of distance-based techniques.\n\n- Time Series Anomaly Detection: In time series data, specialized methods like Seasonal Decomposition of Time Series (STL), Exponential Smoothing, or Recurrent Neural Networks (RNNs) can be used to detect anomalies over time.\n\n- Rule-Based Systems: Domain-specific knowledge can be applied to define rules that identify anomalies based on specific criteria. This approach is often used in fields like cybersecurity.\n\n- Unsupervised and Semi-Supervised Learning: Anomaly detection can be performed in an unsupervised manner, where the model learns from data without the need for labeled anomalies, or in a semi-supervised manner with a limited amount of labeled data.\n\nThe choice of method depends on the characteristics of the data, the specific problem, and the available resources. Anomaly detection is a valuable tool in machine learning for uncovering irregularities and potential issues within datasets, leading to improved decision-making and problem-solving in various domains.\n\n## Importing libraries\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Sample DataFrame for demonstration\ndata = {\n    'Feature1': [0.1, 0.2, 0.1, 0.3, 12.3, 0.2, 0.3],\n    'Feature2': [0.3, 0.1, 0.2, 0.1, 0.2, 15.1, 0.2],\n    # Add more features as needed\n}\ndf = pd.DataFrame(data)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Data visualization before applying Isolation Forest\nsns.pairplot(df)\nplt.title('Original Data Distribution')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=481 height=476}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Data preprocessing\nscaler = StandardScaler()\nscaled_df = scaler.fit_transform(df)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Split the dataset\nX_train, X_test = train_test_split(scaled_df, test_size=0.2, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Initialize and fit the Isolation Forest model\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\niso_forest.fit(X_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Predictions\npredictions = iso_forest.predict(X_test)\n\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Map predictions to -1 for anomalies and 1 for normal\ndf_test = pd.DataFrame(X_test, columns=['Feature1', 'Feature2'])\ndf_test['anomaly'] = predictions\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Visualizing the anomalies detected\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_test, x='Feature1', y='Feature2', hue='anomaly', palette={-1:'red', 1:'blue'})\nplt.title('Anomaly Detection with Isolation Forest')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=840 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}