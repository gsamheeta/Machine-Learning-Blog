{
  "hash": "a796b7401e67dd9394fab3d49cb68d91",
  "result": {
    "markdown": "---\ntitle: Nonlinear Regression\nauthor: Samheeta\ndate: 2023-12-6\ndescription: 'Non-linear regression is a statistical approach for modeling and analyzing relationships between variables that don''t follow a straight-line pattern, allowing for curved and complex relationships in data analysis.'\n---\n\n**Contents:**\n\n-   What Is Nonlinear Regression?\n\n-   Types of Non-Linear Regressions\n\n-   Example of Nonlinear Regression with [Countries GDP dataset](https://www.kaggle.com/datasets/rinichristy/countries-gdp-19602020).\n\n-   Model Development and Evaluation\n\n-   Visual Representation:\n\n# Nonlinear Regression\n\n![Nonlinear Regression](image1.png){fig-alt=\"Nonlinear Regression\"}\n\nNon-linear regression is a statistical modeling technique used to analyze and make predictions based on data that exhibits a non-linear or curvilinear relationship between variables. In contrast to linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression is essential when the data follows a more complex pattern. In this introduction, we will explore why non-linear regression is needed and how it differs from linear regression, especially when dealing with data that shows a curvy trend.\n\n## Types of Non-Linear Regressions\n\nNon-linear regression encompasses a wide range of mathematical functions that can be used to model complex relationships between variables. In this discussion, we'll explore various forms of non-linear functions commonly used in regression analysis. We'll cover quadratic, cubic, exponential, logarithmic, and sigmoidal (logistic) functions.\n\n## Quadratic Regression:\n\nDescription: Quadratic regression is used when the relationship between the independent variable (x) and the dependent variable (y) follows a U-shaped or inverted U-shaped curve. It models a parabolic relationship. This means that as \nx changes, the rate of change in y is not constant; it varies with x in a quadratic manner.\n\n## Cubic Regression:\n\n- Description: Cubic regression is an extension of quadratic regression. It models a cubic relationship between x and y, allowing for capturing more complex curvature in the data. This type of regression is used when the relationship between the variables shows more pronounced S-shaped or inverted S-shaped behavior.\n\n## Exponential Regression:\n\n- Description: Exponential regression is used when the dependent variable (y) exhibits exponential growth or decay concerning the independent variable (x). In this type of regression, the rate of change in y is proportional to the current value of y. It is suitable for modeling processes like population growth or the decay of radioactive substances.\n\n## Logarithmic Regression:\n\n- Description: Logarithmic regression is suitable when the relationship between \nx and y can be expressed as a logarithmic function. It is often used for modeling diminishing returns or saturation effects. In this type of regression, the rate of change in y slows down as x increases, and it eventually reaches a plateau.\n\n## Sigmoidal (Logistic) Regression:\n\n- Description: Sigmoidal regression, often referred to as logistic regression, models an S-shaped curve. It is commonly used for binary classification problems, where y represents probabilities ranging from 0 to 1. This type of regression is ideal for situations where you want to model the probability of an event occurring based on one or more predictor variables.\n\nThese types of non-linear regressions offer a diverse set of tools to model relationships in data that don't follow a straight line. The choice of which type to use depends on the specific characteristics of your data and the underlying behavior you want to capture. Non-linear regression is valuable for capturing complex data patterns in various fields of study.\n\n## Example of Nonlnear Regression\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\n## Types of Non-Linear regressions\nLinear regression models a linear relation between a dependent variable y and independent variable x. It had a simple equation, of degree 1, for example y = 5x + 2.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 5*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=596 height=429}\n:::\n:::\n\n\nNon-linear regressions are a relationship between independent variables  x\n  and a dependent variable  y\n  which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of  k\n  degrees (maximum power of  x\n ).\n\n y=ax3+bx2+cx+d \n \nNon-linear functions can have elements like exponentials, logarithms, fractions, and others. For example:\ny=log(x)\n \nOr even, more complicated such as :\ny=log(ax3+bx2+cx+d)\n \n## Quadratic\nY=X2\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\nydata = y \nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('ydata:', ydata[0:5])\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\nydata: [25.   24.01 23.04 22.09 21.16]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=585 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [ 0.36  0.3  -2.75 -0.36  0.8 ]\nydata: [25.36 24.31 20.29 21.73 21.96]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=585 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [25.   24.01 23.04 22.09 21.16]\ny_noise: [ 0.54 -1.87  1.47 -1.94  2.05]\nydata: [25.54 22.14 24.51 20.15 23.21]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=587 height=429}\n:::\n:::\n\n\nAs can be seen, this function has x2 as independent variables. Also, the graphic of this function is not a straight line over the 2D plane. So this is a non-linear function.\n\n## Cubic function's graph\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 1*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nprint('x:', x[0:5])\nprint('y:', y[0:5])\nprint('y_noise:', y_noise[0:5].round(2))\nprint('ydata:', ydata[0:5].round(2)) \nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx: [-5.  -4.9 -4.8 -4.7 -4.6]\ny: [-102.     -95.539  -89.352  -83.433  -77.776]\ny_noise: [-46.17 -11.98  -2.75 -13.89  15.62]\nydata: [-148.17 -107.52  -92.1   -97.33  -62.16]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=604 height=429}\n:::\n:::\n\n\nThis function has x3 and x2 as independent variables. Also, the graphic of this function is also not a straight line over the 2D plane. So this too is a non-linear function.\n\nSome other types of non-linear functions are:\n\n## Exponential\nAn exponential function with base c is defined by\nY=a+bcX\n \nwhere b ≠0, c > 0 , c ≠1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nX = np.arange(-5.0, 5.0, 0.1)\n##You can adjust the slope and intercept to verify the changes in the graph\nY= np.exp(X)\nY_noise = 7 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(3))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [0.007 0.007 0.008 0.009 0.01 ]\nY_noise: [ 10.54   8.59   5.27   7.3  -13.51]\nYdata: [ 10.55   8.6    5.28   7.31 -13.5 ]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=596 height=430}\n:::\n:::\n\n\n## Logarithmic\nThe response  y\n  is a results of applying logarithmic map from input  x\n 's to output variable  y\n . It is one of the simplest form of log(): i.e.\ny=log(x)\n \nAlso, consider that instead of  x\n ,  X\n  can be, which can be polynomial representation of the  x\n 's. In general form it would be written as\ny=log(X)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nX = np.arange(0.01, 5, 0.1)\nY = np.log(X)\nY_noise = 0.25*np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX: [0.01 0.11 0.21 0.31 0.41]\nY: [-4.61 -2.21 -1.56 -1.17 -0.89]\nY_noise: [-0.04 -0.11  0.27  0.18 -0.15]\nYdata: [-4.65 -2.31 -1.29 -1.   -1.05]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=587 height=430}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nX = np.arange(0.01, 5, 0.1)\nX = np.power(X,2)\nY = np.log(X)\nY_noise = 0.5 * np.random.normal(size=X.size)\nYdata = Y + Y_noise\nprint('X:', X[0:5].round(2))\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX: [0.   0.01 0.04 0.1  0.17]\nY: [-9.21 -4.41 -3.12 -2.34 -1.78]\nY_noise: [-0.68  0.02 -0.1  -0.27  0.13]\nYdata: [-9.89 -4.39 -3.22 -2.61 -1.66]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=598 height=429}\n:::\n:::\n\n\n## Sigmoidal/Logistic\n\nY=a+b1+c(X−d)\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX = np.arange(-5.0, 5.0, 0.1)\nY = 1-4/(1+np.power(3, X-2))\nY_noise = 0.25 * np.random.normal(size=X.size) \nYdata = Y + Y_noise\nprint('X:', X[0:5])\nprint('Y:', Y[0:5].round(2))\nprint('Y_noise:', Y_noise[0:5].round(2))\nprint('Ydata:', Ydata[0:5].round(2)) \nplt.plot(X,Y, '-r') \nplt.plot(X, Ydata,  'bo')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX: [-5.  -4.9 -4.8 -4.7 -4.6]\nY: [-3. -3. -3. -3. -3.]\nY_noise: [-0.44  0.23 -0.22  0.46  0.4 ]\nYdata: [-3.44 -2.77 -3.22 -2.54 -2.59]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=587 height=429}\n:::\n:::\n\n\n## Non-Linear Regression example\n\n## Load the Country GDP dataset\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ncountries = pd.read_csv('C:/Users/Lenovo/Desktop/MLBlog-gh-pages/posts/nonlinregression/Countries GDP 1960-2020.csv')\ncountries.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country Name</th>\n      <th>Country Code</th>\n      <th>1960</th>\n      <th>1961</th>\n      <th>1962</th>\n      <th>1963</th>\n      <th>1964</th>\n      <th>1965</th>\n      <th>1966</th>\n      <th>1967</th>\n      <th>...</th>\n      <th>2011</th>\n      <th>2012</th>\n      <th>2013</th>\n      <th>2014</th>\n      <th>2015</th>\n      <th>2016</th>\n      <th>2017</th>\n      <th>2018</th>\n      <th>2019</th>\n      <th>2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Africa Eastern and Southern</td>\n      <td>AFE</td>\n      <td>1.931311e+10</td>\n      <td>1.972349e+10</td>\n      <td>2.149392e+10</td>\n      <td>2.573321e+10</td>\n      <td>2.352744e+10</td>\n      <td>2.681057e+10</td>\n      <td>2.915216e+10</td>\n      <td>3.017317e+10</td>\n      <td>...</td>\n      <td>9.430000e+11</td>\n      <td>9.510000e+11</td>\n      <td>9.640000e+11</td>\n      <td>9.850000e+11</td>\n      <td>9.200000e+11</td>\n      <td>8.730000e+11</td>\n      <td>9.850000e+11</td>\n      <td>1.010000e+12</td>\n      <td>1.010000e+12</td>\n      <td>9.210000e+11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Africa Western and Central</td>\n      <td>AFW</td>\n      <td>1.040428e+10</td>\n      <td>1.112805e+10</td>\n      <td>1.194335e+10</td>\n      <td>1.267652e+10</td>\n      <td>1.383858e+10</td>\n      <td>1.486247e+10</td>\n      <td>1.583285e+10</td>\n      <td>1.442643e+10</td>\n      <td>...</td>\n      <td>6.710000e+11</td>\n      <td>7.280000e+11</td>\n      <td>8.210000e+11</td>\n      <td>8.650000e+11</td>\n      <td>7.610000e+11</td>\n      <td>6.910000e+11</td>\n      <td>6.840000e+11</td>\n      <td>7.420000e+11</td>\n      <td>7.950000e+11</td>\n      <td>7.850000e+11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Australia</td>\n      <td>AUS</td>\n      <td>1.860679e+10</td>\n      <td>1.968306e+10</td>\n      <td>1.992272e+10</td>\n      <td>2.153993e+10</td>\n      <td>2.380110e+10</td>\n      <td>2.597715e+10</td>\n      <td>2.730989e+10</td>\n      <td>3.044462e+10</td>\n      <td>...</td>\n      <td>1.400000e+12</td>\n      <td>1.550000e+12</td>\n      <td>1.580000e+12</td>\n      <td>1.470000e+12</td>\n      <td>1.350000e+12</td>\n      <td>1.210000e+12</td>\n      <td>1.330000e+12</td>\n      <td>1.430000e+12</td>\n      <td>1.390000e+12</td>\n      <td>1.330000e+12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Austria</td>\n      <td>AUT</td>\n      <td>6.592694e+09</td>\n      <td>7.311750e+09</td>\n      <td>7.756110e+09</td>\n      <td>8.374175e+09</td>\n      <td>9.169984e+09</td>\n      <td>9.994071e+09</td>\n      <td>1.088768e+10</td>\n      <td>1.157943e+10</td>\n      <td>...</td>\n      <td>4.310000e+11</td>\n      <td>4.090000e+11</td>\n      <td>4.300000e+11</td>\n      <td>4.420000e+11</td>\n      <td>3.820000e+11</td>\n      <td>3.960000e+11</td>\n      <td>4.160000e+11</td>\n      <td>4.550000e+11</td>\n      <td>4.450000e+11</td>\n      <td>4.330000e+11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Burundi</td>\n      <td>BDI</td>\n      <td>1.960000e+08</td>\n      <td>2.030000e+08</td>\n      <td>2.135000e+08</td>\n      <td>2.327500e+08</td>\n      <td>2.607500e+08</td>\n      <td>1.589950e+08</td>\n      <td>1.654446e+08</td>\n      <td>1.782971e+08</td>\n      <td>...</td>\n      <td>2.235821e+09</td>\n      <td>2.333308e+09</td>\n      <td>2.451625e+09</td>\n      <td>2.705783e+09</td>\n      <td>3.104395e+09</td>\n      <td>2.732809e+09</td>\n      <td>2.748180e+09</td>\n      <td>2.668496e+09</td>\n      <td>2.631434e+09</td>\n      <td>2.841786e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 63 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Information about the data\nThe dataset is a subset of data obtained from World Bank national accounts data, and OECD National Accounts data files. It contains Country names, Country codes and GDP for years from 1960 - 2020 as annual gross domestic income in US dollars for that year.\n\n### Information from the website:\n\nGDP at purchaser's prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars. Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used.\n\n#### Periodicity: Annual\n\nStatistical Concept and Methodology: Gross domestic product (GDP) represents the sum of value added by all its producers. Value added is the value of the gross output of producers less the value of intermediate goods and services consumed in production, before accounting for consumption of fixed capital in production. The United Nations System of National Accounts calls for value added to be valued at either basic prices (excluding net taxes on products) or producer prices (including net taxes on products paid by producers but excluding sales or value added taxes). Both valuations exclude transport charges that are invoiced separately by producers. Total GDP is measured at purchaser prices. Value added by industry is normally measured at basic prices.\n\n## Data Preprocessing\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nchina = countries[countries[\"Country Name\"]== 'China']\nchina = china.drop(['Country Name', 'Country Code'], axis = 1)\nchina = china.T\nchina.columns = ['GDP in US$']\ndf_gdp = china.reset_index()\ndf_gdp.columns = ['Year', 'GDP in US$']\ndf_gdp.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>GDP in US$</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1960</td>\n      <td>5.971647e+10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961</td>\n      <td>5.005687e+10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1962</td>\n      <td>4.720936e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1963</td>\n      <td>5.070680e+10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1964</td>\n      <td>5.970834e+10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndf_gdp.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 61 entries, 0 to 60\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Year        61 non-null     object \n 1   GDP in US$  61 non-null     float64\ndtypes: float64(1), object(1)\nmemory usage: 1.1+ KB\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndf_gdp['Year'] = df_gdp['Year'].astype('int32')\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.figure(figsize=(8,5))\nX = df_gdp['Year']\ny = df_gdp[\"GDP in US$\"]\nplt.plot(X, y, 'bo')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=663 height=443}\n:::\n:::\n\n\n## Method Development : Logistic (Sigmoid) Regression\nFrom an initial look at the plot, the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nThe formula for the logistic function is the following:\n\nY^=11+eβ1(X−β2)\n \nβ1\n : Controls the curve's steepness,\n\nβ2\n : Slides the curve on the x-axis.\n\n### Building The Model\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nX, y = (df_gdp[\"Year\"].values, df_gdp[\"GDP in US$\"].values)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n#Define a function named sigmoid\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Develop the sigmoid model with a random beta_1 and beta_2 values\nbeta_1 = 0.10\nbeta_2 = 1990.0\n#logistic function\ny_pred = sigmoid(X, beta_1 , beta_2)\n#plot initial prediction against datapoints\nplt.plot(X, y_pred*15000000000000)\nplt.plot(X, y, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=571 height=425}\n:::\n:::\n\n\n## Find the best parameters for the fit line\n\nTo find the best parameters beta_1 and beta_2 values for the model. For this first normalize X and y:\n\nThen use curve_fit which uses non-linear least squares to fit the sigmoid function, to data to find Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.\n\npopt are optimized parameters.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Normalizing the data\nx =X/max(X)\ny =y/max(y)\n```\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x, y)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n beta_1 = 565.120191, beta_2 = 0.995458\n```\n:::\n:::\n\n\nPlot the resulting regression model with the optimized parameters popt\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nplt.figure(figsize=(8,5))\ny_pred = sigmoid(x, *popt)\nplt.plot(X, y, 'ro', label='original data')\nplt.plot(X,y_pred, linewidth=3.0, label='predicted fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=663 height=429}\n:::\n:::\n\n\n## Model Evaluation - Logistic (Sigmoid) Regression\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# split data into train/test\nmsk = np.random.rand(len(df_gdp)) < 0.8\nx_train = x[msk]\nx_test = x[~msk]\ny_train = y[msk]\ny_test = y[~msk]\n\n# build the model using train set\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, x_train, y_train)\n\n# predict using test set\ny_hat = sigmoid(x_test, *popt)\n\n# Evaluation metrics\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , y_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.99\n```\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nk = df_gdp[['GDP in US$']].shape[1]\nn = len(x_test)\n\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\nprint('Mean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is:', metrics.mean_absolute_error(y_test, y_hat))\nprint('Mean Squared Error(MSE) of Logistic (Sigmoid)Regression model is:', metrics.mean_squared_error(y_test, y_hat))\nprint('Root Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))\n# Explained variance score: 1 is perfect prediction\nprint('Explained Variance Score (EVS) of Logistic (Sigmoid)Regression model is:',explained_variance_score(y_test, y_hat))\n#Residual sum of squares (rss)\nprint(\"Residual sum of squares of Logistic (Sigmoid)Regression model is: %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint('R2 of Logistic (Sigmoid)Regression model is:',metrics.r2_score(y_test, y_hat))\nprint('R2 rounded of Logistic (Sigmoid)Regression model is:',(metrics.r2_score(y_test, y_hat)).round(2))\nr2 = r2_score(y_test, y_hat)\nr2_rounded = r2_score(y_test, y_hat).round(2)\nadjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)\nprint('Adjusted_r2 of Logistic (Sigmoid)Regression model is: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error(MAE) of Logistic (Sigmoid)Regression model is: 0.01691289541887371\nMean Squared Error(MSE) of Logistic (Sigmoid)Regression model is: 0.0004875880086879474\nRoot Mean Squared Error (RMSE) of Logistic (Sigmoid)Regression model is: 0.02208139508020151\nExplained Variance Score (EVS) of Logistic (Sigmoid)Regression model is: 0.9927396411660685\nResidual sum of squares of Logistic (Sigmoid)Regression model is: 0.00\nR2 of Logistic (Sigmoid)Regression model is: 0.9879656354848502\nR2 rounded of Logistic (Sigmoid)Regression model is: 0.99\nAdjusted_r2 of Logistic (Sigmoid)Regression model is:  0.986\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}